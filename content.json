{"meta":{"title":"Yury's Note","subtitle":"","description":"my study note","author":"Yury","url":"https://yury757.github.io","root":"/"},"pages":[{"title":"About me","date":"2021-08-23T14:45:11.000Z","updated":"2021-08-24T08:02:02.783Z","comments":false,"path":"about/index.html","permalink":"https://yury757.github.io/about/index.html","excerpt":"","text":"这个人很懒，什么都没留下。"},{"title":"categories","date":"2021-08-24T08:03:04.000Z","updated":"2021-08-24T08:03:37.908Z","comments":false,"path":"categories/index.html","permalink":"https://yury757.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"redis","slug":"database/redis/redis","date":"2022-05-02T16:00:00.000Z","updated":"2022-10-07T12:47:35.366Z","comments":true,"path":"database/redis/redis/","link":"","permalink":"https://yury757.github.io/database/redis/redis/","excerpt":"","text":"一、ACL123456789101112131415ACL &lt;subcommand&gt; [&lt;arg&gt; [value] [opt] ...]. # access control list，访问控制列表，可以结合subcommand子命令形成很多命令组合，主要用于权限访问相关命令。如：acl cat [&lt;category&gt;] # 列举category目录下的所有命令，若未指定category，则列举所有categoryacl users # 展示所有用户acl whoami # 展示当前用户acl deluser &lt;username&gt; [&lt;username&gt; ...] # 删除用户acl getuser &lt;username&gt; # 获取用户详细信息acl list # 以配置文件格式展示所有用户详情acl setuser &lt;username&gt; &lt;attribute&gt; [&lt;attribute&gt; ...] # 新增或者修改用户属性acl genpass [&lt;bits&gt;] # 生成一个指定位长度（默认256位）的伪随机密码，注意这个长度是位的长度而不是生成的密码的长度acl load # 从ACL文件中重新加载用户配置文件acl save # 将当前配置保存在ACL文件中acl log [&lt;count&gt; | RESET] # 展示acl日志入口acl help # 显示acl命令帮助 1、修改用户权限 acl setuser &lt;username&gt; &lt;attribute&gt; [&lt;attribute&gt; ...] 该命令的attribute叫ACL RULE，ACL RULE分为两部分，一部分是命令访问权限相关（Command rules），另一部分是定义了用户状态（User management rules） 完整的使用方式参考：[ACL SETUSER | Redis](https://redis.io/commands/acl-setuser/) 命令规则主要有： ~&lt;pattern&gt;：控制用户只能访问符合pattern模式的key allkeys：可以访问所有key，等于~* &amp;&lt;pattern&gt;：控制用户只能访问符合pattern模式的发布/订阅channel allchannels：可以访问所有channel，，等于&amp;* +&lt;command&gt;：为用户添加一个命令访问权限，即可以访问这个命令 -&lt;command&gt;：为用户减少一个命令访问权限，即无法访问这个命令 allcommands：用户可以访问所有命令 nocommands：移除用户所有命令的访问权限，啥都干不了了 +@&lt;category&gt;：为用户添加一个目录的访问权限，该目录下的所有命令都可以访问 -@&lt;category&gt;：为用户减少一个目录的访问权限，该目录下的所有命令都无法访问 用户状态规则主要有： on：将用户设置为有效 off：将用户设置为无效用户 &gt;mypassword：为用户新增一个密码，输入方式为可读的明文。redis允许为一个用户设置多个密码 #hashedpassword：为用户新增一个密码，输入方式为hash加密后的密文 &lt;mypassword：为用户减少一个密码 !hashedpassword：为用户减少一个密码，输入方式为hash加密后的密文 reset：重置为空用户，即设置为off, without passwords，无法执行任何命令，也无法访问任何key 开放权限用+，收回权限用-，对应的权限为acl cat [&lt;category&gt;]列举出来的目录或者命令 二、数据库相关命令1234select &lt;index&gt; # 切换数据库dbsize # 查看当前数据库的key的数量，O(1)时间复杂度flushdb &lt;async|sync&gt; # 清空当前库。默认参数是同步，6.2版本后，可以通过设置lazyfree-lazy-user-flush配置参数为yes，将该命令的默认参数调整为异步。异步清空时，只会删除执行命令的时间点之前的key，在那个时间点之后写入的key不受影响flushall &lt;async|sync&gt; # 清空所有库。参数同上。 三、key相关命令123456789keys &lt;pattern&gt; # 按照glob pattren，列举所有满足条件的key。# 注意如果数据量很大，这个速度会很慢，O(N)时间复杂度，生产环境要尤其小心使用# 在应用程序中应尽量避免使用keys，如果需要搜索key，应该考虑使用scan或者setsexists &lt;key...&gt; [&lt;key&gt;...] # 判断key...是否存在，返回值是存在的key的数量type &lt;key&gt; # 查看key对应的value的类型del &lt;key...&gt; [&lt;key&gt;...] # 直接删除一个key，阻塞删除unlink &lt;key...&gt; [&lt;key&gt;...] # 从keyspace删除一个key，然后会在另一个线程异步删除实际内存中的key，非阻塞删除expire &lt;key&gt; &lt;second&gt; # 给key设置过期时间，单位秒ttl &lt;key&gt; # 查看key的剩余过期时间，返回-1表示永不过期，返回-2表示已经过期了 四、常用数据类型1、String（1）数据结构 String底层是安全的二进制形式，这样String可以包含所有类型的数据。 String使用的数据结构叫简单动态字符串，相当于java的List&lt;Character&gt;。 String最大512兆个字节，对于ascii字符，一个字符占用一个字节，512兆就是也可以代表长度，但是对于中文，一个unicode中文字符占用3个字节，因此若包含非ascii字符，则最大长度小于512兆。 （2）常用命令 123456789101112131415161718192021222324252627set &lt;key&gt; &lt;value&gt;get &lt;key&gt;append &lt;key&gt; &lt;value&gt;strlen &lt;key&gt;setnx &lt;key&gt; &lt;value&gt; # 重要！！只有在key不存在时，才设置成功，否则设置失败。多用于分布式锁# 数值型字符串操作incr &lt;key&gt; # 使数字值增加1decr &lt;key&gt; # 使数字值减小1incrby &lt;key&gt; &lt;increment&gt; # 使数字值增加incrementdecrby &lt;key&gt; &lt;increment&gt; # 使数字值减小increment# 由于redis是单线程存取，没有并发问题，因此这些命令本身就是原子性操作。# 批量操作，可以批量设置key以及获取keymset &lt;key&gt; &lt;value&gt; [&lt;key&gt; &lt;value&gt; ...]mget &lt;key&gt; [&lt;key&gt; ...] # 返回结果按照get的顺序展示msetnx &lt;key&gt; &lt;value&gt; [&lt;key&gt; &lt;value&gt; ...] # 注意！！该命令是原子操作，只要有一个key存在则全部失败。# range操作getrange &lt;key&gt; &lt;start&gt; &lt;end&gt; # 获取字符串中[start, end]区间内的字符setrange &lt;key&gt; &lt;offset&gt; &lt;value&gt; # 从offset下标开始设置value# 带货期时间的setsetex &lt;key&gt; &lt;second&gt; &lt;value&gt;# get原值并set新值getset &lt;key&gt; &lt;value&gt; # set新值，并返回原先的值 2、List（1）数据结构 redis的List列表是双向链表，底层数据结构用压缩列表+链表实现，表现形式相当于java的LinkedList&lt;String&gt;。 两边存取的时间复杂度为O(1)，随机访问的时间复杂度为O(N)。 当list中没有任何元素时，key也会被移除。 List常用于获取最新数据、或者最近信息的业务场景，比如获取最近10条短信，获取最新资讯等等。 （2）常用命令 123456789101112131415161718# 获取长度，时间复杂度O(1)。若key不存在则返回0，若key不是list，则报错llen &lt;key&gt;# l表示从左边按顺序操作，r表示从右边按顺序操作lpush/rpush &lt;key&gt; &lt;element&gt; [&lt;element&gt;...] # 将元素push到列表中lrange/rrange &lt;key&gt; &lt;start&gt; &lt;stop&gt; # 获取列表中[start, stop]区间内的元素，stop若为-1表示所有lpop/rpop &lt;key&gt; [count] # 移除count（默认值1）个数量的元素并按顺序返回这些元素blpop/brpop &lt;key&gt; [&lt;key&gt;...] &lt;timeout&gt; # 阻塞式地取出一个元素，直到超时或者元素出现。当timeout为0时，永久等待# rpoplpush，当需要将一个list按顺序移动到另外一个list上时，可以循环这么操作rpoplpush &lt;source&gt; &lt;destination&gt; # 对source执行rpop，并将取出来的元素对destination执行lpush# 这里的l代表list，而不是leftlindex &lt;key&gt; &lt;index&gt; # 从左边根据下标index获取对应元素lset &lt;key&gt; &lt;index&gt; &lt;value&gt; # 从左边将下标为index的元素替换成valuelrem &lt;key&gt; &lt;count&gt; &lt;element&gt; # 从左边移除count个element元素ltrim &lt;key&gt; &lt;start&gt; &lt;stop&gt; # 从左到右，只保留[start, stop]区间内的元素linsert &lt;key&gt; &lt;before/after&gt; &lt;pivot&gt; &lt;element&gt; # 从左边在扫描到的第一个pivot元素前面或后面插入element元素 3、Set（1）数据结构 redis的Set就是一个哈希表，底层数据结构叫字典，相当于java的HashSet&lt;String&gt;。 （2）常用命令 1234567891011121314sadd &lt;key&gt; &lt;member&gt; [&lt;member&gt;...]smembers &lt;key&gt; # 获取set中的所有元素，返回顺序可能不是插入顺序sismember &lt;key&gt; &lt;member&gt; # 判断member是否存在于集合中scard &lt;key&gt; # 获取集合中元素数量srem &lt;key&gt; &lt;member&gt; [&lt;member&gt;...] # 删除指定元素spop &lt;key&gt; [count] # 随机删除count个元素，并返回这些元素srandmember &lt;key&gt; [count] # 随机取出count个元素smove &lt;key&gt; &lt;source&gt; &lt;destination&gt; &lt;member&gt; # 将元素member从source移动到destination集合中sinter &lt;key&gt; [&lt;key&gt;...] # 取所有集合的交集，若只给定一个参数，则返回所有元素sunion &lt;key&gt; [&lt;key&gt;...] # 取所有集合的并集，若只给定一个参数，则返回所有元素sdiff &lt;key1&gt; [&lt;keyn&gt;...] # 取key1中不存在于keyn里面的元素，若只给定一个参数，则返回所有元素 4、Hash（1）数据结构 redis的Hash是一个哈希表，即value本身又是一个哈希表，相当于Java的HashMap&lt;String, String&gt;。数据结构为压缩列表和哈希表两种，当数量小时用压缩列表，当数量大时用哈希表。 （2）常用命令 123456789hset &lt;key&gt; &lt;field&gt; &lt;value&gt; [&lt;field&gt; &lt;value&gt;...] # 等于hmset，hmset已经被弃用hget &lt;key&gt; &lt;field&gt; [&lt;field&gt;...] # 等于hmget，hmget已经被弃用hexistshkeyshvals# 重要操作hincrby &lt;key&gt; &lt;field&gt; &lt;increment&gt; # 给field增加incrementhsetnx &lt;key&gt; &lt;field&gt; &lt;value&gt; # 只有field不存在时才set成功，否则set失败 5、Zset（1）数据结构 Zset是指有序集合，即按照一定评分（score）排序后的Set，即Sorted Set。使用的数据结构是哈希表结合跳跃表，在哈希表中，field是对应的member，而value是对应的score，然后在此基础上建立一层跳跃表。 Zset常用于需要排序的业务场景，如获取热度前十的新闻等。 （2）常用命令 1234567891011zadd &lt;key&gt; &lt;score&gt; &lt;member&gt; [&lt;score&gt; &lt;member&gt;...]zrange &lt;key&gt; &lt;min&gt; &lt;max&gt; [withscores] # 获取排序在[min, max]区间内的元素zrangebyscore &lt;key&gt; &lt;min&gt; &lt;max&gt; [withscores] # 获取score在[min, max]区间内的元素zount &lt;key&gt; &lt;min&gt; &lt;max&gt; # 统计score在[min, max]区间内的元素数量zrank &lt;key&gt; &lt;member&gt; # 获取member的排名，若score相同，则按照写入顺序获取排名zrevrange &lt;key&gt; &lt;start&gt; &lt;stop&gt; # 反向，从大到小获取排序在[start, stop]区间内的元素zrevrangebyscore &lt;key&gt; &lt;max&gt; &lt;min&gt; [withscores] # 反向，从大到小获取score在[min, max]区间内的元素zincrby &lt;key&gt; &lt;increment&gt; &lt;member&gt; # 给member的score加上increment，自动重新排序zrem &lt;key&gt; &lt;member&gt; [&lt;member&gt;...] # 删除member 6、Bitmaps（1）数据结构 Bitmaps位图，即只有0和1的一个数组，类似于java的boolean[]。c++的select底层就使用了bitmaps作为socket的存储结构。redis的bitmaps通过一个数字型字符串的位操作来实现，通过偏移量（偏移量从1开始）表示元素的位置。 之前说过String的最大大小为512MB，因此这里存储位时最多可以存储512MB*8个位。 bitmaps适用于只需要是/否两种值的业务场景，如用户是否是活跃用户，用户是否拥有某个功能的权限等。 （2）常用命令 1234567setbit &lt;key&gt; &lt;offset&gt; &lt;value&gt;getbit &lt;key&gt; &lt;offset&gt;bitcount &lt;key&gt; [&lt;start&gt; &lt;end&gt; &lt;bit/byte&gt;] # 按位或按字节统计为1的位的数量bitop &lt;operation&gt; &lt;destkey&gt; &lt;srckey&gt; [&lt;srckey&gt;...] # 按位操作，将结果写入destkey，因此这个destkey最好是不存在的新key# 位操作：and按位与，or按位或，xor按位异或，not按位取反# 其中当使用not时，srckey只能用一个 7、HyperLogLog（1）数据结构 HyperLogLog是用于基数计数的一种数据类型。基数计数是指统计集合中不重复元素的数量。 HyperLogLog数据类型使用**HyperLogLog Counting(HLL)**实现，只做基数计算，不会保存元数据。采用稀疏矩阵存储，空间占用很小，仅仅在计数基数个数慢慢变大，稀疏矩阵占用空间渐渐超过了阈值时才会一次性转变成稠密矩阵，转变成稠密矩阵之后最高占用12K的内存空间。即每个HyperLogLog的key最高占用12K。 两个缺点：使用概率算法计算，存在标准误差；不保存原始数据，对需要使用原始数据的场景不友好。 （2）命令 123pfadd &lt;key&gt; &lt;element&gt; [&lt;element&gt;...]pfcount &lt;key&gt; [&lt;key&gt;...] # 统计基数计数pfmerge &lt;destkey&gt; &lt;srckey&gt; [&lt;srckey&gt;...] # 将srckey全部合并到新的destkey中 8、Geospecial indexes（1）数据结构 Geospecial是基于地理位置索引的一种数据类型。 （2）命令 123456geoadd &lt;key&gt; &lt;longitude&gt; &lt;latitude&gt; &lt;member&gt; # 添加一个元素的地理位置geopos &lt;key&gt; &lt;member&gt; # 获取某个元素的地理位置geodist &lt;key&gt; &lt;member1&gt; &lt;member2&gt; [m|km|ft|mi] # 获取两个元素之间地理位置的直线距离，后面那个参数表示单位，默认是米# 以longitude、latitude为中心，radius为半径，找出这个范围内的元素，并做一些其他统计操作georadius &lt;key&gt; &lt;longitude&gt; &lt;latitude&gt; radius m|km|ft|mi [withcrood] [withdist] [withhash] [count count [any]] [asc|desc] [store key] [storedist key] 五、发布订阅redis支持发布订阅（publish/subscribe）的消息通信模式。 类似于不支持历史消息、不能持久化的kafka，就像收音机，从打开的那一时刻开始接收后面的消息，前面的历史消息没有了。 命令： 12345# 发布者publish &lt;channel&gt; &lt;message&gt; # 向一个频道发布消息# 订阅者subscribe &lt;channel&gt; [&lt;channel&gt;...] # 打开某个频道，接收后续消息 六、持久化1、RDBRDB（Redis Database），在指定时间间隔将数据集快照存储到硬盘中，类似于MySQL的dump all操作。 12save # 立即阻塞式执行持久化，会阻塞其他客户端的操作，不建议使用bgsave # 立即在后台执行持久化，生成子进程执行持久化最后向父进程返回消息 触发后台进行持久化的redis.conf配置参数，若以下save配置被注释则不会自动持久化，只要有任意配置生效就会按规则自动持久化。 12345678save 900 1 # 900秒（15分钟）内至少1个key值改变，则进行持久化save 300 10 # 300秒（5分钟）内至少10个key值改变，则进行持久化save 60 10000 # 60秒（1分钟）内至少10000个key值改变，则进行持久化stop-writes-on-bgsave-error yes # 后台持久化发生异常时则会拒绝执行写命令，这样做是为了让用户感知到这里发生了异常rdbcompression yes # rdb文件是否进行压缩rdbchecksum yes # 从第5个版本开始，rdb文件末尾放了一个CRC64的校验码，用于校验数据文件的完整性dbfilename dump.rdb # 备份的文件名，当redis启动时，会自动加载这个备份数据文件并加载到内存中。 优点： 1、适合大规模的数据恢复。 2、如果业务对数据完整性和一致性要求不高，RDB是很好的选择。 缺点： 1、数据的完整性和一致性不高，因为RDB可能在最后一次备份时宕机了。 2、备份时占用内存，因为Redis 在备份时会独立创建一个子进程，将数据写入到一个临时文件（此时内存中的数据是原来的两倍哦），最后再将临时文件替换之前的备份文件。 2、AOFAOF（Append Only File），记录所有服务器接收到的写操作，并在服务器重新启动时重放这些操作，以重新构建数据集，类似于MySQL的binlog日志。 redis默认不开启AOF，需要手动开启。AOF相关配置如下： 1234567891011appendonly yes # 设置为yes开发AOFappendfilename &quot;appendonly.aof&quot; # aof日志文件名，redis启动时会自动加载该文件中的操作日志并在服务器中重放，生成数据。# appendfsync always # 记录时机，always表示每次写操作执行后都持久化，会损耗性能appendfsync everysec # everysec表示每秒进行一次持久化操作# appendfsync no # no又操作系统决定什么时候进行持久化 # 使用everysec或always时，redis会启动一个子进程在后台进行磁盘IO操作。no-appendfsync-on-rewrite no # 自动重写aof文件机制。当aof文件增长一定大小的百分比时，就会调用BGREWRITEAOF重写日志文件auto-aof-rewrite-percentage 100 # 需要达到的增长百分比auto-aof-rewrite-min-size 64mb # 增长百分比的基数 3、比较和建议1、RDB适合用于冷备份，即每周、每月定时执行一次全量备份这种，可以不开启；而AOF类似于MySQL的binlog，最好开启并设置持久化时机为everysec，既能保证性能又可以保证较高的数据一致性，最多丢失1秒内的数据。 2、当dump.rdb文件和appendonly.aof文件同时存在时，redis启动时会加载aof文件 七、java编程1、jedisjedis基本是完全模拟redis-cli的命令实现的java端的redis客户端。其方法和redis-cli命令基本一致。jedis的使用略。 注意点： 1、jedis客户端是线程不安全的，意味着如果有多个线程通过同一个jedis客户端访问时，会出现socket和IO流方面的异常。因为在jedis底层一个jedis对象代表一个socket和一个IO流，多个线程使用同一个jedis代表多个线程使用同一个socket和同一个IO流，肯定会出现异常。因此应该使用JedisPool或lettuce。 2、spring-data-redis（1）配置123456789101112131415161718&lt;!-- springboot --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt;&lt;/dependency&gt;&lt;!-- springboot redis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;version&gt;2.6.7&lt;/version&gt;&lt;/dependency&gt;&lt;!-- springboot集成redis pool需要这个依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;2.11.1&lt;/version&gt;&lt;/dependency&gt; 123456789spring.redis.host=192.168.141.141spring.redis.port=6379spring.redis.password=rootspring.redis.database=0spring.redis.timeout=1800000spring.redis.lettuce.pool.max-active=10spring.redis.lettuce.pool.max-wait=100msspring.redis.lettuce.pool.max-idle=5spring.redis.lettuce.pool.min-idle=0 （2）简单使用redisTemplate对redis的几种数据类型分别封装了几种操作类型，如下： redisTemplate方法 redis类型 opsForValue String opsForHash Hash opsForCluster 对集群的操作 opsForList List opsForSet Set opsForGeo Geo地理位置数据类型 opsForHyperLogLog HyperLogLog opsForStream 对流进行操作 opsForZSet ZSet 123456789101112131415161718@SpringBootApplicationpublic class Demo4SpringBoot implements CommandLineRunner &#123; @Qualifier(&quot;redisTemplate&quot;) @Autowired private RedisTemplate template; public static void main(String[] args) &#123; SpringApplication.run(Demo4SpringBoot.class, args); &#125; @Override public void run(String[] args) &#123; template.opsForValue().set(&quot;k1&quot;, &quot;v1&quot;); String v1 = (String)template.opsForValue().get(&quot;k1&quot;); System.out.println(v1); &#125;&#125; （3）序列化redisTemplate需要四个序列化器： keySerializer valueSerializer hashKeySerializer hashValueSerializer 若我们没指定序列化方式时，redisTemplate的四个序列化器都会使用默认的JdkSerializationRedisSerializer来进行序列化。 JdkSerializationRedisSerializer的作用是将java对象序列化为二进制字节，基本无法在业务中使用，因此建议使用StringRedisSerializer，将所有key和value都手动序列化成string，然后由StringRedisTemplate写入redis。读取时，从redis读取到的所有value都是string，然后手动反序列化成自己想要的类型。 1234567891011121314151617181920212223242526272829303132333435363738394041424344@SpringBootApplicationclass Demo4SpringBoot2 implements CommandLineRunner &#123; @Resource(name=&quot;stringRedisTemplate&quot;) private RedisTemplate template; public static void main(String[] args) &#123; SpringApplication.run(Demo4SpringBoot.class, args); &#125; @Override public void run(String[] args) throws JsonProcessingException &#123; ObjectMapper mapper = new ObjectMapper(); int v = 12; template.opsForValue().set(&quot;k1&quot;, String.valueOf(v)); String v1 = (String)template.opsForValue().get(&quot;k1&quot;); System.out.println(v1); template.opsForValue().set(&quot;k2&quot;, &quot;v2&quot;); String v2 = (String)template.opsForValue().get(&quot;k2&quot;); System.out.println(v2); User user = new User(&quot;小明&quot;, 20L); template.opsForValue().set(&quot;k3&quot;, mapper.writeValueAsString(user)); String v3 = (String)template.opsForValue().get(&quot;k3&quot;); User user2 = mapper.readValue(v3, User.class); System.out.println(user2); template.opsForHash().put(&quot;k4&quot;, &quot;f1&quot;, &quot;v1&quot;); template.opsForHash().put(&quot;k4&quot;, &quot;f2&quot;, &quot;v2&quot;); template.opsForHash().put(&quot;k4&quot;, &quot;f3&quot;, &quot;v3&quot;); Map&lt;Object, Object&gt; k4 = template.opsForHash().&lt;String, String&gt;entries(&quot;k4&quot;); System.out.println(k4); &#125;&#125;@Data@NoArgsConstructor@AllArgsConstructorclass User &#123; private String name; private long age;&#125; 八、并发1、事务介绍redis的事务是一个单独的隔离操作：事务中的所有命令都会被序列化，按顺序地执行，在执行过程中不会被其他命令打断。主要用处是将多个命令串联在一起，防止被别的命令插队修改数据导致无法实现预期的操作。 类似于将多个命令按顺序打包成一个命令。 1234# redis-cli命令如下multi # 开启事务，相当于begin，开启后该客户端发送给服务器的命令会按顺序被打包起来，并不直接执行exec # 执行事务，先当与commit，将打包的命令依次执行，执行期间其他任何命令都会被阻塞discard # 抛弃事务，相当于rollback，主动抛弃所有打包的命令 123456789101112131415161718192021// java代码如下 public static void test1() &#123; Jedis jedis = getJedis(); Transaction multi = jedis.multi(); try &#123; multi.set(&quot;k1&quot;, &quot;v1&quot;); // int a = 1 / 0; multi.set(&quot;k2&quot;, &quot;v2&quot;); System.out.println(&quot;all command queued&quot;); multi.setnx(&quot;k1&quot;, &quot;v1&quot;); System.out.println(&quot;execute queued command&quot;); List&lt;Object&gt; results = multi.exec(); System.out.println(results); &#125;catch (Exception ex) &#123; ex.printStackTrace(); try &#123; multi.discard(); &#125;catch (Exception ignored) &#123; &#125; System.out.println(&quot;all command discarded&quot;); &#125; &#125; 出现异常时的几种情况： 1、multi后打包命令时出现异常（相当于编译时异常）：multi会自动结束，相当于手动调用了discard命令 2、exec执行中某个命令异常（相当于运行时异常）：出现异常的命令返回对应的异常值，不影响其他命令的执行。这一点和关系型数据库的事务差别很大，如在postgres中，在默认事务隔离级别下，事务在执行过程中若出现异常，后续语句都会被忽略，整个事务会自动回滚。 和关系型数据库的差异： 1、单独的隔离操作，所有命令依次执行，中间不会被别人打断。而关系型数据库的事务可以穿插执行，只是不同sql的锁机制不一样。 2、没有隔离级别的概念，redis事务执行exec之前不会实际执行，只有exec后才会真正执行。 3、不保证原子性，若一条命令运行失败，不影响其他被打包的命令。而关系型是数据库的事务是保证原子性的，要么全部成功，要么全部失败。 2、并发冲突redis中有一个key为10000元，一共有10000个消费线程，每个线程判断金额是否大于等于10元，若满足则消费10元，否则不消费。 如下，预期最后redis的key应该为0，但是若没有任何对redis进行加锁的机制，则这个程序运行的结果大概率是小于0的（也称“超卖问题”）。 解决方案：乐观锁或lua脚本 1234567891011121314151617public static void test2 () &#123; class MyConsumer implements Runnable&#123; @Override public void run() &#123; Jedis jedis = getJedis(); String a = jedis.get(&quot;a&quot;); Integer v = Integer.valueOf(a); if (v &gt;= 10) &#123; jedis.decrBy(&quot;a&quot;, 10L); &#125; jedis.close(); &#125; &#125; for (int i = 0; i &lt; 10000; i++) &#123; new Thread(new MyConsumer()).start(); &#125;&#125; 3、连接超时在高并发情况下，连接redis的客户端很多，但是redis有一个max_client，超过该数量时新客户端会进入等待状态，若等待超过一定时间就会超时。 解决方案：使用连接池。 4、乐观锁和悲观锁乐观锁是指：读加共享锁，对写进行CAS写。 悲观锁是指：读写都加排他锁，每次对这个key进行操作的只可能有一个客户端。 redis无法实现悲观锁，只能实现乐观锁，乐观锁是通过watch + multi组合实现的。 乐观锁的实现 watch命令：可以监控某些key，当事务在这些key上进行写操作时，若该key被其他命令修改，则该事务会被打断，事务执行exec时返回null或异常。 unwatch命令：取消对某些key的监控。 原理：watch命令会获取key的版本号，事务中对key进行的操作会对比watch时的版本号和现在的版本号，若不一样则事务会被打断。 1234watch amultidecrby a 10exec 5、秒杀案例实现（1）通过乐观锁实现秒杀案例介绍：模拟2000个用户抢购1000个商品。java代码如下。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091package net.yury;import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisPool;import redis.clients.jedis.JedisPoolConfig;import redis.clients.jedis.Transaction;import java.time.Duration;import java.util.ArrayList;import java.util.List;import java.util.concurrent.ExecutionException;/** * 商品秒杀案例 */public class Demo6SecKill &#123; public final static String redisHost = &quot;192.168.141.141&quot;; public final static int redisPort = 6379; public final static String passWord = &quot;root&quot;; private final static JedisPool pool; static&#123; JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(200); // 最大连接数量 config.setBlockWhenExhausted(true); // 当连接池没有连接可用时，是否等待 config.setMaxWait(Duration.ofSeconds(10)); // 从连接池中获取连接时等待时间 config.setMaxIdle(32); // 连接池中的连接最大空闲数量 config.setTestOnBorrow(true); // 从连接池中获取连接时测试连接是否有效 pool = new JedisPool(new JedisPoolConfig(), redisHost, redisPort, 60000, passWord); &#125; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; long time1 = System.currentTimeMillis(); doSecKill(); System.out.println((System.currentTimeMillis() - time1) + &quot;ms&quot;); &#125; public static void doSecKill() &#123; // 模拟2000个人来进行秒杀 List&lt;Thread&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 2000; i++) &#123; list.add(new Thread(new SecKillThread(&quot;user&quot; + i))); &#125; for (Thread thread : list) &#123; thread.start(); &#125; &#125; private static class SecKillThread implements Runnable&#123; public SecKillThread(String userID) &#123; this.userID = userID; &#125; private final String userID; // 用户id // 执行秒杀的动作 @Override public void run() &#123; final Jedis jedis = pool.getResource(); // 1、监控库存 jedis.watch(&quot;a&quot;); // 2、判断库存是否为null或小于等于0 String total = jedis.get(&quot;a&quot;); if (total == null || Integer.parseInt(total) &lt;= 0) &#123; pool.returnResource(jedis); System.out.println(userID + &quot; 秒杀已结束&quot;); return; &#125; // 3、判断用户是否重复秒杀，每个用户只能秒杀一次 if (jedis.sismember(&quot;exists&quot;, userID)) &#123; pool.returnResource(jedis); System.out.println(userID + &quot; 已经秒杀过一次了，不能秒杀第二次了&quot;); return; &#125; // 4、秒杀过程 Transaction multi = jedis.multi(); multi.decrBy(&quot;a&quot;, 1); multi.sadd(&quot;exists&quot;, userID); List&lt;Object&gt; results = multi.exec(); if (results == null || results.size() == 0) &#123; pool.returnResource(jedis); System.out.println(userID + &quot; 秒杀失败&quot;); return; &#125; System.out.println(userID + &quot; 秒杀成功&quot;); pool.returnResource(jedis); &#125; &#125;&#125; 存在的问题：库存遗留问题。即一些用户第一次因为并发问题抢购失败，但是方法直接return了，当因为并发问题抢购失败的用户 &gt; 总商品数量 - 抢购成功的用户时，可能会存在部分商品没有卖掉的情况。 一种解决方案是，在外层套一个while循环，当没有抢购成功时，进行重试，代码如下。这种性能很差，不建议使用。 另一种解决方案就是使用lua脚本。 1234567891011121314151617181920212223242526272829303132333435363738394041424344private static class SecKillThread2 implements Runnable&#123; public SecKillThread2(String userID) &#123; this.userID = userID; &#125; private final String userID; // 用户id // 执行秒杀的动作 @Override public void run() &#123; int n = 0; final Jedis jedis = pool.getResource(); while (true)&#123; // 1、监控库存 jedis.watch(&quot;a&quot;); // 2、判断库存是否为null或小于等于0 String total = jedis.get(&quot;a&quot;); if (total == null || Integer.parseInt(total) &lt;= 0) &#123; System.out.println(userID + &quot; 秒杀已结束&quot;); return; &#125; // 3、判断用户是否重复秒杀，每个用户只能秒杀一次 if (jedis.sismember(&quot;exists&quot;, userID)) &#123; System.out.println(userID + &quot; 已经秒杀过一次了，不能秒杀第二次了&quot;); return; &#125; // 4、秒杀过程 Transaction multi = jedis.multi(); multi.decrBy(&quot;a&quot;, 1); multi.sadd(&quot;exists&quot;, userID); List&lt;Object&gt; results = multi.exec(); if (results == null || results.size() == 0) &#123; System.out.println(userID + &quot; 秒杀失败，进行第&quot; + ++n + &quot;次重试&quot;); continue; &#125; System.out.println(userID + &quot; 秒杀成功&quot;); pool.returnResource(jedis); return; &#125; &#125;&#125; （2）通过lua脚本实现秒杀lua脚本其实就是另外一种事务的实现，lua脚本内的命令都是一组打包好的命令。 通过lua脚本实现秒杀代码如下。 1234567891011121314151617181920212223242526272829303132333435363738private static class SecKillThread3 implements Runnable&#123; public final static String secKillScripts = &quot;local function secKill(userID)\\n&quot; + &quot; local exists = redis.call(\\&quot;sismember\\&quot;, \\&quot;exists\\&quot;, userID);\\n&quot; + &quot; if tonumber(exists) == 1 then return 3 end;\\n&quot; + &quot; local current = redis.call(\\&quot;get\\&quot;, \\&quot;a\\&quot;);\\n&quot; + &quot; if current == nil then return 2 end;\\n&quot; + &quot; if tonumber(current) &lt;= 0 then return 0 end;\\n&quot; + &quot; redis.call(\\&quot;decr\\&quot;, \\&quot;a\\&quot;);\\n&quot; + &quot; redis.call(\\&quot;sadd\\&quot;, \\&quot;exists\\&quot;, userID);\\n&quot; + &quot; return 1;\\n&quot; + &quot;end\\n&quot; + &quot;return secKill(KEYS[1]);&quot;; public SecKillThread3(String userID) &#123; this.userID = userID; &#125; private final String userID; // 用户id @Override public void run() &#123; final Jedis jedis = pool.getResource(); String sha1 = jedis.scriptLoad(secKillScripts); Object r = jedis.evalsha(sha1, 1, userID); String result = String.valueOf(r); if (&quot;0&quot;.equals(result)) &#123; System.out.println(userID + &quot; 秒杀失败&quot;); &#125;else if (&quot;1&quot;.equals(result)) &#123; System.out.println(userID + &quot; 秒杀成功&quot;); &#125;else if (&quot;2&quot;.equals(result)) &#123; System.out.println(userID + &quot; 秒杀已结束&quot;); &#125;else if (&quot;3&quot;.equals(result))&#123; System.out.println(userID + &quot; 已经秒杀过一次了，不能秒杀第二次&quot;); &#125;else &#123; System.out.println(userID + &quot; 秒杀返回值异常&quot;); &#125; pool.returnResource(jedis); &#125;&#125; 九、缓存相关问题1、缓存穿透用户恶意访问一个不存在的key，redis中不存在，数据库也不存在，因此每次查询都会将请求穿透到数据库层，对数据库造成压力。 解决方案： 为不存在的key也在redis中缓存一个null value，并设置TTL，表示不存在该key，防止请求穿透到数据库 在访问数据之前加一层布隆过滤器。我们将数据库key按照一定的hash算法计算对应的byte放入布隆过滤器中，这样布隆过滤器可以快速判断该key是否存在，当存在则放行，当不存在则拒绝请求。 布隆过滤器是基于概率的判断，当它认为这个key存在时，其实是大概率存在，而不是一定存在，当它认为这个key不存在时，则一定不存在。 2、缓存雪崩redis服务在同一段时间大量key同时失效，或者redis宕机，导致所有请求都打到数据库层，对数据库造成压力。 解决方案： 对不同的key的失效时间加一个随机值，让这些key在一段时间内慢慢失效，而不是同时失效。 使用redis集群提高服务的可用性 给缓存业务添加降级限流策略 给业务添加多层缓存 3、缓存击穿客户端在访问一个热点key时，该key突然失效，导致这些处理线程都去数据库获取数据重建缓存，从而对数据库造成压力。 解决方案： 互斥锁，即对重建缓存这个过程进行加锁，只能有一个线程来执行这个操作，其他线程必须等待那个线程完成。 互斥锁工作流程： 0ms，线程1，查询redis未命中，获取互斥锁成功 0ms，线程2，查询redis未命中，获取互斥锁失败，等待50ms 0ms，线程1，查询数据库，重建缓存。 50ms，线程2，等待结束，重试查询redis，仍然未命中，获取互斥锁依然失败，等待50ms 100ms，线程2，等待结束，重试查询redis，仍然未命中，获取互斥锁依然失败，等待50ms 150ms，线程2，等待结束，重试查询redis，仍然未命中，获取互斥锁依然失败，等待50ms 200ms，线程1，重建redis缓存成功，返回数据。耗时200ms 200ms，线程2，等待结束，重试查询redis，命中缓存，返回数据。 互斥锁实现：使用setnx实现。","categories":[{"name":"redis","slug":"redis","permalink":"https://yury757.github.io/categories/redis/"}],"tags":[]},{"title":"java-文件IO常用操作对比","slug":"java/java-文件IO常用操作对比","date":"2022-04-19T16:00:00.000Z","updated":"2022-10-07T12:57:46.139Z","comments":true,"path":"java/java-文件IO常用操作对比/","link":"","permalink":"https://yury757.github.io/java/java-%E6%96%87%E4%BB%B6IO%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%AF%B9%E6%AF%94/","excerpt":"","text":"文件IO中，常用的方法如下方代码中的readMethod1~8方法所示。 测试了2.5M读100次、100M读3次、250M读1次三种情况，耗时（单位：毫秒）如下： 2.5M读100次 2.5M读100次 100M读3次 100M读3次 250M读1次 250M读1次 普通 HDFS 普通 HDFS 普通 HDFS method1，一次性全部读取 635 1604 976 965 1270 482 method2，ByteArrayOutputStream+byte[] 616 5759 669 5135 843 4375 method3，InputStreamReader+char[8192]+StringWriter 1236 5097 1454 4370 1167 3976 method4，InputStreamReader+BufferedReader(char[8192]) 1565 4556 1986 4763 1608 3230 method5，bufferedReader+stream 1414 4167 62546 140485 - - method6，bufferedReader+stream+parallel 1941 4526 OOM OOM OOM OOM method7，Deque&lt;byte[8196]&gt; 628 5331 761 4456 669 3321 method8，ByteBuffer(2048)+LineBuffer 1910 5325 2310 4426 2300 3575 个人思考： 1、普通文件系统，使用char[]作为中间缓冲（method3~6），速度都比较慢，因为java的string底层是byte[]，先转成char[]，又转回byte[]，会消耗多余的时间。 2、使用method6使用parallel并不能提升性能，因为底层InputStreamReader是加锁的，IO是不能并行的。 3、HDFS不会用，使用最朴素的连接方式，肯定是那里有问题，才会导致IO速度这么慢。但是好像一次性全部读取HDFS的速度，会随着文件的增大而相对更快。 4、最后method7是google guava库中的一种读取全部字符串的方法，脑洞大开，性能都还不错。 5、method3、4、5、6、8都是一行一行读取的模式，适用于需要对每一行进行后续处理的情况。 6、谨慎对读取全部字符串这种批作业使用流处理方式，速度很慢，parallel的甚至直接OOM。 7、总结下来，如果是读取文件中全部字符串，method2 和 method7都是比较不错的方式；如果是需要一行一行处理，则可能还是method4的BufferedReader性能更好。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242package net.yury;import com.google.common.io.ByteStreams;import org.apache.commons.io.IOUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.file.tfile.ByteArray;import java.io.*;import java.nio.ByteBuffer;import java.nio.charset.StandardCharsets;import java.util.concurrent.TimeUnit;import java.util.stream.Collectors;public class Test1 &#123; public static FileSystem fileSystem; static &#123; Configuration configuration = new Configuration(); configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://myubuntu1:8020&quot;); try &#123; fileSystem = FileSystem.get(configuration); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) throws Exception &#123; System.out.println(&quot;测试普通文件系统：&quot;); testReadMethod(new InputStreamBuilder(&quot;NORMAL&quot;, &quot;C:/Users/yury/Desktop/size100M.txt&quot;), 3); System.out.println(&quot;=====================================&quot;); System.out.println(&quot;测试HDFS文件系统：&quot;); testReadMethod(new InputStreamBuilder(&quot;HDFS&quot;, &quot;/test1/size100M.txt&quot;), 3); &#125; public static class InputStreamBuilder &#123; private String type; private String fileName; public InputStreamBuilder(String type, String fileName) &#123; this.type = type; this.fileName = fileName; &#125; public InputStream getInputStream () throws Exception &#123; switch (type) &#123; case &quot;NORMAL&quot;: return new FileInputStream(fileName); case &quot;HDFS&quot;: return fileSystem.open(new Path(fileName)); default: throw new Exception(&quot;unkonw file system&quot;); &#125; &#125; &#125; public static void testReadMethod(InputStreamBuilder builder, int n) throws Exception &#123; long time1 = System.currentTimeMillis(); for (int i = 0; i &lt; n; i++) &#123; readMethod1(builder.getInputStream()); &#125; long time2 = System.currentTimeMillis(); System.out.println(&quot;method1，耗时：&quot; + (time2 - time1) + &quot; 直接读取&quot;); long time3 = System.currentTimeMillis(); for (int i = 0; i &lt; n; i++) &#123; readMethod2(builder.getInputStream()); &#125; long time4 = System.currentTimeMillis(); System.out.println(&quot;method2，耗时：&quot; + (time4 - time3) + &quot; ByteArrayOutputStream+byte[]&quot;); long time5 = System.currentTimeMillis(); for (int i = 0; i &lt; n; i++) &#123; readMethod3(builder.getInputStream()); &#125; long time6 = System.currentTimeMillis(); System.out.println(&quot;method3，耗时：&quot; + (time6 - time5) + &quot; InputStreamReader+char[8192]+StringWriter&quot;); long time7 = System.currentTimeMillis(); for (int i = 0; i &lt; n; i++) &#123; readMethod4(builder.getInputStream()); &#125; long time8 = System.currentTimeMillis(); System.out.println(&quot;method4，耗时：&quot; + (time8 - time7) + &quot; InputStreamReader+BufferedReader(char[8192])&quot;); long time9 = System.currentTimeMillis(); for (int i = 0; i &lt; 100; i++) &#123; readMethod5(builder.getInputStream()); &#125; long time10 = System.currentTimeMillis(); System.out.println(&quot;method5，耗时：&quot; + (time10 - time9) + &quot; bufferedReader+stream&quot;); long time11 = System.currentTimeMillis(); for (int i = 0; i &lt; n; i++) &#123; readMethod6(builder.getInputStream()); &#125; long time12 = System.currentTimeMillis(); System.out.println(&quot;method6，耗时：&quot; + (time12 - time11) + &quot; bufferedReader+stream+parallel&quot;); long time13 = System.currentTimeMillis(); for (int i = 0; i &lt; n; i++) &#123; readMethod7(builder.getInputStream()); &#125; long time14 = System.currentTimeMillis(); System.out.println(&quot;method7，耗时：&quot; + (time14 - time13) + &quot; Deque&lt;byte[8196]&gt;&quot;); long time15 = System.currentTimeMillis(); for (int i = 0; i &lt; n; i++) &#123; readMethod8(builder.getInputStream()); &#125; long time16 = System.currentTimeMillis(); System.out.println(&quot;method8，耗时：&quot; + (time16 - time15) + &quot; ByteBuffer(2048)+LineBuffer&quot;); &#125; /** * 一次性全部读取 * 不建议使用 */ public static String readMethod1(InputStream inputStream) throws Exception &#123; byte[] bytes = new byte[inputStream.available()]; int size = inputStream.read(bytes); String s = new String(bytes, 0, size, StandardCharsets.UTF_8);// System.out.println(s.length()); inputStream.close(); return s; &#125; /** * 使用ByteArrayOutputStream+自定义缓冲区，缓冲区大小可以依据文件大小而定 * 本质：ByteArrayOutputStream在write数据时，会检测容量是否满足需求，若不满足需求则会扩容，直到InputStream读取完毕 * 最佳实践：可以使用new ByteArrayOutputStream(inputStream.available()); 这样可以避免扩容时产生的时间损耗；同时按照大小调整缓冲区大小。 */ public static String readMethod2(InputStream inputStream)throws Exception &#123; ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(inputStream.available()); byte[] buffer = new byte[1024 * 1024]; int len = 0; while ((len = inputStream.read(buffer)) &gt; 0) &#123; byteArrayOutputStream.write(buffer, 0, len); &#125; String s = byteArrayOutputStream.toString(StandardCharsets.UTF_8);// System.out.println(s.length()); byteArrayOutputStream.close(); inputStream.close(); return s; &#125; /** * 使用StringWriter+org.apache.commons.io.IOUtils.copy * 本质：该copy方法使用的InputStreamReader，每次读取char[8192]作为缓冲区，然后while循环写入StringBuffer * InputStreamReader是将字节流按照编码转换为字符流，read方法是按编码来读取字符，而不是读取字节。 * StringWriter底层是StringBuffer，StringBuffer底层还是byte[]，若超过初始设定的长度，则进行扩容 * 关键代码：AbstractStringBuilder的683行append(char str[], int offset, int len)方法 */ public static String readMethod3(InputStream inputStream) throws Exception &#123; StringWriter writer = new StringWriter(); IOUtils.copy(inputStream, writer, StandardCharsets.UTF_8); String s = writer.toString();// System.out.println(s.length()); writer.close(); inputStream.close(); return s; &#125; /** * 使用BufferedReader+while * 本质：BufferedReader是建立再InputStreamReader之上，读取char[8192]作为缓冲区 * readLine()方法则是将缓冲区上的字符按换行符处理成一行字符串后返回，若缓冲区读完了还没有换行符则继续读取下一批char[8192] * BufferedReader.readLine()适用于一行一行，并有后续操作的需求，而不是读取整个文件到字符串中 */ public static String readMethod4(InputStream inputStream) throws Exception &#123; InputStreamReader reader = new InputStreamReader(inputStream); BufferedReader bufferedReader = new BufferedReader(reader); String s; StringBuilder sb = new StringBuilder(); while ((s = bufferedReader.readLine()) != null) &#123; sb.append(s).append(&quot;\\n&quot;); &#125; s = sb.toString();// System.out.println(s.length()); bufferedReader.close(); reader.close(); inputStream.close(); return s; &#125; /** * 使用bufferedReader+stream * 本质：lines()方法返回一个Stream，该流的数据由迭代器生成，迭代器方法还是readList() */ public static String readMethod5(InputStream inputStream) throws Exception &#123; InputStreamReader reader = new InputStreamReader(inputStream); BufferedReader bufferedReader = new BufferedReader(reader); String s = bufferedReader.lines().collect(Collectors.joining(System.lineSeparator()));// System.out.println(s.length()); bufferedReader.close(); reader.close(); inputStream.close(); return s; &#125; /** * 使用bufferedReader+stream+parallel * 同上，只是使用parallel并行计算 */ public static String readMethod6(InputStream inputStream) throws Exception &#123; InputStreamReader reader = new InputStreamReader(inputStream); BufferedReader bufferedReader = new BufferedReader(reader); String s = bufferedReader.lines().parallel().collect(Collectors.joining(System.lineSeparator()));// System.out.println(s.length()); bufferedReader.close(); reader.close(); inputStream.close(); return s; &#125; /** * 使用google的guava * 本质：独树一帜，不使用缓冲区，而是使用Deque&lt;byte[8196]&gt;作为接收byte的数据区，等全部接收完毕后，再整合成一个完整的byte[] * 注意guava 27.0版本的该方法还是beta方法，可能会存在潜在风险 */ public static String readMethod7(InputStream inputStream) throws Exception &#123; String s = new String(ByteStreams.toByteArray(inputStream), StandardCharsets.UTF_8); return s; &#125; /** * 使用google的guava的CharStreams.readLines()方法 * 本质：以ByteBuffer(2048)为缓冲区读取字符流，并使用LineBuffer作为行缓冲，底层是StringBuilder */ public static String readMethod8(InputStream inputStream) throws Exception &#123; InputStreamReader reader = new InputStreamReader(inputStream); List&lt;String&gt; stringList = CharStreams.readLines(reader); StringBuilder sb = new StringBuilder(); for (String s : stringList) &#123; sb.append(s).append(&quot;\\n&quot;); &#125; String s = sb.toString();// System.out.println(s.length()); reader.close(); inputStream.close(); return s;&#125;","categories":[{"name":"java","slug":"java","permalink":"https://yury757.github.io/categories/java/"}],"tags":[]},{"title":"Computer Networking Base","slug":"computer-science/Computer-Networking/Computer-Networking","date":"2022-04-14T16:00:00.000Z","updated":"2022-10-07T12:45:09.723Z","comments":true,"path":"computer-science/Computer-Networking/Computer-Networking/","link":"","permalink":"https://yury757.github.io/computer-science/Computer-Networking/Computer-Networking/","excerpt":"","text":"Computer Networking本文档内容主要来源于《计算机网络——自顶向下方法》一书和中科大计算机网络课程（中科大郑烇、杨坚全套《计算机网络（自顶向下方法 第7版，James F.Kurose，Keith W.Ross）》课程_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili）。 C++太难了，本次学习中的代码都以java为示例。 O、没用的东西网线：四组双绞线 1、双绞线为什么要缠绕？ 信号的传输方式是电流，电流会产生电磁场，对周围的的线路造成干扰，且干扰会随着线路的长度而增强。而在双绞线中，两根导线会传输幅值相同，极性相反的信号，这样一来，由于大小相等、极性相反，一条导线所产生的干扰就会与另一条导线所产生的干扰相互抵消，最后接收到的就是未被干扰的纯净信号。 2、网线制作标准 主要遵循EIA/TIA的568A、568B布线标准。 3、路由器工作原理 主要接收两种报文： （1）普通数据报文 （2）路由报文 4、路由表的维护方式 （1）静态路由 （2）动态路由：RIP、OSPF、BGP 5、路由器控制 可以使用网线连接路由器的CONSOLE端口到计算机上，就可以在计算机上控制路由器了。在计算机的超级终端控制路由器的命令： （1）display current-configuration（dis cu）：显示当前配置 （2）？：帮助命令，类似于linux的–help 6、网卡通常的缺省工作模式 （1）广播模式：只能接收广播报文 （2）直接模式：只能接受与自身硬件地址相匹配的单播报文 7、简单组网 192.168.2.1/24：即前24位都是固定网段，只有最后面8位是局域网可以使用的ip，而1开头是网关，因此可用的ip为254个。 8、专用网段 RFC1918规定，10网段、172.16网段、192.168网段只能作为私网（Intranet，内联网）地址（即局域网内使用），不能作为公网（Internet，互联网）地址（互联网ip地址）。 9、通过WireShark抓包 下载WireShark软件，安装一个虚拟机安装一个linux，设置固定IP地址（192.168.141.141）和默认网关等，打开WireShark找到一个VMware Network Adapter VMnet8网卡，双击或者右键start capture。此时可能会有一些数据出来，也可能没有。在shell中ping我们的虚拟机ip，就可以看到四组粉色的数据。这八条数据都可以点进去看，就是Source ip机器向Desination ip机器传输了一组数据，然后Desination ip机器回应了一组数据。这就是数据包。 1ping 192.168.141.141 10、地址转换 一个企业只有5个可以访问互联网的ip地址，可以建一个局域网，在出口路由器上配置地址转换，让所有设备共享这五个ip，则所有设备都可以访问互联网。 原理：将不同设备的ip和端口映射到可以访问互联网的ip的不同端口号上。如一个转换表如下。报文发出去时从左到右转换，封装成公网ip，报文回来时从右往左转换，转换回本机局域网ip。 本地地址 本地端口 公网地址 公网端口 10.0.1.10 1001 202.0.01 1044 10.0.1.20 1001 202.0.01 1045 多级转换。即进行ip转换的层级有多层。 一、概论1、互联网通信的层次结构 2、什么是协议？1协议 = 语法 + 语义 + 次序 + 动作 即我们要遵循某一个协议，就是指我们在传输数据的过程中，传输的数据报文要遵循一定的语法，比如前几位数字用于表示某个属性，中间几位数字表示某个属性等；还要遵循一定的语义，即在某个属性中，1代表什么，0代表什么，10又代表什么；还要遵循次序，即我发送一个数据，不能想发就发，而是我收到一个请求之后再发；最后还要遵循动作，即我收到数据报文之后，内部要做什么处理的动作。 3、TCP/IP协议簇各层的作用！！ 层级 协议 协议数据单元 作用 应用层 HTTP等 报文message 提供应用服务，用于两个进程之间的逻辑通信 传输层 TCP，UDP 报文段segment 进程到进程之间的传输，可靠。 网络层（E2E，End to End） IP 分组package 在链路层的基础上实现从源主机（Source）到目标主机（Destination）的传输。不可靠，尽力而为，可能会丢包。路由器工作在这一层 数据链路层（P2P，Point to Point） 帧frame 在一个局域网内相邻的两个结点之间传输以帧为单位的数据。交换机工作在这一层 物理层 位bit 将电信号、光信号等物理信号转换为数字信号（0、1） Media 传输媒介，如网线、光纤等 4、封装和解封装 5、多路复用和多路分解 6、C/S模式和P2P模式C/S模式是指客户/服务器模式，如web浏览器/服务器，email客户端/服务器。主要问题是可扩展性较差，请求增加时，性能下降。 P2P模式是指peer-peer模式（对等模式），每个客户端都可以作为服务器提供资源，当请求资源的主体增加时，该主体已经获取到的资源碎片可以给其他主机提供，即提供资源的主体也会相应增加，典型的比如迅雷、电驴等。 7、TCP和UDP的特性 面向连接 可靠的、按顺序地传送数据 流量控制 拥塞控制 UDP的特性和上面四个相反，具体内容后面讲。 8、计算机数据单位计算机是以二进制存储和发送接收数据的，二进制的一位（0或1），就是1bit，即一个比特。bit是计算机存储的最小单位。计算机cpu位数也是这个bit，如32位cpu计算机一次最多可以处理32个bit数据。网络传输中物理层的传输就是以bit为单位来传输的。 而Byte是指字节，是一种基本单位，但不是最小单位，一个Byte=8个bit。通常说的1KB就是指1024Byte，1MB就是指1024*1024Byte。 可以大概理解为，bit用于微观上的数据计量，而Byte用于宏观上的数据计量；bit在计算机或其他硬件层面上使用， 而Byte在实际生活和计算机软件应用层面更常见。 9、电路交换和分组交换电路交换：将传输链路的传输能力切分给不同主机使用。比如主机通过一跟光纤传输数据时，会约定好将光纤的传输能力（带宽）按一定的规则切分为30等分，每台主机只能使用其中一份传输能力。如：频分（FDM，按频段分，每台主机使用特定频段）、时分（TDM）、波分（WDM，按波长分，每台主机使用特定波长） 分组交换：将传输的数据切分一个个的package发送，数据在传输链路上使用全部带宽传输，每个网络节点收到数据后会完全存储下来，再转发给下一个网络节点。 分组交换的缺点：分组丢失、处理延迟、传播延迟、传输延迟和排队延迟。 分组丢失：当分组队列已满，新进来的分组会被丢弃，这就是分组丢失。 处理延迟：路由器设备接收到数据包并决定如何处理它所需要的时间。一般是微妙数量级或更少，依赖于处理器。 传播延迟：分组在物理介质中传播的时间； 传输延迟：每个分组需要完整接收后，存储下来，才会发送给下一个节点，因此一份分组从刚开始接受到接收完毕，存在一个传输时间的延迟； 排队延迟：有多个不同种类的数据要传给下一个节点时，需要排队，存在等待时间，取决于流量强度[0, 1)。 分组交换的优点：共享性高，按需使用。 分组交换中分组在每个网络节点之间的传输叫做“跳”。 10、虚电路网络和数据报网络虚电路网络，构建一张类似于路由表的虚电路表，根据虚电路表转发数据。（有连接，有连接和面向连接不是一个意思） 数据报网络，每个数据包都有源地址、目标地址等所有信息。（无连接） 数据报网络的特点： 传输时无需建立一个连接，有数据就传 每一个分组都独立路由（路径不一样，可能会失序） 路由器根据分组的目标地址进行路由 11、ICMP协议ICMP（Internet Control Message Protocol，互联网控制消息协议），它用于 TCP/IP 网络中发送控制消息，通过这些消息以及反馈，网络管理者可以对网络中所发生的问题进行诊断。 深入理解ICMP协议 - 知乎 (zhihu.com) 即这个协议常用于网络测试。常用的两个实现是ping和traceroute。 TTL：time to live，数据包允许存活的跳数，即数据包每经过一跳，则TTL减1，当TTL为0时，路由器会抛弃该数据包，并给源主机发送ICMP报文，说数据包因为TTL减为零，被干掉了。 RTT：round trip time，往返延迟，数据包发送出去后，收到回应的数据包的时间。 12、其他术语吞吐量：在源主机和目标主机之间传输的速率（数据量/单位时间）。 瓶颈链路：端到端路径上，限制吞吐的链路。 DU：data unit，数据单元，网络信息传输的基本单位。 SDU：service data unit，服务数据单元。 PDU：protocol data unit，协议数据单元。这两个数据单元大概意思可能是，SDU为服务层的业务数据，而PDU为对应层的协议数据。 二、应用层1、分布式进程需要解决的问题问题1：进程标识和寻址问题进程标识：ip + TCP或UDP端口号 如HTTP，采用TCP 80端口，Mail采用TCP 25端口，ftp采用TCP 2端口。 寻址：协议头指定源ip和目标ip 问题2：传输层提供的服务-层间信息的代表如果Socket Api每次传输报文，都携带上面的这么多的信息，太繁琐易出错，不便于管理。 因此用一个代号标示通信的双方或单方，即为socket，注意socket只用于本地标示！ TCP socket：TCP建立稳定连接后，可以用一个整数标识两个应用实体之间的通信关系，本地标示。TCP socket包含源IP、源端口、目标IP、目标端口。发送消息时只需要带上这个socket，操作系统就可以查到这个socket对应的源IP、源端口、目标IP、目标端口，然后按照这些内容发送对应的消息。对于网络通信而言，socket对应一个网络会话关系的本地标示。 socket 源IP 源端口 目标IP 目标端口 状态… 10 192.168.141.141 9000 192.168.141.142 9001 20 192.168.141.141 9000 192.168.141.143 9001 UDP socket：通信之间不需要建立连接，其socket仅包含源IP和源端口，但是传输报文时，必须提供目标IP和目标端口。接收报文时，传输层必须上传对方IP和对方端口。 socket 源IP 源端口 状态… 10 192.168.141.141 8000（用于发送） 20 192.168.141.141 8001（用于接收） 问题3：使用传输层提供的服务实现应用（1）规定应用层协议，公用协议（HTTP，ftp）和私有协议（Skype使用的通讯协议） （2）考虑数据丢失率、吞吐、延迟、安全性 （3）应用层协议和传输层协议对应 （4）安全性，如：HTTP+SSL=HTTPS 2、Web与HTTP简介HTTP：超文本传输协议。 url格式： 12协议名://用户名:口令@主机:端口号/路径名protocol://user:password@www.someschool.edu.cn:port/somedept/pic.gif 建立在TCP协议之上，HTTP是无状态的，即服务器不维护关于客户的任何信息，维护状态的TCP协议是很复杂的，需要考虑很多对状态有影响的情况。 HTTP1.0：非持久连接，HTTP请求步骤如下：建立TCP连接请求，确认TCP连接，发送请求数据包，解析处理，返回响应数据包，断开连接。 HTTP1.1：持久连接，HTTP请求步骤如下：建立TCP连接请求，确认TCP连接，发送请求数据包，解析处理，返回响应数据包，不断开连接，如果有其他请求可以继续使用这个连接。 HTTP请求报文格式123456get /somedir/page.html HTTP/1.1\\r\\nHost: www.someschool.edu\\r\\nUser-agent: Mozilla/4.0\\r\\nConnection: close\\r\\nAccept-language: fr\\r\\n\\r\\n HTTP/1.0支持的请求方法：GET、POST、HEAD HTTP/1.1支持的请求方法：GET、POST、HEAD、PUT、DELETE HTTP响应报文格式12345678910HTTP/1.1 200 OK\\r\\nConnection: keep-alive\\r\\nDate: Thu, 06 Aug 1998 12:00:15 GMT\\r\\nServer: Apache/1.3.0 (Unix)\\r\\nLast-Modified: Mon, 22 Jun 1998 ...\\r\\nContent-Length: 6821\\r\\nContent-Type: text/html\\r\\n\\r\\n\\r\\ndata ..data .. data .. cookie之前提到HTTP是无状态的，而cookie可以维持一定的状态，如登陆状态等。 cookie的理解如下： 客户端第一次访问服务器，服务器发现这个东西没有带cookie，是新来的 服务器给这个客户端分配一个cookie，存储在服务器，服务器响应时顺便把这个cookie带回去 客户端拿到响应html和cookie，将cookie存储在本地 客户端第二次访问服务器，就会带上这个cookie，服务器在自己这里也找到同样一个cookie，就知道这个东西之前来过我这里。 cookie的携带也会写在报文中。 cookie如果和很多信息关联在一起，容易泄露隐私。 web缓存（代理服务器）即在客户端和原始服务器之间加了一个代理服务器，客户端访问时首先会经过这个代理服务器，如果代理服务器中有这个资源，则返回这个资源，如果代理服务器没有这个资源则发给原始服务器去访问。 好处：减少资源响应时间，减少原始服务器的负载。 问题：原始服务器更新了资源怎么办？缓存服务器可以原始服务器发GET，报文中带上一个特殊的参数如下： 1If-modified-since: &lt;date&gt; 如果原始服务器的响应资源在这个时间点之后都没发生改变，则不发送这个资源，并通过响应一个特殊的状态码告诉缓存服务器，资源在这个时间点后没有发生改变。 1HTTP/1.0 304 Not Modified 3、FTPFTP协议是有状态的。 客户端发出用户名、口令等，向服务器建立TCP连接-控制连接，服务器确认连接 服务器向客户端建立TCP连接-数据连接，在数据连接上进行发送数据。 4、EMail应用EMail应用由三个组成部分： （1）用户代理 （2）邮件服务器 （3）邮件传输协议，如SMTP 流程： 用户代理通过SMTP协议，把邮件发送到发件人邮箱对应的邮件服务器中，并放到发件箱队列中 发件人邮箱对应的邮件服务器从发件箱队列中取出邮件，通过SMTP协议，发送给收件人邮箱对应的邮件服务器中，并放在一个待收取的收件箱中，发送成功。 收件人通过POP3协议，从收件人邮箱对应的邮件服务器中查询收件箱中是否有新邮件，并收取邮件。 smtp协议报文格式： 12345678To: xxxFrom: xxxSubject: xxxMIME-Version: 1.0Content-Transfer-Encoding: base64Content-Type: image/jpegbase64 encoded data ........................ 邮件访问协议，常见POP3、IMAP、HTTP 5、DNSDNS，Domain Name System，域名解析系统，即完成从域名到ip地址的解析工作的系统。 之前说过了进程之间的标示和寻址是通过ip和端口，但是ip很难记住，因此可以通过给ip设置一个比较容易记住的域名来表示这个主机的ip，而DNS服务器就是通过给定域名，查询出响应的ip地址的系统。 特点： 域名通过分层的、基于域的形式命名 通过分布式数据库完成域名到ip地址的解析。 运行在UDP之上端口号为53的应用服务 是internet的核心功能，但以应用层协议实现，处于网络边缘。 域名 ip 6、P2P应用7、CDN8、TCP套接字编程应用层要实现网络通信，要把报文传给下一层传输层去传输，而应用层和传输层之间的接口就是socket；传输层把报文段传给下一层就是他们之间的工作，我们先不用管。因此socket就像分布式应用进程之间的门，通过socket就可以实现分布式应用进程之间的逻辑通信。socket服务提供的就是字节流服务。 TCP套接字编程流程（1）启动服务器进程 创建服务器的socket 和本地ip、端口绑定 在welcome socket上阻塞式等待接收用户的连接。 （2）客户端主动和服务器建立连接 创建客户端的socket，自动绑定本地端口 指定服务器ip和端口，调用连接api，向服务器端发送连接请求。 （3）服务器接收到连接请求 服务端接收到客户端的请求，解除阻塞时等待，返回一个新的socket（和welcome socket不一样）来和客户端通信，原welcome socket重新回到阻塞式等待模式等待后面的用户连接。 服务端服务器socket类为ServerSocket，其初始化函数中有一个bind函数，即将IP地址、端口绑定到这个serversocket上。如果没有指定绑定的ip，则默认为InetAddress.anyLocalAddress()，即任意的本地地址0.0.0.0。 12345678910111213141516171819202122232425public ServerSocket(int port) throws IOException &#123; this(port, 50, null);&#125;public ServerSocket(int port, int backlog) throws IOException &#123; this(port, backlog, null);&#125;public ServerSocket(int port, int backlog, InetAddress bindAddr) throws IOException &#123; setImpl(); if (port &lt; 0 || port &gt; 0xFFFF) throw new IllegalArgumentException( &quot;Port value out of range: &quot; + port); if (backlog &lt; 1) backlog = 50; try &#123; bind(new InetSocketAddress(bindAddr, port), backlog); &#125; catch(SecurityException e) &#123; close(); throw e; &#125; catch(IOException e) &#123; close(); throw e; &#125;&#125; 创建的serversocket即为课程中的welcome socket，accept函数会阻塞，直到接收到一个连接，连接成功后会返回一个新的socket，发送和接受数据都在这个新的socket上。 123456789101112131415161718192021222324package net.yury757;import java.io.IOException;import java.net.InetAddress;import java.net.ServerSocket;import java.net.Socket;public class ServerTest01 &#123; public static void main(String[] args) throws IOException &#123; InetAddress serveraddr = InetAddress.getByName(&quot;192.168.0.101&quot;); ServerSocket serverSocket = new ServerSocket(8888, 50, serveraddr); // 这个socket为2222 int nums = 0; while (true)&#123; Socket socket = serverSocket.accept(); // 这个socket为新创建的 nums++; System.out.println(&quot;收到第&quot; + nums + &quot;个连接。目标ip地址为：&quot; + socket.getInetAddress() + &quot;，目标端口为：&quot; + socket.getPort() + &quot;。&quot;); if (nums &gt;= 10)&#123; break; &#125; socket.close(); &#125; serverSocket.close(); &#125;&#125; 服务器中的socket表，调用socket.close方法后对应的socket在表中的数据会被移除。 socket 源IP 源端口 目标IP 目标端口 状态 2222（welcome socket） 192.168.0.101 8888 3333（每次新创建的socket） 192.168.0.101 8888 192.168.0.101 10266 连接成功 4444 192.168.0.101 8888 192.168.0.101 10496 连接成功 5555 192.168.0.101 8888 192.168.0.101 10503 连接成功 客户端客户端的socket类的初始化有很多重载方法，不管那种方式创建，至少要给服务器地址和端口两个参数，如下。若不指定本机地址和端口，则本机操作系统会自动分配端口出去，Socket类创建成功后即连接上了服务器，即不需要像C/C++那样显示调用connect。 1234567891011121314package net.yury757;import java.io.IOException;import java.net.Socket;public class ClientTest01 &#123; public static void main(String[] args) throws IOException &#123; String serverHost = &quot;192.168.0.101&quot;; int port = 8888; Socket socket = new Socket(serverHost, port); // 创建后即连接成功 socket.close(); &#125;&#125; 客户端的socket表 socket 源IP 源端口 目标IP 目标端口 状态 333 192.168.0.101 10266 192.168.0.101 8888 连接成功 444 192.168.0.101 10496（自动分配） 192.168.0.101 8888 连接成功 555 192.168.0.101 10503 192.168.0.101 8888 连接成功 客户端多次启动后，服务器打印的内容如下，可以发现客户端的端口是操作系统自己分配的。 123收到第1个连接。目标ip地址为：/192.168.0.101，目标端口为：10266。收到第2个连接。目标ip地址为：/192.168.0.101，目标端口为：10496。收到第3个连接。目标ip地址为：/192.168.0.101，目标端口为：10503。 客户端服务端通信建立TCP连接后，客户端和服务端之间的通信其实就是IO操作，注意每次输出完了之后都要调用socket.shutdownOutput()方法，表明其中一方输出结束，另外一方可以开始接收消息。一个简单的通信的例子如下： 服务端： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package net.yury757;import java.io.IOException;import java.io.InputStream;import java.io.OutputStream;import java.net.InetAddress;import java.net.ServerSocket;import java.net.Socket;import java.nio.charset.StandardCharsets;public class ServerTest02 &#123; public static void main(String[] args) throws IOException &#123; InetAddress serveraddr = InetAddress.getByName(&quot;192.168.0.101&quot;); ServerSocket serverSocket = new ServerSocket(8888, 50, serveraddr); int nums = 0; while (true)&#123; Socket socket = serverSocket.accept(); nums++; // 读取客户端消息 InputStream inputStream = socket.getInputStream(); byte[] buffer = new byte[1024]; int len = 0; StringBuilder sb = new StringBuilder(); while ((len = inputStream.read(buffer)) &gt; 0)&#123; sb.append(new String(buffer, 0, len, StandardCharsets.UTF_8)); &#125; System.out.println(sb.toString()); // 向客户端回复 String reply = &quot;客户端，你好！—— reply from server(&quot; + socket.getLocalPort() + &quot;) to client(&quot; + socket.getPort() + &quot;)&quot;; OutputStream outputStream = socket.getOutputStream(); outputStream.write(reply.getBytes(StandardCharsets.UTF_8)); outputStream.flush(); socket.shutdownOutput(); // 关闭 inputStream.close(); outputStream.close(); socket.close(); if (nums &gt;= 10)&#123; break; &#125; &#125; serverSocket.close(); &#125;&#125; 客户端： 1234567891011121314151617181920212223242526272829303132333435363738package net.yury757;import java.io.IOException;import java.io.InputStream;import java.io.OutputStream;import java.net.InetAddress;import java.net.Socket;import java.nio.charset.StandardCharsets;public class ClientTest02 &#123; public static void main(String[] args) throws IOException &#123; InetAddress serverAddr = InetAddress.getByName(&quot;192.168.0.101&quot;); int port = 8888; Socket socket = new Socket(serverAddr, port); // 向服务器发送消息 OutputStream outputStream = socket.getOutputStream(); String message = &quot;服务器，你好！—— message from client whose port is &quot; + socket.getLocalPort(); outputStream.write(message.getBytes(StandardCharsets.UTF_8)); outputStream.flush(); socket.shutdownOutput(); // 读取服务器回复的消息 InputStream inputStream = socket.getInputStream(); int len = 0; byte[] buffer = new byte[1024]; StringBuilder sb = new StringBuilder(); while ((len = inputStream.read(buffer)) &gt; 0)&#123; sb.append(new String(buffer, 0, len, StandardCharsets.UTF_8)); &#125; System.out.println(sb.toString()); // 关闭资源 outputStream.close(); inputStream.close(); socket.close(); &#125;&#125;","categories":[],"tags":[]},{"title":"socket入门学习","slug":"computer-science/Computer-Networking/socket","date":"2022-04-14T16:00:00.000Z","updated":"2022-10-07T12:45:16.319Z","comments":true,"path":"computer-science/Computer-Networking/socket/","link":"","permalink":"https://yury757.github.io/computer-science/Computer-Networking/socket/","excerpt":"","text":"本学习笔记基于ubuntu18+cpp14中socket代码，学习socket。 最权威的文档：man命令。 1、socket是什么socket是一个**文件描述符（file descriptor）**，可以通过设置DOMAIN、TYPE、PROTOCOL来创建不同类型的socket。 linux将网络IO抽象为对文件的IO，socket就是网络IO的一个通道或者接口。 123456// &lt;sys/socket.h&gt;/* Create a new socket of type TYPE in domain DOMAIN, using protocol PROTOCOL. If PROTOCOL is zero, one is chosen automatically. Returns a file descriptor for the new socket, or -1 for errors. */extern int socket (int __domain, int __type, int __protocol) __THROW; （1）DOMAINDOMAIN是指地址簇，或者协议簇，表示不同类型的地址识别方式。如下所示有这么多种地址簇。 最常用的就是TCP/IP协议簇，即PF_INET，或AF_INET；以及IPV6协议簇，PF_INET6，或AF_INET6。 protocol family和address family本质上是同一种分类，只是为了适配不同的系统才形成两类宏名。在linux上我们一般用AF这一套。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899/* Protocol families. */#define PF_UNSPEC 0 /* Unspecified. */#define PF_LOCAL 1 /* Local to host (pipes and file-domain). */#define PF_UNIX PF_LOCAL /* POSIX name for PF_LOCAL. */#define PF_FILE PF_LOCAL /* Another non-standard name for PF_LOCAL. */#define PF_INET 2 /* IP protocol family. */#define PF_AX25 3 /* Amateur Radio AX.25. */#define PF_IPX 4 /* Novell Internet Protocol. */#define PF_APPLETALK 5 /* Appletalk DDP. */#define PF_NETROM 6 /* Amateur radio NetROM. */#define PF_BRIDGE 7 /* Multiprotocol bridge. */#define PF_ATMPVC 8 /* ATM PVCs. */#define PF_X25 9 /* Reserved for X.25 project. */#define PF_INET6 10 /* IP version 6. */#define PF_ROSE 11 /* Amateur Radio X.25 PLP. */#define PF_DECnet 12 /* Reserved for DECnet project. */#define PF_NETBEUI 13 /* Reserved for 802.2LLC project. */#define PF_SECURITY 14 /* Security callback pseudo AF. */#define PF_KEY 15 /* PF_KEY key management API. */#define PF_NETLINK 16#define PF_ROUTE PF_NETLINK /* Alias to emulate 4.4BSD. */#define PF_PACKET 17 /* Packet family. */#define PF_ASH 18 /* Ash. */#define PF_ECONET 19 /* Acorn Econet. */#define PF_ATMSVC 20 /* ATM SVCs. */#define PF_RDS 21 /* RDS sockets. */#define PF_SNA 22 /* Linux SNA Project */#define PF_IRDA 23 /* IRDA sockets. */#define PF_PPPOX 24 /* PPPoX sockets. */#define PF_WANPIPE 25 /* Wanpipe API sockets. */#define PF_LLC 26 /* Linux LLC. */#define PF_IB 27 /* Native InfiniBand address. */#define PF_MPLS 28 /* MPLS. */#define PF_CAN 29 /* Controller Area Network. */#define PF_TIPC 30 /* TIPC sockets. */#define PF_BLUETOOTH 31 /* Bluetooth sockets. */#define PF_IUCV 32 /* IUCV sockets. */#define PF_RXRPC 33 /* RxRPC sockets. */#define PF_ISDN 34 /* mISDN sockets. */#define PF_PHONET 35 /* Phonet sockets. */#define PF_IEEE802154 36 /* IEEE 802.15.4 sockets. */#define PF_CAIF 37 /* CAIF sockets. */#define PF_ALG 38 /* Algorithm sockets. */#define PF_NFC 39 /* NFC sockets. */#define PF_VSOCK 40 /* vSockets. */#define PF_KCM 41 /* Kernel Connection Multiplexor. */#define PF_QIPCRTR 42 /* Qualcomm IPC Router. */#define PF_SMC 43 /* SMC sockets. */#define PF_MAX 44 /* For now.. *//* Address families. */#define AF_UNSPEC PF_UNSPEC#define AF_LOCAL PF_LOCAL#define AF_UNIX PF_UNIX#define AF_FILE PF_FILE#define AF_INET PF_INET#define AF_AX25 PF_AX25#define AF_IPX PF_IPX#define AF_APPLETALK PF_APPLETALK#define AF_NETROM PF_NETROM#define AF_BRIDGE PF_BRIDGE#define AF_ATMPVC PF_ATMPVC#define AF_X25 PF_X25#define AF_INET6 PF_INET6#define AF_ROSE PF_ROSE#define AF_DECnet PF_DECnet#define AF_NETBEUI PF_NETBEUI#define AF_SECURITY PF_SECURITY#define AF_KEY PF_KEY#define AF_NETLINK PF_NETLINK#define AF_ROUTE PF_ROUTE#define AF_PACKET PF_PACKET#define AF_ASH PF_ASH#define AF_ECONET PF_ECONET#define AF_ATMSVC PF_ATMSVC#define AF_RDS PF_RDS#define AF_SNA PF_SNA#define AF_IRDA PF_IRDA#define AF_PPPOX PF_PPPOX#define AF_WANPIPE PF_WANPIPE#define AF_LLC PF_LLC#define AF_IB PF_IB#define AF_MPLS PF_MPLS#define AF_CAN PF_CAN#define AF_TIPC PF_TIPC#define AF_BLUETOOTH PF_BLUETOOTH#define AF_IUCV PF_IUCV#define AF_RXRPC PF_RXRPC#define AF_ISDN PF_ISDN#define AF_PHONET PF_PHONET#define AF_IEEE802154 PF_IEEE802154#define AF_CAIF PF_CAIF#define AF_ALG PF_ALG#define AF_NFC PF_NFC#define AF_VSOCK PF_VSOCK#define AF_KCM PF_KCM#define AF_QIPCRTR PF_QIPCRTR#define AF_SMC PF_SMC#define AF_MAX PF_MAX （2）TYPETYPE是指socket传输类型，表示不同的传输层类型，如下所示。 SOCK_STREAM，即有序的、可靠的、基于连接的字节流。（TCP） SOCK_DGRAM，面向无连接的，不可靠，固定最大长度数据包。（UDP） SOCK_RAW，原始socket数据包，需要手动解析数据包格式并作出对应处理。可以理解为TCP/UDP一种封装好了的数据包格式，而RAW则需要自己封装或解析数据包。 SOCK_PACKET，直接从网络链路层获取数据并处理数据，如MySQL、postgresql数据库连接的各种驱动就是这种网络连接方式。 123456789101112131415161718192021222324252627282930313233/* Types of sockets. */enum __socket_type&#123; SOCK_STREAM = 1, /* Sequenced, reliable, connection-based byte streams. */#define SOCK_STREAM SOCK_STREAM SOCK_DGRAM = 2, /* Connectionless, unreliable datagrams of fixed maximum length. */#define SOCK_DGRAM SOCK_DGRAM SOCK_RAW = 3, /* Raw protocol interface. */#define SOCK_RAW SOCK_RAW SOCK_RDM = 4, /* Reliably-delivered messages. */#define SOCK_RDM SOCK_RDM SOCK_SEQPACKET = 5, /* Sequenced, reliable, connection-based, datagrams of fixed maximum length. */#define SOCK_SEQPACKET SOCK_SEQPACKET SOCK_DCCP = 6, /* Datagram Congestion Control Protocol. */#define SOCK_DCCP SOCK_DCCP SOCK_PACKET = 10, /* Linux specific way of getting packets at the dev level. For writing rarp and other similar things on the user level. */#define SOCK_PACKET SOCK_PACKET /* Flags to be ORed into the type parameter of socket and socketpair and used for the flags parameter of paccept. */ SOCK_CLOEXEC = 02000000, /* Atomically set close-on-exec flag for the new descriptor(s). */#define SOCK_CLOEXEC SOCK_CLOEXEC SOCK_NONBLOCK = 00004000 /* Atomically mark descriptor(s) as non-blocking. */#define SOCK_NONBLOCK SOCK_NONBLOCK&#125;; （3）PROTOCALPROTOCAL表示最终socket通信协议类型（= 网络层协议 + 传输层协议）。基于IP协议簇包括的协议类型如下。 如果选择0，用于TCP的虚拟协议，则操作系统会自动根据前两个参数选择一种协议。若自己填，却填错了，则无法创建socket。所以一般填0即可。 IPPROTO_TCP，网络层使用IP协议，传输层使用TCP协议。 IPPROTO_TCP，网络层使用IP协议，传输层使用TCP协议。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/* Standard well-defined IP protocols. */enum &#123; IPPROTO_IP = 0, /* Dummy protocol for TCP. */#define IPPROTO_IP IPPROTO_IP IPPROTO_ICMP = 1, /* Internet Control Message Protocol. */#define IPPROTO_ICMP IPPROTO_ICMP IPPROTO_IGMP = 2, /* Internet Group Management Protocol. */#define IPPROTO_IGMP IPPROTO_IGMP IPPROTO_IPIP = 4, /* IPIP tunnels (older KA9Q tunnels use 94). */#define IPPROTO_IPIP IPPROTO_IPIP IPPROTO_TCP = 6, /* Transmission Control Protocol. */#define IPPROTO_TCP IPPROTO_TCP IPPROTO_EGP = 8, /* Exterior Gateway Protocol. */#define IPPROTO_EGP IPPROTO_EGP IPPROTO_PUP = 12, /* PUP protocol. */#define IPPROTO_PUP IPPROTO_PUP IPPROTO_UDP = 17, /* User Datagram Protocol. */#define IPPROTO_UDP IPPROTO_UDP IPPROTO_IDP = 22, /* XNS IDP protocol. */#define IPPROTO_IDP IPPROTO_IDP IPPROTO_TP = 29, /* SO Transport Protocol Class 4. */#define IPPROTO_TP IPPROTO_TP IPPROTO_DCCP = 33, /* Datagram Congestion Control Protocol. */#define IPPROTO_DCCP IPPROTO_DCCP IPPROTO_IPV6 = 41, /* IPv6 header. */#define IPPROTO_IPV6 IPPROTO_IPV6 IPPROTO_RSVP = 46, /* Reservation Protocol. */#define IPPROTO_RSVP IPPROTO_RSVP IPPROTO_GRE = 47, /* General Routing Encapsulation. */#define IPPROTO_GRE IPPROTO_GRE IPPROTO_ESP = 50, /* encapsulating security payload. */#define IPPROTO_ESP IPPROTO_ESP IPPROTO_AH = 51, /* authentication header. */#define IPPROTO_AH IPPROTO_AH IPPROTO_MTP = 92, /* Multicast Transport Protocol. */#define IPPROTO_MTP IPPROTO_MTP IPPROTO_BEETPH = 94, /* IP option pseudo header for BEET. */#define IPPROTO_BEETPH IPPROTO_BEETPH IPPROTO_ENCAP = 98, /* Encapsulation Header. */#define IPPROTO_ENCAP IPPROTO_ENCAP IPPROTO_PIM = 103, /* Protocol Independent Multicast. */#define IPPROTO_PIM IPPROTO_PIM IPPROTO_COMP = 108, /* Compression Header Protocol. */#define IPPROTO_COMP IPPROTO_COMP IPPROTO_SCTP = 132, /* Stream Control Transmission Protocol. */#define IPPROTO_SCTP IPPROTO_SCTP IPPROTO_UDPLITE = 136, /* UDP-Lite protocol. */#define IPPROTO_UDPLITE IPPROTO_UDPLITE IPPROTO_MPLS = 137, /* MPLS in IP. */#define IPPROTO_MPLS IPPROTO_MPLS IPPROTO_RAW = 255, /* Raw IP packets. */#define IPPROTO_RAW IPPROTO_RAW IPPROTO_MAX &#125;; （4）创建socket若成功创建socket，则返回大于0的数字。创建失败则返回-1。 1234567891011// 基于ipv4的TCP连接int socket1 = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);cout &lt;&lt; socket1 &lt;&lt; endl;// 基于ipv4的UDP连接int socket2 = socket(AF_INET, SOCK_DGRAM, IPPROTO_UDP);cout &lt;&lt; socket2 &lt;&lt; endl;// 基于ipv4的原始数据包int socket3 = socket(AF_INET, SOCK_RAW, 0);cout &lt;&lt; socket3 &lt;&lt; endl; 2、创建socket地址以ipv4地址为例。sockaddr_in结构体即为socket地址，三个变量分别为： SOCKADDR_COMMON (sin_)，地址簇，即创建socket的第一个参数 in_port_t sin_port，端口，是一个2字节无符号整数（unsigned short int）。 htons方法用于将unsigned short int型主机字节序的端口数字转换成网络传输需要的字节序的端口数字。 struct in_addr sin_addr，具体ip地址，是一个4字节无符号整数（unsigned int）。 htonl方法用于将unsigned int型主机字节序的端口数字转换成网络传输需要的字节序的端口数字。 常用地址如下： INADDR_LOOPBACK，本机回环地址，127.0.0.0 INADDR_ANY，接收任意收到的消息，0.0.0.0 INADDR_BROADCASE，广播地址，发动消息到任意主机，255.255.255.255 12345678910111213141516171819202122232425262728/* Structure describing an Internet socket address. */struct sockaddr_in &#123; __SOCKADDR_COMMON (sin_); in_port_t sin_port; /* Port number. */ struct in_addr sin_addr; /* Internet address. */ /* Pad to size of `struct sockaddr&#x27;. */ unsigned char sin_zero[sizeof (struct sockaddr) - __SOCKADDR_COMMON_SIZE - sizeof (in_port_t) - sizeof (struct in_addr)]; &#125;;/* Internet address. */typedef uint32_t in_addr_t;struct in_addr &#123; in_addr_t s_addr; &#125;;typedef __uint32_t uint32_t;typedef unsigned int __uint32_t;extern uint32_t htonl (uint32_t __hostlong) __THROW __attribute__ ((__const__));extern uint16_t htons (uint16_t __hostshort) __THROW __attribute__ ((__const__)); 创建socket地址： 12345678sockaddr_in addr&#123;&#125;;addr.sin_family = AF_INET;short unsigned int port = htons(3000);cout &lt;&lt; &quot;转换前的端口为：&quot; &lt;&lt; 3000 &lt;&lt; &quot;，转换后的端口为：&quot; &lt;&lt; port &lt;&lt; endl;addr.sin_port = port;unsigned long int address = htonl(INADDR_LOOPBACK);cout &lt;&lt; &quot;转换前的地址为：&quot; &lt;&lt; INADDR_LOOPBACK &lt;&lt; &quot;，转换后的地址为：&quot; &lt;&lt; address &lt;&lt; endl;addr.sin_addr.s_addr = address; 3、setsockopt该方法用于设置socket选项，可选调用，也可以不调用该方法，使用默认选项。但通常来说启动其中的“复用端口号”选项可以开启，若不开启该选项，重启server端的socket程序时，会绑定端口错误，说端口正在被使用。 1234567/* Set socket FD&#x27;s option OPTNAME at protocol level LEVEL to *OPTVAL (which is OPTLEN bytes long). Returns 0 on success, -1 for errors. */extern int setsockopt (int __fd, int __level, int __optname, const void *__optval, socklen_t __optlen) __THROW;setsockopt(welcome_socket, SOL_SOCKET, SO_REUSEADDR, &amp;enable, sizeof(int)); socket选项如下： 123456789101112131415161718192021222324#define SO_DEBUG 1#define SO_REUSEADDR 2#define SO_TYPE 3#define SO_ERROR 4#define SO_DONTROUTE 5#define SO_BROADCAST 6#define SO_SNDBUF 7#define SO_RCVBUF 8#define SO_SNDBUFFORCE 32#define SO_RCVBUFFORCE 33#define SO_KEEPALIVE 9#define SO_OOBINLINE 10#define SO_NO_CHECK 11#define SO_PRIORITY 12#define SO_LINGER 13#define SO_BSDCOMPAT 14#define SO_REUSEPORT 15#ifndef SO_PASSCRED /* powerpc only differs in these */#define SO_PASSCRED 16#define SO_PEERCRED 17#define SO_RCVLOWAT 18#define SO_SNDLOWAT 19#define SO_RCVTIMEO 20 // 接收消息的超时时间#define SO_SNDTIMEO 21 // 发送消息的超时时间 4、server socket创建socket表示网络IO的通道，服务器和客户端通信都需要创建socket，服务端接收连接，客户端发起连接。 创建服务端的socket，有以下步骤： 创建服务端welcome socket，该socket专门用于接收连接而不会从该socket读取数据。 创建服务端地址结构体sockaddr_in 绑定端口并监听 接收连接，获取connection socket，该socket代表一个连接会话，可以从该socket中读取数据。 读取字节流，处理，写入输出字节流 （1）绑定、监听将创建的socket绑定到创建的ip地址上。 其中需要将sockaddr_in强制转换为sockaddr指针再绑定。 123456789101112/* Give the socket FD the local address ADDR (which is LEN bytes long). */extern int bind (int __fd, __CONST_SOCKADDR_ARG __addr, socklen_t __len) __THROW;/* Structure describing a generic socket address. */struct sockaddr &#123; __SOCKADDR_COMMON (sa_); /* Common data: address family and length. */ char sa_data[14]; /* Address data. */ &#125;;bind(welcome_socket1, (struct sockaddr *) &amp;addr, sizeof(addr)); 开启对socket的监听，准备接受客户端连接。 N表示最大队列数，当程序已经接收到一个connection时，若还有其他客户端发起连接，则会进入队列中，当前连接处理结束后会直接从队列中取出下一个连接进行处理。超过该数字的连接将会被抛弃。 123456/* Prepare to accept connections on socket FD. N connection requests will be queued before further requests are refused. Returns 0 on success, -1 for errors. */extern int listen (int __fd, int __n) __THROW;listen(welcome_socket1, 10); （2）接收连接在已经创建好的服务端socket上阻塞等待客户端连接。 addr指针表示连接对方的地址，可以用INADDR_ANY表示任意地址，或者填如nullptr，也表示任意地址。若指定了某个地址，则只有该地址可以访问。 addr_len指针表示addr的sizeof。 当接收到一个客户端连接时，该方法就返回一个整数，代表本次会话的连接socket。 注意这个连接socket和服务端socket不一样。服务端socket只用于接收新的连接，而连接socket 123456789101112/* Await a connection on socket FD. When a connection arrives, open a new socket to communicate with it, set *ADDR (which is *ADDR_LEN bytes long) to the address of the connecting peer and *ADDR_LEN to the address&#x27;s actual length, and return the new socket&#x27;s descriptor, or -1 for errors. This function is a cancellation point and therefore not marked with __THROW. */extern int accept (int __fd, __SOCKADDR_ARG __addr, socklen_t *__restrict __addr_len);int connection_socket = accept(welcome_socket1, nullptr, nullptr); （3）读取返回从使用read方法可以从刚才得到的连接socket中读取字节流，可以用一个缓冲区接收该字节流。 经处理后，再将响应消息通过write方法，写入这个连接socket 1234567char buffer[1024] = &#123;0&#125;;read(connection_socket, buffer, sizeof(buffer));printf(&quot;%s\\n&quot;, buffer);char* response = &quot;hello world!&quot;;send(connection_socket, response, strlen(response), 0);close(welcome_socket);close(connection_socket); （4）服务端完整代码1234567891011121314151617181920212223242526272829303132333435363738394041int my_server(int port) &#123; // TCP连接 int welcome_socket = socket(AF_INET, SOCK_STREAM, 0); if (welcome_socket &lt; 0) &#123; cout &lt;&lt; &quot;socket创建失败&quot;; exit(-1); &#125; int enable = 1; setsockopt(welcome_socket, SOL_SOCKET, SO_REUSEADDR | SO_REUSEADDR, &amp;enable, sizeof(int)); // 创建地址 struct sockaddr_in addr&#123;&#125;; addr.sin_family = AF_INET; addr.sin_port = htons(port); addr.sin_addr.s_addr = htonl(INADDR_ANY); int len = sizeof(addr); // 绑定并并监听 if (bind(welcome_socket, (sockaddr*) &amp;addr, len) &lt; 0) &#123; cout &lt;&lt; &quot;绑定失败&quot;; exit(-1); &#125; // listen的第二个参数n，表示最大队列数 if (listen(welcome_socket, 10) &lt; 0) &#123; cout &lt;&lt; &quot;监听失败&quot;; exit(-1); &#125; while (true) &#123; int conn_socket = accept(welcome_socket, nullptr, nullptr); if (conn_socket &lt; 0) &#123; cout &lt;&lt; &quot;客户端连接失败&quot;; exit(-1); &#125; char buffer[1024] = &#123;0&#125;; read(conn_socket, buffer, sizeof(buffer)); printf(&quot;%s\\n&quot;, buffer); char* response = &quot;OK&quot;; send(conn_socket, response, strlen(response), 0); close(conn_socket); &#125;&#125; 5、client socket创建创建client socket和server端的socket差异不大，只是没有welcome socket和client的区分，以及连接服务端的方法不是accept，而是connect。代码如下： 1234567891011121314151617181920212223242526272829303132int my_client(char* host, int port) &#123; int client_socket = socket(AF_INET, SOCK_STREAM, 0); if (client_socket &lt; 0) &#123; cout &lt;&lt; &quot;客户端socket创建失败！&quot;; exit(-1); &#125; struct sockaddr_in serverAddr&#123;&#125;; serverAddr.sin_family = AF_INET; serverAddr.sin_port = htons(port); int res = inet_pton(AF_INET, host, &amp;serverAddr.sin_addr); if (res &lt; 0) &#123; cout &lt;&lt; &quot;解析host失败&quot;; exit(-1); &#125; res = connect(client_socket, (struct sockaddr *) &amp;serverAddr, sizeof(serverAddr)); if (res &lt; 0) &#123; cout &lt;&lt; &quot;连接服务器失败&quot;; exit(-1); &#125; char buffer[1024] = &#123;0&#125;; char* message = &quot;hello server, I&#x27;m client!&quot;; int n = send(client_socket, message, strlen(message), 0); if (n &lt; 0) &#123; cout &lt;&lt; &quot;消息发送失败&quot; &lt;&lt; endl; &#125;else &#123; cout &lt;&lt; &quot;消息已发送&quot; &lt;&lt; endl; &#125; read(client_socket, buffer, sizeof(buffer)); printf(&quot;%s&quot;, buffer); close(client_socket); return 0;&#125; 6、fork server以上说的tcp server只能处理一个连接，无法处理多个请求。想要处理多个请求，第一时间可以想到用多进程或多进线程，可以通过fork一个server进程来实现处理多请求。在以往的unix操作系统中，使用多线程的难度高于使用多进程，因此大多数c++程序员使用多进程而不是多线程来处理多任务。 代码如下。 signal(SIGCHLD, SIG_IGN);，忽略子进程的信号，子进程会交给操作系统内核来回收 fork()，可以创建一个子进程，子进程使用的资源全部是从父进程复制的来。在子进程中该方法返回0，父进程中返回子进程的pid。 这里不深入讨论fork和signal的细节。 题外话：postgres、linux上的oracle数据库的连接就是多进程的，而MySQL是多线程。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556int fork_server(int port) &#123; signal(SIGCHLD, SIG_IGN); // 忽略子进程的信号，子进程会交给操作系统内核来回收 // TCP连接 int welcome_socket = socket(AF_INET, SOCK_STREAM, 0); if (welcome_socket &lt; 0) &#123; cout &lt;&lt; &quot;socket创建失败&quot;; exit(-1); &#125; int enable = 1; setsockopt(welcome_socket, SOL_SOCKET, SO_REUSEADDR | SO_REUSEADDR, &amp;enable, sizeof(int)); // 创建地址 struct sockaddr_in addr&#123;&#125;; addr.sin_family = AF_INET; addr.sin_port = htons(port); addr.sin_addr.s_addr = htonl(INADDR_ANY); int len = sizeof(addr); // 绑定并并监听 if (bind(welcome_socket, (sockaddr*) &amp;addr, len) &lt; 0) &#123; cout &lt;&lt; &quot;绑定失败&quot;; exit(-1); &#125; // listen的第二个参数n，表示最大队列数 if (listen(welcome_socket, 10) &lt; 0) &#123; cout &lt;&lt; &quot;监听失败&quot;; exit(-1); &#125; int count = 0; while (true) &#123; int conn_socket = accept(welcome_socket, nullptr, nullptr); if (conn_socket &lt; 0) &#123; cout &lt;&lt; &quot;客户端连接失败&quot;; exit(-1); &#125; // 如果该进程是父进程，收到一个connection，只需要fork一个子进程，将连接交给子进程来处理，自己可以回去继续接收连接 if (fork() &gt; 0) &#123; continue; &#125; // 以下是子进程操作 while (true) &#123; char buffer[1024] = &#123;0&#125;; int size = read(conn_socket, buffer, sizeof(buffer)); if (size &lt;= 0) &#123; break; &#125; printf(&quot;%s %d\\n&quot;, buffer, count++); char* response = buffer; send(conn_socket, response, strlen(response), 0); &#125; close(conn_socket); // 子进程需要关闭welcome_socket，并结束进程 close(welcome_socket); return 0; &#125;&#125; 7、心跳服务端对于一个已经建立好的连接，经常会设置接收消息的超时时间，即若超过这个时间没有消息到来时，就会断开该连接。 服务端设置超时时间只需要对connectin socket设置setsockopt即可，如下。 当服务端加上以下代码时，若客户端连接上后，超过10秒没有发送消息，服务端则会自动断开该连接。 12345// 设置接收报文超时时间struct timeval tv&#123;&#125;;tv.tv_usec = 0;tv.tv_sec = 10;setsockopt(conn_socket, SOL_SOCKET, SO_RCVTIMEO, &amp;tv, sizeof(tv)); 而客户端为了保证一直保持连接状态，就需要定时向服务端发送心跳包。可以简单通过fork一个客户端进程来时间定时发送心跳包。 1234567891011if (fork() == 0) &#123; // 若为子进程，则无限循环每过5秒发送发送心跳包。 while(true) &#123; send(client_socket, &quot;0&quot;, 1, 0); char buffer[1024] = &#123;&#125;; read(client_socket, buffer, sizeof(buffer)); printf(&quot;%s&quot;, buffer); sleep(5); &#125; return 0;&#125; 同时服务端也需要对收到的消息进行额外处理。若为心跳包，则不进行业务处理，而是直接返回一个success。 12345678910// 简单设定心跳报文为一个字符0，则当服务端收到心跳报文，不进行业务处理，只返回可以只返回一个成功消息即可。if (size == 1 &amp;&amp; strcmp(buffer, &quot;0&quot;) == 0) &#123; cout &lt;&lt; &quot;heart beat!&quot; &lt;&lt; endl; char* response = &quot;success&quot;; send(conn_socket, response, strlen(response), 0); continue;&#125;else if (size &lt;= 0) &#123; close(conn_socket); break;&#125; 8、select（1）介绍前面说的server socket每次只能处理一个客户端连接，当有多个客户端尝试请求服务器时，会以队列的形式一个一个处理。因此要处理多个客户端的连接，可以使用select命令。 select命令允许监控多个socket，当这些socket都没有事件发生时，select被阻塞，当任意一个socket变为活动状态时就会返回对应socket数字，当select的返回值为-1时，则表示产生异常。 select命令使用的结构体是fd_set，即socket要放在这个结构体中，才能被监控到。该默认大小是128个字节，结构体底层使用bitmap，128个字节对应1024位，每个位代表一个socket，因此最多可以记录1024个socket情况。 12345678910111213/* fd_set for select and pselect. */typedef struct &#123; /* XPG4.2 requires this member name. Otherwise avoid the name from the global namespace. */#ifdef __USE_XOPEN __fd_mask fds_bits[__FD_SETSIZE / __NFDBITS];# define __FDS_BITS(set) ((set)-&gt;fds_bits)#else __fd_mask __fds_bits[__FD_SETSIZE / __NFDBITS];# define __FDS_BITS(set) ((set)-&gt;__fds_bits)#endif &#125; fd_set; 对fd_set数据结构的操作方法如下： 12345678910111213fd_set read_fd;// 清空fd_setFD_ZERO(&amp;read_fd); // 往fd_set中新增一个文件描述符，即将bitmap对应位置的值设置为1FD_SET(master_sock, &amp;read_fd); // 从fd_set中移除一个文件描述符，即将bitmap对应位置的值设置为0FD_CLR(master_sock, &amp;read_fd); // 如果一个文件描述符中发生了一些事件，则返回1，否则返回0。该事件可以是新连接、可读事件、可写事件等等。FD_ISSET(master_sock, &amp;readfds); select函数如下： 返回值 &gt; 1，即活跃的socket 返回值 = 0，超时 返回值 = -1，出现异常 123456789101112/* Check the first NFDS descriptors each in READFDS (if not NULL) for read readiness, in WRITEFDS (if not NULL) for write readiness, and in EXCEPTFDS (if not NULL) for exceptional conditions. If TIMEOUT is not NULL, time out after waiting the interval specified therein. Returns the number of ready descriptors, or -1 for errors. This function is a cancellation point and therefore not marked with __THROW. */extern int select (int __nfds, fd_set *__restrict __readfds, fd_set *__restrict __writefds, fd_set *__restrict __exceptfds, struct timeval *__restrict __timeout); （2）流程使用select常用轮询的方式： 1、先创建一个welcome_socket准备接收连接，创建一个数组int client_socket[30]用于存放收到的socket。 2、清空fd_set，将welcome_socket放入fd_set中监控，将client_socket中已有的连接放入fd_set中进行监控 3、先判断welcome是否有活动。若有活动，则创建新连接，并加入到client_socket数组中。 4、再循环client_socket依次判断这里面的所有socket是否有活动。若有活动则进行IO读写。若发现已经断开连接，则从client_socket中移除。 5、重复步骤2。 （3）完整实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168int select_server(int port) &#123; int opt = true; int welcome_socket , addrlen , new_socket , client_socket[30] , max_clients = 30 , activity, i , valread , sd; int max_sd, count = 0; struct sockaddr_in address &#123; &#125;; char buffer[1025]; //data buffer of 1K //set of socket descriptors fd_set readfds; //a message char *message = &quot;ECHO Daemon v1.0 \\r\\n&quot;; //initialise all client_socket[] to 0 so not checked for (i = 0; i &lt; max_clients; i++) &#123; client_socket[i] = 0; &#125; //create a master socket if( (welcome_socket = socket(AF_INET , SOCK_STREAM , 0)) == 0) &#123; perror(&quot;socket failed&quot;); exit(EXIT_FAILURE); &#125; //set master socket to allow multiple connections , //this is just a good habit, it will work without this if( setsockopt(welcome_socket, SOL_SOCKET, SO_REUSEADDR, (char *)&amp;opt, sizeof(opt)) &lt; 0 ) &#123; perror(&quot;setsockopt&quot;); exit(EXIT_FAILURE); &#125; //type of socket created address.sin_family = AF_INET; address.sin_addr.s_addr = INADDR_ANY; address.sin_port = htons( port ); //bind the socket to localhost port 8888 if (bind(welcome_socket, (struct sockaddr *)&amp;address, sizeof(address))&lt;0) &#123; perror(&quot;bind failed&quot;); exit(EXIT_FAILURE); &#125; printf(&quot;Listener on port %d \\n&quot;, port); //try to specify maximum of 3 pending connections for the master socket if (listen(welcome_socket, 3) &lt; 0) &#123; perror(&quot;listen&quot;); exit(EXIT_FAILURE); &#125; //accept the incoming connection addrlen = sizeof(address); puts(&quot;Waiting for connections ...&quot;); while(true) &#123; //clear the socket set FD_ZERO(&amp;readfds); //add master socket to set FD_SET(welcome_socket, &amp;readfds); max_sd = welcome_socket; //add child sockets to set for ( i = 0 ; i &lt; max_clients ; i++) &#123; //socket descriptor sd = client_socket[i]; //if valid socket descriptor then add to read list if(sd &gt; 0) FD_SET( sd , &amp;readfds); //highest file descriptor number, need it for the select function if(sd &gt; max_sd) max_sd = sd; &#125; //wait for an activity on one of the sockets , timeout is NULL , //so wait indefinitely activity = select( max_sd + 1 , &amp;readfds , nullptr , nullptr , nullptr); if ((activity &lt; 0) &amp;&amp; (errno!=EINTR)) &#123; printf(&quot;select error&quot;); &#125; //If something happened on the master socket , //then its an incoming connection if (FD_ISSET(welcome_socket, &amp;readfds)) &#123; if ((new_socket = accept(welcome_socket, (struct sockaddr *)&amp;address, (socklen_t*)&amp;addrlen))&lt;0) &#123; perror(&quot;accept&quot;); exit(EXIT_FAILURE); &#125; //inform user of socket number - used in send and receive commands printf(&quot;New connection , socket fd is %d , ip is : %s , port : %d\\n&quot;, new_socket , inet_ntoa(address.sin_addr) , ntohs(address.sin_port)); //send new connection greeting message if( send(new_socket, message, strlen(message), 0) != strlen(message) ) &#123; perror(&quot;send&quot;); &#125; puts(&quot;Welcome message sent successfully&quot;); //add new socket to array of sockets for (i = 0; i &lt; max_clients; i++) &#123; //if position is empty if( client_socket[i] == 0 ) &#123; client_socket[i] = new_socket; printf(&quot;Adding to list of sockets as %d\\n&quot; , i); break; &#125; &#125; &#125; //else its some IO operation on some other socket for (i = 0; i &lt; max_clients; i++) &#123; sd = client_socket[i]; if (FD_ISSET( sd , &amp;readfds)) &#123; //Check if it was for closing , and also read the //incoming message if ((valread = read( sd , buffer, 1024)) == 0) &#123; //Somebody disconnected , get his details and print getpeername(sd , (struct sockaddr*)&amp;address , \\ (socklen_t*)&amp;addrlen); printf(&quot;Host disconnected , ip %s , port %d \\n&quot; , inet_ntoa(address.sin_addr) , ntohs(address.sin_port)); //Close the socket and mark as 0 in list for reuse close( sd ); client_socket[i] = 0; &#125; //Echo back the message that came in else &#123; //set the string terminating NULL byte on the end //of the data read buffer[valread] = &#x27;r&#x27;; buffer[valread + 1] = &#x27;\\0&#x27;; printf(&quot;%s %d\\n&quot;, buffer, count++); send(sd , buffer , strlen(buffer) , 0 ); &#125; &#125; &#125; &#125; return 0;&#125; （4）优缺点优点： IO复用 适用于并发量小的场景，性能强 缺点： 支持的文件描述符只有1024。由于采用轮询的方式，因此调大该数值的意义不大。 只要连接没有关闭，即使没有活动，也会被遍历到，每次都要遍历所有socket，连接数越多性能越差。 每次循环之前需要拷贝socket （5）pselectpselect和select基本一样，在select的基础上有两处变化： timeout结构体从timeval（秒+微秒）改成了timespec（秒+纳秒） 新增了一个__sigmask信号掩码参数 嗯，不太懂。 （6）注意点 select对于没有结束的事件，在下一次轮询中还会继续活跃。比如第一次read指定长度的buffer，但是客户端发送的数据包很大，一个buffer没有读完，因此在下一次循环中该socket依然是活跃的，可以继续从socket中读取数据。 select本质上是针对文件描述符的，因此也可以多个文件的读写进行IO复用，但很少这样用。 9、pollpoll和select在本质上没有区别，也是管理多个socket然后进行轮询，根据socket的状态进行处理。 poll使用的数据结构是pollfd，一个封装了socket的机构体。 12345678910/* Data structure describing a polling request. */struct pollfd &#123; // socket int fd; /* File descriptor to poll. */ // 用户注册的需要监听的事件 short int events; /* Types of events poller cares about. */ // 实际在socket上发生的事件 short int revents; /* Types of events that actually occurred. */ &#125;; 事件共有以下几种： 123456789101112131415// 常用事件/* Event types that can be polled for. These bits may be set in `events&#x27; to indicate the interesting event types; they will appear in `revents&#x27; to indicate the status of the file descriptor. */#define POLLIN 0x001 /* There is data to read. */#define POLLPRI 0x002 /* There is urgent data to read. */#define POLLOUT 0x004 /* Writing now will not block. */// revents事件如下，revents事件不能用于events，用于表示socket状态/* Event types always implicitly polled for. These bits need not be set in `events&#x27;, but they will appear in `revents&#x27; to indicate the status of the file descriptor. */#define POLLERR 0x008 /* Error condition. */#define POLLHUP 0x010 /* Hung up. */#define POLLNVAL 0x020 /* Invalid polling request. */ poll方法从一个pollfd数组中，获取出和用户注册的事件一致的一个socket。poll方法如下： 返回值 = 1，即活跃的socket 返回值 = 0，超时 返回值 = -1，出现异常 123456789/* Poll the file descriptors described by the NFDS structures starting at FDS. If TIMEOUT is nonzero and not -1, allow TIMEOUT milliseconds for an event to occur; if TIMEOUT is -1, block until an event occurs. Returns the number of file descriptors with events, zero if timed out, or -1 for errors. This function is a cancellation point and therefore not marked with __THROW. */extern int poll (struct pollfd *__fds, nfds_t __nfds, int __timeout); poll代码和select也差不多，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586int poll_server(int port) &#123; int MAX_CONNECTION = 1024; int max_fd; struct sockaddr_in addr&#123;&#125;; int addr_len = sizeof(addr); char buffer[1024]; char response[1024] = &quot;hello&quot;; int count = 0; int welcome_socket = create_welcome_socket(port); cout &lt;&lt; &quot;welcome_socket:&quot; &lt;&lt; welcome_socket &lt;&lt; endl; struct pollfd fds[MAX_CONNECTION]; // 初始化，将fd设置为-1 // 对于poll函数，若fd为-1，则会被忽略 for (int i = 0; i &lt; MAX_CONNECTION; i++) fds[0].fd = -1; // 初始化welcome_socket fds[welcome_socket].fd = welcome_socket; // 为welcome_socket注册可读事件 fds[welcome_socket].events = POLLIN; max_fd = welcome_socket; while (true) &#123; int activity = poll(fds, max_fd + 1, -1); if (activity &lt; 0) &#123; printf(&quot;poll error!\\n&quot;); break; &#125; for (int i = 0; i &lt;= max_fd; ++i) &#123; pollfd f = fds[i]; if (f.fd == -1) continue; if (f.revents != POLLIN) continue; if (f.fd == welcome_socket) &#123; // 处理welcome_socket // (struct sockaddr *)&amp;addr, (socklen_t *)(sizeof(addr)) int new_connection = accept(welcome_socket, (struct sockaddr *)&amp;addr, (socklen_t*)&amp;addr_len); if (new_connection &lt; 0) &#123; printf(&quot;accept error!\\n&quot;); continue; &#125; cout &lt;&lt; &quot;new socket:&quot; &lt;&lt; new_connection &lt;&lt; endl; //inform user of socket number - used in send and receive commands printf(&quot;New connection , socket:%d , ip:%s , port:%d\\n&quot;, new_connection , inet_ntoa(addr.sin_addr) , ntohs(addr.sin_port)); fds[new_connection].fd = new_connection; fds[new_connection].events = POLLIN; max_fd = max(max_fd, new_connection); f.events = POLLIN; cout &lt;&lt; &quot;max_fd:&quot; &lt;&lt; max_fd &lt;&lt; endl; &#125;else &#123; cout &lt;&lt; &quot;fd:&quot; &lt;&lt; f.fd &lt;&lt; endl; if (f.revents == POLLIN) &#123; // 处理读事件 int size = read(f.fd, buffer, sizeof(buffer)); if (size &lt;= 0) &#123; getpeername(f.fd, (struct sockaddr *)&amp;addr, (socklen_t*)&amp;addr_len); printf(&quot;client disconnected, ip:%s, port:%d\\n&quot;, inet_ntoa(addr.sin_addr) , ntohs(addr.sin_port)); if (max_fd == f.fd) &#123; for (int j = max_fd - 1; j &gt; 0; --j) &#123; if (fds[j].fd != -1) &#123; max_fd = fds[j].fd; break; &#125; &#125; &#125; cout &lt;&lt; &quot;max_fd:&quot; &lt;&lt; max_fd &lt;&lt; endl; close(f.fd); f.fd = -1; &#125;else &#123; printf(&quot;接收消息%d:%s\\n&quot;, count++, buffer); buffer[size] = &#x27;\\0&#x27;; size = send(f.fd, buffer, strlen(buffer), 0); if (size &lt; 0) &#123; printf(&quot;write error!\\n&quot;); &#125; // 将该socket重新注册监听可读事件 f.events = POLLIN; &#125; &#125; &#125; &#125; &#125; return 0;&#125; poll的缺点： 没有避免拷贝socket这个环节 依然采用轮询的方式，连接数越多性能越差 10、epoll（1）简介为解决以上select和poll轮询的缺点，epoll出现了。 epoll的本质不是主动轮询找到活跃的fd，而是使用系统内核在每个fd上的回调函数实现的。即当有fd活跃时，会触发系统内核的一个callback函数，加入到Ready队列中，并通知应用程序来处理这个事件。因此应用程序只要无限循环处理这个Ready队列中的fd即可。 epoll的两种工作模式： level triggered：水平触发，即当一个fd就绪时，内核会通知你，并且直到该fd事件完全结束。假如你读取了一部分数据，还剩一部分数据没读取，内核会继续通知这个事件。select和poll都是这种工作模式。支持block和no-block socket。 edge triggered：边缘触发，即当一个fd就绪时，内核会通知你，直到你做了某些操作导致这个fd不再时就绪状态，比如读取了一半数据的fd，并不是就绪状态，后面内核就不再通知这个事件了。除非下一次IO又开始了。只支持non-block socket。这种方式会导致代码变得复杂，并且容易丢失数据。 man文档是这样举例的： 当一个IO事件如下时，若该fd注册到epoll中使用的是edge triggered时，下面第5步不会是就绪状态，而是阻塞状态。尽管在fd中还有可用数据。同时客户端会一直等待服务器针对这次IO的响应消息。但如果注册到epoll中使用的是level triggered时，第5步则会是就绪状态，即内核还会继续通知应用程序该fd事件。 1.将一个read fd注册到epoll实例上 2.read fd对应的客户端发送一个2kb的数据包给read fd 3.epoll_wait就绪（解除阻塞）并且返回对应的fd给应用程序 4.应用程序只读取了1kb的数据 5.由于事件未结束，因此下一次调用epoll_wait函数时也是就绪状态。 （2）epoll使用的结构体和系统调用结构体： epoll_event：epoll适用的事件结构体，events属性表示具体事件类型，data又是一个结构体，fd就是该事件对应的socket 函数： epoll_create：创建一个epoll实例，需要将其他socket与之关联在一起 epoll_create1：同epoll_create，去掉了size参数，加上了flags参数 epoll_ctl：对epoll_fd进行操作，op参数为操作类型，可以新增、删除、修改一个普通fd的绑定关系。 epoll_wait：等待与epoll_fd绑定的fd上的事件，当有事件发生时解除阻塞，并返回事件数量 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950struct epoll_event&#123; uint32_t events; /* Epoll events */ epoll_data_t data; /* User data variable */&#125; __EPOLL_PACKED;typedef union epoll_data&#123; void *ptr; int fd; uint32_t u32; uint64_t u64;&#125; epoll_data_t;/* Creates an epoll instance. Returns an fd for the new instance. The &quot;size&quot; parameter is a hint specifying the number of file descriptors to be associated with the new instance. The fd returned by epoll_create() should be closed with close(). */extern int epoll_create (int __size) __THROW;/* Same as epoll_create but with an FLAGS parameter. The unused SIZE parameter has been dropped. */extern int epoll_create1 (int __flags) __THROW;/* Manipulate an epoll instance &quot;epfd&quot;. Returns 0 in case of success, -1 in case of error ( the &quot;errno&quot; variable will contain the specific error code ) The &quot;op&quot; parameter is one of the EPOLL_CTL_* constants defined above. The &quot;fd&quot; parameter is the target of the operation. The &quot;event&quot; parameter describes which events the caller is interested in and any associated user data. */extern int epoll_ctl (int __epfd, int __op, int __fd, struct epoll_event *__event) __THROW;/* Valid opcodes ( &quot;op&quot; parameter ) to issue to epoll_ctl(). */#define EPOLL_CTL_ADD 1 /* Add a file descriptor to the interface. */#define EPOLL_CTL_DEL 2 /* Remove a file descriptor from the interface. */#define EPOLL_CTL_MOD 3 /* Change file descriptor epoll_event structure. *//* Wait for events on an epoll instance &quot;epfd&quot;. Returns the number of triggered events returned in &quot;events&quot; buffer. Or -1 in case of error with the &quot;errno&quot; variable set to the specific error code. The &quot;events&quot; parameter is a buffer that will contain triggered events. The &quot;maxevents&quot; is the maximum number of events to be returned ( usually size of &quot;events&quot; ). The &quot;timeout&quot; parameter specifies the maximum wait time in milliseconds (-1 == infinite). This function is a cancellation point and therefore not marked with __THROW. */extern int epoll_wait (int __epfd, struct epoll_event *__events, int __maxevents, int __timeout); （3）epoll代码流程1、创建welcome_socket，将welcome_socket设置为非阻塞 2、使用epoll_create1方法创建一个epoll_fd 3、将welcome_socket和epoll_fd绑定 4、调用epoll_wait方法，获取对应的时间数量nfds 5、0-nfds循环，对事件进行处理，若为welcome_socket则获取连接socket，设置为非阻塞，绑定epoll_fd；若为连接socket则执行IO （4）完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122static int make_socket_non_blocking (int sfd)&#123; int flags, s; flags = fcntl (sfd, F_GETFL, 0); if (flags == -1) &#123; perror (&quot;fcntl&quot;); return -1; &#125; flags |= O_NONBLOCK; s = fcntl (sfd, F_SETFL, flags); if (s == -1) &#123; perror (&quot;fcntl&quot;); return -1; &#125; return 0;&#125;int epoll_server(int port) &#123; int MAX_CONNECTION = 10; struct epoll_event ev&#123;&#125;; struct epoll_event *events; int conn_sock, nfds, epollfd; struct sockaddr_in addr&#123;&#125;; int addr_len = sizeof(addr); int count = 0; events = static_cast&lt;epoll_event *&gt;(calloc(MAX_CONNECTION, sizeof(ev))); int welcome_socket = create_welcome_socket(port); int s = make_socket_non_blocking(welcome_socket); if (s == -1) abort(); epollfd = epoll_create1(0); if (epollfd == -1) &#123; perror(&quot;epoll_create1&quot;); exit(EXIT_FAILURE); &#125; ev.events = EPOLLIN; ev.data.fd = welcome_socket; if (epoll_ctl(epollfd, EPOLL_CTL_ADD, welcome_socket, &amp;ev) == -1) &#123; perror(&quot;epoll_ctl: listen_sock&quot;); exit(EXIT_FAILURE); &#125; for (;;) &#123; nfds = epoll_wait(epollfd, events, MAX_CONNECTION, -1); if (nfds == -1) &#123; perror(&quot;epoll_wait&quot;); exit(EXIT_FAILURE); &#125; for (int i = 0; i &lt; nfds; ++i) &#123; if (events[i].data.fd == welcome_socket) &#123; conn_sock = accept(welcome_socket, (struct sockaddr *) &amp;addr, (socklen_t *)&amp;addr_len); if (conn_sock == -1) &#123; perror(&quot;accept&quot;); exit(EXIT_FAILURE); &#125; make_socket_non_blocking(conn_sock); ev.events = EPOLLIN | EPOLLET; ev.data.fd = conn_sock; if (epoll_ctl(epollfd, EPOLL_CTL_ADD, conn_sock, &amp;ev) == -1) &#123; perror(&quot;epoll_ctl: conn_sock&quot;); exit(EXIT_FAILURE); &#125; &#125; else &#123; int done = 0; while (true) &#123; ssize_t size; char *buffer[1024]; getpeername(events[i].data.fd, (struct sockaddr *)&amp;addr, (socklen_t*)&amp;addr_len); size = read (events[i].data.fd, buffer, sizeof(buffer)); if (size == -1) &#123; /* If errno == EAGAIN, that means we have read all data. So go back to the main loop. */ if (errno != EAGAIN) &#123; perror (&quot;read&quot;); done = 1; &#125; break; &#125; else if (size == 0) &#123; /* End of file. The remote has closed the connection. */ done = 1; break; &#125; printf(&quot;收到消息[%d] [ip:%s] [port:%d] - %s\\n&quot;, count++, inet_ntoa(addr.sin_addr) , ntohs(addr.sin_port), buffer); /* Write the buffer to standard output */ s = send(events[i].data.fd, buffer, size, 0); if (s == -1) &#123; perror (&quot;write&quot;); abort (); &#125; &#125; if (done) &#123; printf (&quot;Closed connection on descriptor %d\\n&quot;, events[i].data.fd); /* Closing the descriptor will make epoll remove it from the set of descriptors which are monitored. */ close (events[i].data.fd); &#125; &#125; &#125; &#125;&#125;","categories":[],"tags":[]},{"title":"常用组件端口号","slug":"常用组件端口号","date":"2022-04-04T16:00:00.000Z","updated":"2022-10-07T12:59:03.598Z","comments":true,"path":"常用组件端口号/","link":"","permalink":"https://yury757.github.io/%E5%B8%B8%E7%94%A8%E7%BB%84%E4%BB%B6%E7%AB%AF%E5%8F%A3%E5%8F%B7/","excerpt":"","text":"常用组件端口号大数据zookeeper2181：客户端连接 2888：follower与leader的rpc通信 3888：选举 hadoop9870：web管理页面 8088：yarn的web管理页面 9000：hdfs客户端连接，hdfs://myubuntu1:9000 hbase16010：web管理页面 8080：REST web服务 kafka9092：客户端连接 flink8081：web管理页面 数据库MySQL3306：客户端连接 postgresql5432：客户端连接 clickhouse8123：http访问数据库端口，java程序、数据库IDE连接clickhouse时用这个 9003：tcp访问数据库端口，clickhouse-client客户端访问时使用的端口 9004：使用MySQL客户端连接clickhouse数据库时使用的端口 9009：replica之间通信使用的端口 9100：gRPC协议端口 服务器运维grafana3000：web页面访问 账户密码默认都是admin prometheus9090：web访问端口 9100：node_exporter的指标web访问端口，需要独立额外启动node_exporter","categories":[{"name":"other","slug":"other","permalink":"https://yury757.github.io/categories/other/"}],"tags":[]},{"title":"java常用配置","slug":"java/java常用配置","date":"2021-12-24T16:00:00.000Z","updated":"2022-08-09T13:42:00.552Z","comments":true,"path":"java/java常用配置/","link":"","permalink":"https://yury757.github.io/java/java%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/","excerpt":"","text":"1、pom123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110&lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.report.outputEncoding&gt;UTF-8&lt;/project.report.outputEncoding&gt;&lt;/properties&gt;&lt;build&gt; &lt;plugins&gt; &lt;!-- 将dependencies打包到jar包中 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;finalName&gt;$&#123;project.artifactId&#125;-jar-with-dependencies&lt;/finalName&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- jar包插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;!-- 以下文件不打包 --&gt; &lt;exclude&gt;*.properties&lt;/exclude&gt; &lt;exclude&gt;*.xml&lt;/exclude&gt; &lt;exclude&gt;*.yml&lt;/exclude&gt; &lt;exclude&gt;*.yaml&lt;/exclude&gt; &lt;exclude&gt;*.config&lt;/exclude&gt; &lt;/excludes&gt; &lt;archive&gt; &lt;manifest&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;!-- 是否要把依赖jar包加入到manifest中 --&gt; &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt; &lt;!-- 指定依赖jar包从classpath下指定路径查找 --&gt; &lt;mainClass&gt;net.yury.Test&lt;/mainClass&gt; &lt;!-- 指定main方法所在类 --&gt; &lt;/manifest&gt; &lt;manifestEntries&gt; &lt;Class-Path&gt;config/&lt;/Class-Path&gt; &lt;!-- 指定配置文件从classpath下的指定路径查找 --&gt; &lt;/manifestEntries&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- 下面两个插件配置，可以执行一次，将所有jar包和配置文件拿到，部署到服务器后，就可以删掉了 --&gt; &lt;!-- 将依赖jar包从maven仓库复制到指定目录的插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-lib&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;target/lib&lt;/outputDirectory&gt; &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt; &lt;stripVersion&gt;false&lt;/stripVersion&gt; &lt;includeScope&gt;runtime&lt;/includeScope&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- 将配置文件从类文件中复制到指定目录的插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-config-file&lt;/id&gt; &lt;phase&gt;process-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-resources&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;$&#123;basedir&#125;/target/config/&lt;/outputDirectory&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;$&#123;basedir&#125;/src/main/resources/&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;*.properties&lt;/include&gt; &lt;include&gt;*.xml&lt;/include&gt; &lt;include&gt;*.yml&lt;/include&gt; &lt;include&gt;*.yaml&lt;/include&gt; &lt;include&gt;*.config&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 2、springboot123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102server: port: 8888spring: resources: static-locations: classpath:/static,classpath:/public,classpath:/resources,classpath:/META-INF/resources mvc: static-path-pattern: /resources/*.html datasource: # database driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/flinkcdc?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=true&amp;serverTimezone=GMT%2B8&amp;rewriteBatchedStatements=true username: root password: root type: com.alibaba.druid.pool.DruidDataSource # druid druid: max-wait: 60000 # 最大等待时间，配置获取连接等待超时，时间单位都是毫秒ms max-active: 3 # 最大活跃连接 min-idle: 1 # 最小空闲连接 initial-size: 1 # 初始化大小 min-evictable-idle-time-millis: 60000 # 配置一个连接在池中最小生存的时间 time-between-eviction-runs-millis: 300000 # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接 test-on-borrow: false test-on-return: false test-while-idle: true pool-prepared-statements: true max-pool-prepared-statement-per-connection-size: 20 # 最大PSCache连接 use-global-data-source-stat: true connection-properties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=500 # 通过connectProperties属性来打开mergeSql功能；慢SQL记录 filter: # 配置监控统计拦截的filters，去掉后监控界面sql无法统计 stat: enabled: true wall: # wall用于防火墙 enabled: true log4j2: enabled: true web-stat-filter: # 配置StatFilter enabled: true # 默认为false，设置为true启动 exclusions: &quot;*.js,*.gif,*.jpg,*.bmp,*.png,*.css,*.ico,/druid/*&quot; stat-view-servlet: # 配置StatViewServlet enabled: true url-pattern: &quot;/druid/*&quot; allow: localhost # ip白名单 deny: 192.168.141.141 # ip黑名单 login-username: root # 账号密码 login-password: root reset-enable: true # 是否可以重置 kafka: bootstrap-servers: 192.168.141.141:9092,192.168.141.142:9092,192.168.141.143:9092 producer: retries: 3 # 发送失败时，重试的次数 batch-size: 16384 # 批量发送时的大小，byte，默认16k buffer-memory: 33554432 # 缓冲区大小，byte，默认32M acks: 1 # 生产者确定服务器接收消息的策略 consumer: group-id: default-group # 组id enable-auto-commit: false # 自动提交 auto-offset-reset: earliest max-poll-records: 10 # 批量提取提取时，一次性提取的大小# mybatismybatis: mapper-locations: classpath:mybatis/mapper/*.xml type-aliases-package: net.yury.pojo# redisspring: redis: host: 127.0.0.1 port: 6379 password: root # 连接超时时间 timeout: 10s lettuce: pool: min-idle: 0 max-idle: 8 max-active: 8 # 连接池最大阻塞等待时间（使用负值表示没有限制） max-wait: -1m---# 开发环境spring: profiles: devmysql: ipPort: localhost:3306---# 测试环境spring: profiles: testmysql: ipPort: 192.168.141.141:3306---# 生产环境spring: profiles: prodmysql: ipPort: xxxx:xxxx# 启动时指定对应环境：java -jar test.jar --spring.profiles.active=test 2、spring-mybatis12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/tx https://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop https://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/mvc https://www.springframework.org/schema/mvc/spring-mvc.xsd&quot;&gt; &lt;!-- 1.关联数据库配置文件 --&gt; &lt;context:property-placeholder location=&quot;classpath:database.properties&quot;/&gt; &lt;!-- 2.配置数据库连接池 --&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot; destroy-method=&quot;close&quot;&gt; &lt;!-- 数据库基本信息配置 --&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;jdbc.driver&#125;&quot; /&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot; /&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot; /&gt; &lt;property name=&quot;filters&quot; value=&quot;$&#123;filters&#125;&quot; /&gt; &lt;!-- 最大并发连接数 --&gt; &lt;property name=&quot;maxActive&quot; value=&quot;$&#123;maxActive&#125;&quot; /&gt; &lt;!-- 初始化连接数量 --&gt; &lt;property name=&quot;initialSize&quot; value=&quot;$&#123;initialSize&#125;&quot; /&gt; &lt;!-- 配置获取连接等待超时的时间 --&gt; &lt;property name=&quot;maxWait&quot; value=&quot;$&#123;maxWait&#125;&quot; /&gt; &lt;!-- 最小空闲连接数 --&gt; &lt;property name=&quot;minIdle&quot; value=&quot;$&#123;minIdle&#125;&quot; /&gt; &lt;!-- 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒 --&gt; &lt;property name=&quot;timeBetweenEvictionRunsMillis&quot; value=&quot;$&#123;timeBetweenEvictionRunsMillis&#125;&quot; /&gt; &lt;!-- 配置一个连接在池中最小生存的时间，单位是毫秒 --&gt; &lt;property name=&quot;minEvictableIdleTimeMillis&quot; value=&quot;$&#123;minEvictableIdleTimeMillis&#125;&quot; /&gt; &lt;property name=&quot;validationQuery&quot; value=&quot;$&#123;validationQuery&#125;&quot; /&gt; &lt;property name=&quot;testWhileIdle&quot; value=&quot;$&#123;testWhileIdle&#125;&quot; /&gt; &lt;property name=&quot;testOnBorrow&quot; value=&quot;$&#123;testOnBorrow&#125;&quot; /&gt; &lt;property name=&quot;testOnReturn&quot; value=&quot;$&#123;testOnReturn&#125;&quot; /&gt; &lt;property name=&quot;maxOpenPreparedStatements&quot; value=&quot;$&#123;maxOpenPreparedStatements&#125;&quot; /&gt; &lt;!-- 打开removeAbandoned功能 --&gt; &lt;property name=&quot;removeAbandoned&quot; value=&quot;$&#123;removeAbandoned&#125;&quot; /&gt; &lt;!-- 1800秒，也就是30分钟 --&gt; &lt;property name=&quot;removeAbandonedTimeout&quot; value=&quot;$&#123;removeAbandonedTimeout&#125;&quot; /&gt; &lt;!-- 关闭abanded连接时输出错误日志 --&gt; &lt;property name=&quot;logAbandoned&quot; value=&quot;$&#123;logAbandoned&#125;&quot; /&gt; &lt;/bean&gt; &lt;!-- 3.配置sqlSessionFactory --&gt; &lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:mybatis-config.xml&quot;/&gt; &lt;/bean&gt; &lt;!-- 4.配置dao接口扫描，动态生成接口的实现类（动态代理自动生成，不用自己写实现类），并注入IOC容器中 --&gt; &lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;!-- 注入sqlSessionFactory，这里不能用ref，因为他要注入的是一个string，而不是一个sqlSessionFactory对象 --&gt; &lt;property name=&quot;sqlSessionFactoryBeanName&quot; value=&quot;sqlSessionFactory&quot;/&gt; &lt;!-- 要扫描的包 --&gt; &lt;property name=&quot;basePackage&quot; value=&quot;net.yury.dao&quot;/&gt; &lt;/bean&gt; &lt;!-- 5.声明式事务管理 --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!-- 6.结合AOP实现事务织入 --&gt; &lt;!-- 配置事务通知的的类：需要导入tx命名空间 --&gt; &lt;tx:advice id=&quot;txAdvisor&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;!-- name：给哪些方法配置事务，propagation：配置事务的传播特性 --&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;add*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;delete*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;update*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;select*&quot; read-only=&quot;true&quot;/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- 7.配置事务的切入点 --&gt; &lt;aop:config&gt; &lt;aop:pointcut id=&quot;txPointCut&quot; expression=&quot;execution(* net.yury.dao.*.*(..))&quot;/&gt; &lt;aop:advisor advice-ref=&quot;txAdvisor&quot; pointcut-ref=&quot;txPointCut&quot;/&gt; &lt;/aop:config&gt;&lt;/beans&gt; 4、spring-mvc123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/tx https://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop https://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/mvc https://www.springframework.org/schema/mvc/spring-mvc.xsd&quot;&gt; &lt;!-- 开启映射器和适配器的注解支持 --&gt; &lt;!-- 并添加Jackson支持，它可以自动把RestController方法返回的对象封装成json字符串，并解决json乱码问题--&gt; &lt;mvc:annotation-driven&gt; &lt;mvc:message-converters&gt; &lt;bean class=&quot;org.springframework.http.converter.StringHttpMessageConverter&quot;&gt; &lt;constructor-arg value=&quot;UTF-8&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;org.springframework.http.converter.json.MappingJackson2HttpMessageConverter&quot;&gt; &lt;property name=&quot;objectMapper&quot;&gt; &lt;bean class=&quot;org.springframework.http.converter.json.Jackson2ObjectMapperFactoryBean&quot;&gt; &lt;property name=&quot;failOnEmptyBeans&quot; value=&quot;false&quot;/&gt; &lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/mvc:message-converters&gt; &lt;/mvc:annotation-driven&gt; &lt;!-- 静态资源过滤 --&gt; &lt;mvc:default-servlet-handler/&gt; &lt;!-- 扫描包 --&gt; &lt;context:component-scan base-package=&quot;net.yury.controller&quot;/&gt; &lt;context:component-scan base-package=&quot;net.yury.service&quot;/&gt; &lt;!-- 视图解析器 --&gt; &lt;bean class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/jsp/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt; &lt;!-- 登录拦截器 --&gt; &lt;mvc:interceptors&gt; &lt;mvc:interceptor&gt; &lt;mvc:mapping path=&quot;/**&quot;/&gt; &lt;bean class=&quot;net.yury.interceptor.LoginInterceptor&quot;/&gt; &lt;/mvc:interceptor&gt; &lt;/mvc:interceptors&gt; &lt;!-- 文件上传配置 --&gt; &lt;bean id=&quot;multipartResolver&quot; class=&quot;org.springframework.web.multipart.commons.CommonsMultipartResolver&quot;&gt; &lt;!-- 编码 --&gt; &lt;property name=&quot;defaultEncoding&quot; value=&quot;utf-8&quot;/&gt; &lt;!-- 最大上传大小，单位为b（字节），10485760b = 10Mb --&gt; &lt;property name=&quot;maxUploadSize&quot; value=&quot;10485760&quot;/&gt; &lt;!-- 编码 --&gt; &lt;property name=&quot;maxInMemorySize&quot; value=&quot;40960&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 5、log4j2123456789101112131415161718192021222324252627282930313233343536373839404142&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;Configuration status=&quot;info&quot; name=&quot;RoutingTest&quot;&gt; &lt;Properties&gt; &lt;Property name=&quot;logFilename&quot;&gt;D:/tmp/test.log&lt;/Property&gt; &lt;Property name=&quot;filePattern&quot;&gt;D:/tmp/test-%d&#123;MM-dd-yy&#125;-%i.log&lt;/Property&gt; &lt;Property name=&quot;logPattern&quot;&gt;%d&#123;yyyy-MM-dd HH:mm:ss&#125; [%-5p] [%t] [%l] - %m%n&lt;/Property&gt; &lt;/Properties&gt; &lt;ThresholdFilter level=&quot;info&quot;/&gt; &lt;Appenders&gt; &lt;Console name=&quot;STDOUT&quot;&gt; &lt;PatternLayout pattern=&quot;$&#123;logPattern&#125;&quot;/&gt; &lt;ThresholdFilter level=&quot;info&quot;/&gt; &lt;/Console&gt; &lt;Routing name=&quot;Routing&quot;&gt; &lt;Routes pattern=&quot;$$&#123;sd:type&#125;&quot;&gt; &lt;Route&gt; &lt;RollingFile name=&quot;Rolling-$&#123;sd:type&#125;&quot; fileName=&quot;$&#123;logFilename&#125;&quot; filePattern=&quot;$&#123;filePattern&#125;&quot;&gt; &lt;PatternLayout&gt; &lt;pattern&gt;$&#123;logPattern&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/PatternLayout&gt; &lt;policies&gt; &lt;SizeBasedTriggeringPolicy size=&quot;25 MB&quot; /&gt; &lt;TimeBasedTriggeringPolicy interval=&quot;1&quot;/&gt; &lt;/policies&gt; &lt;/RollingFile&gt; &lt;/Route&gt; &lt;Route ref=&quot;STDOUT&quot; key=&quot;Audit&quot;/&gt; &lt;/Routes&gt; &lt;/Routing&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Root level=&quot;info&quot;&gt; &lt;AppenderRef ref=&quot;Routing&quot;/&gt; &lt;AppenderRef ref=&quot;STDOUT&quot;/&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; 6、springboot-logback1234567891011121314151617181920212223242526272829&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration&gt; &lt;include resource=&quot;org/springframework/boot/logging/logback/defaults.xml&quot;/&gt; &lt;property name=&quot;LOG_PATH&quot; value=&quot;$&#123;BASE_DIR&#125;/logs&quot;/&gt; &lt;property name=&quot;LOG_FILE&quot; value=&quot;$&#123;LOG_PATH&#125;/project-name.log&quot;/&gt; &lt;property name=&quot;LOG_PATTERN&quot; value=&quot;%d&#123;yyyy-MM-dd HH:mm:ss&#125; [%-5p] [%t] [%l] - %m%n&quot;/&gt; &lt;appender name=&quot;FILE&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;LOG_PATTERN&#125;&lt;/pattern&gt; &lt;/encoder&gt; &lt;file&gt;$&#123;LOG_FILE&#125;&lt;/file&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy&quot;&gt; &lt;fileNamePattern&gt;$&#123;LOG_FILE&#125;.%d&#123;yyyy-MM-dd&#125;.%i&lt;/fileNamePattern&gt; &lt;maxFileSize&gt;20MB&lt;/maxFileSize&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;totalSizeCap&gt;1GB&lt;/totalSizeCap&gt; &lt;/rollingPolicy&gt; &lt;/appender&gt; &lt;appender name=&quot;CONSOLE&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;LOG_PATTERN&#125;&lt;/pattern&gt; &lt;charset&gt;utf8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;root level=&quot;INFO&quot;&gt; &lt;appender-ref ref=&quot;FILE&quot;/&gt; &lt;appender-ref ref=&quot;CONSOLE&quot;/&gt; &lt;/root&gt;&lt;/configuration&gt;","categories":[{"name":"java","slug":"java","permalink":"https://yury757.github.io/categories/java/"}],"tags":[]},{"title":"jetbrains系列IDE推荐设置","slug":"jetbrains系列IDE推荐设置","date":"2021-12-20T16:00:00.000Z","updated":"2022-10-07T12:58:43.455Z","comments":true,"path":"jetbrains系列IDE推荐设置/","link":"","permalink":"https://yury757.github.io/jetbrains%E7%B3%BB%E5%88%97IDE%E6%8E%A8%E8%8D%90%E8%AE%BE%E7%BD%AE/","excerpt":"","text":"jetbrains系列IDE推荐设置1、Editor - Files Encoding，修改为utf-8 2、Editor - Code Style - 对应语言，设置tab size为4个空格 3、Editor - Code Style，设置Line Separator为\\n 4、Appearance &amp; Behavior - System Settings，设置取消勾选Reopen projects on startup，并且设置在新窗口打开新项目。 5、Build, Execution, Deployment - Build Tools - Maven/Gradle，设置自定义的maven设置（仅限java）","categories":[{"name":"java","slug":"java","permalink":"https://yury757.github.io/categories/java/"}],"tags":[]},{"title":"MySQL迁移数据目录","slug":"database/mysql/MySQL迁移数据目录/MySQL迁移数据目录","date":"2021-12-10T16:00:00.000Z","updated":"2022-10-07T12:47:03.083Z","comments":true,"path":"database/mysql/MySQL迁移数据目录/MySQL迁移数据目录/","link":"","permalink":"https://yury757.github.io/database/mysql/MySQL%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE%E7%9B%AE%E5%BD%95/MySQL%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE%E7%9B%AE%E5%BD%95/","excerpt":"","text":"Linux版本：Ubuntu18.04 MySQL版本：8.0.26 1、配置文件首先要搞清楚MySQL的配置文件在哪里。 MySQL :: MySQL 8.0 Reference Manual :: 4.2.2.2 Using Option Files 大概意思是，说明了windows和Linux下MySQL读取配置文件的顺序，我们只要找到几个常用配置文件即可。 windows下的配置文件在：C:\\%PROGRAMDIR%\\MySQL\\MySQL 8.0 Server\\my.ini这个文件中。 %PROGRAMDIR%是指windows安装时的数据目录，在中文版本中的windows系统中，这个目录经常是ProgramData，而在英文版本的windows系统中，这个目录经常是Program Files。 Linux下的配置文件在：/etc/mysql/my.cnf或/etc/mysql/mysql.conf.d/mysqld.cnf（不同MySQL和Linux版本，配置文件不一样） 2、数据迁移找到配置文件后，配置文件中指定了数据目录datadir。 windows下只要将这个值修改为自己想要的新目录，然后将原数据目录复制过去即可。 Linux中，同样修改datadir的值，但是复制文件时一定要注意权限！一般要加上-p。 1cp -rp /var/lib/mysql/* /disk4/mysql/data/ 此外新目录的创建后也要将这个目录拥有者改成MySQL。 1chown -R mysql:mysql /disk4/mysql/ 最后，还要修改/etc/apparmor.d/usr.sbin.mysqld这个文件（如果有的话），这个文件限制了Linux系统中某个程序可以访问的目录。如果没有修改这个东西，则会报错： 1Can&#x27;t create test file /disk4/mysql/data/mysqld_tmp_file_case_insensitive_test.lower 在原来的旧数据目录下加上自己修改的新目录。 12/disk4/mysql/ r,/disk4/mysql/** rwk, 重启apparmor 1/etc/inid.d/apparmor restart 其中还要一个问题会导致上面说的无法在新目录下创建新文件，即启用了selinux，将这个东西关闭即可。 123456# 临时关闭selinuxsetenforce 0# 永久关闭selinuxvi /etc/selinux/configSELINUX=disabled 3、重启12345# windows 管理员模式下net restart mysql# Linuxsystemctl restart mysql Linux下如果重启失败，可以查看报错信息，上面说的配置文件中有一个log-error文件路径，报错信息就在这个文件中。","categories":[{"name":"mysql","slug":"mysql","permalink":"https://yury757.github.io/categories/mysql/"}],"tags":[]},{"title":"flinkcdc","slug":"bigdata/flinkcdc/flinkcdc","date":"2021-11-30T16:00:00.000Z","updated":"2022-10-07T15:55:07.164Z","comments":true,"path":"bigdata/flinkcdc/flinkcdc/","link":"","permalink":"https://yury757.github.io/bigdata/flinkcdc/flinkcdc/","excerpt":"","text":"一、介绍cdc，是指变动数据捕获（change data capture），即捕获数据库数据变动信息。 cdc 是一种数据集成方法，公司一般通过变动数据捕获来获取几乎实时的增量数据，并执行相应的 ETL 操作，实现低延迟的数据更新。cdc 一般也用于实时 ETL 的第一步数据抽取的过程。 flinkcdc 是指在 flink 的一组数据源连接器（data source connector），集成 cdc 并从不同数据库获取变更数据。flinkcdc 底层使用 debezium 组件作为引擎来捕获变更数据。flinkcdc 支持 MySQL、postgresql 等多个数据库以提供其变更数据的数据源。 1、文档 Welcome to Flink CDC — Flink CDC 2.0.0 documentation (ververica.github.io) 2、版本 flink：1.13.6 scale：2.12 flink-cdc：2.0.1 3、支持的数据库 Database Version MySQL Database: 5.7, 8.0.x JDBC Driver: 8.0.16 PostgreSQL Database: 9.6, 10, 11, 12 JDBC Driver: 42.2.12 4、flink 和 flinkcdc 的兼容性 Flink CDC Connector Version Flink Version 1.0.0 1.11.* 1.1.0 1.11.* 1.2.0 1.12.* 1.3.0 1.12.* 1.4.0 1.13.* 2.0.* 1.13.* 5、一个较为通用的数据库的序列化器 flink 序列化类，常用于 MySQL 和 postgresql。这里的数据类型映射可能并不完整，需要根据不同数据库做调整。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149package net.yury.serialize;import com.fasterxml.jackson.databind.JsonNode;import com.fasterxml.jackson.databind.node.ArrayNode;import com.fasterxml.jackson.databind.node.NullNode;import com.fasterxml.jackson.databind.node.ObjectNode;import com.ververica.cdc.debezium.DebeziumDeserializationSchema;import io.debezium.data.Envelope;import lombok.extern.slf4j.Slf4j;import net.yury.utils.DateUtil;import net.yury.utils.JsonUtil;import org.apache.flink.api.common.typeinfo.TypeInformation;import org.apache.flink.util.Collector;import org.apache.kafka.connect.data.*;import org.apache.kafka.connect.source.SourceRecord;import java.math.BigDecimal;import java.util.Iterator;import java.util.List;import java.util.Map;@Slf4jpublic class DBDeserialization implements DebeziumDeserializationSchema&lt;ObjectNode&gt; &#123; public static final DBDeserialization instance = new DBDeserialization(); public final static String READ = &quot;r&quot;; public final static String UPDATE = &quot;u&quot;; public final static String INSERT = &quot;c&quot;; public final static String DELETE = &quot;d&quot;; public final static String TRUNCATE = &quot;t&quot;; public final static String BEFORE = &quot;before&quot;; public final static String AFTER = &quot;after&quot;; @Override public void deserialize(SourceRecord sourceRecord, Collector&lt;ObjectNode&gt; collector) throws Exception &#123; ObjectNode node = JsonUtil.getMapper().createObjectNode(); Struct value = (Struct)sourceRecord.value(); Struct source = (Struct) value.get(&quot;source&quot;); String database = source.getString(&quot;db&quot;); String schemaName = source.getString(&quot;schema&quot;); String tableName = source.getString(&quot;table&quot;); Long modifyTime = source.getInt64(&quot;ts_ms&quot;); Envelope.Operation operation = Envelope.operationFor(sourceRecord); node.put(&quot;mt&quot;, modifyTime); // modify time node.put(&quot;db&quot;, database); // database node.put(&quot;sm&quot;, schemaName); // schemaName node.put(&quot;tb&quot;, tableName); // table name node.put(&quot;op&quot;, operation.code()); // operation ObjectNode before = parseNode(value, BEFORE); ObjectNode after = parseNode(value, AFTER); ArrayNode diffField = parseDiff(before, after, operation.code()); node.set(BEFORE, before); node.set(AFTER, after); node.set(&quot;diffField&quot;, diffField); collector.collect(node); &#125; @Override public TypeInformation&lt;ObjectNode&gt; getProducedType() &#123; return TypeInformation.of(ObjectNode.class); &#125; public static ObjectNode parseNode(Struct struct, String field) &#123; Object o = struct.get(field); if (o == null) &#123; return null; &#125; Struct v = (Struct)o; ObjectNode node = JsonUtil.getMapper().createObjectNode(); List&lt;Field&gt; fields = v.schema().fields(); for (Field f : fields) &#123; Schema.Type type = f.schema().type(); String fName = f.name(); switch (type) &#123; case INT8: node.put(fName, v.getInt8(fName)); break; case INT16: node.put(fName, v.getInt16(fName)); break; case INT32: Integer int32 = v.getInt32(fName); if (int32 == null) &#123; node.set(fName, NullNode.instance); break; &#125; if (io.debezium.time.Date.SCHEMA_NAME.equals(f.schema().name())) &#123; node.put(fName, DateUtil.formatToString(new java.util.Date(int32 * DateUtil.ONE_DAY_SEC))); &#125;else &#123; node.put(fName, int32); &#125; break; case INT64: node.put(fName, v.getInt64(fName)); break; case FLOAT32: node.put(fName, v.getFloat32(fName)); break; case FLOAT64: node.put(fName, v.getFloat64(fName)); break; case STRING: node.put(fName, v.getString(fName)); break; case BOOLEAN: node.put(fName, v.getBoolean(fName)); break; case BYTES: Object o1 = v.get(fName); if (o1 instanceof BigDecimal) &#123; node.put(fName, (BigDecimal)o1); break; &#125;else if (o1 instanceof byte[]) &#123; node.put(fName, (byte[]) o1); break; &#125;else &#123; log.error(&quot;unsupported type [&#123;&#125;], we parse them as string.&quot;, f.schema().name()); node.put(fName, String.valueOf(o1)); break; &#125; default: log.error(&quot;unsupported type [&#123;&#125;], we parse them as string.&quot;, type.getName()); node.put(fName, String.valueOf(v.get(fName))); break; &#125; &#125; return node; &#125; public static ArrayNode parseDiff(ObjectNode before, ObjectNode after, String operation) &#123; if (UPDATE.equals(operation)) &#123; ArrayNode diff = JsonUtil.getMapper().createArrayNode(); Iterator&lt;Map.Entry&lt;String, JsonNode&gt;&gt; nodeEntries = before.fields(); while (nodeEntries.hasNext()) &#123; Map.Entry&lt;String, JsonNode&gt; next = nodeEntries.next(); String f = next.getKey(); JsonNode v1 = next.getValue(); JsonNode v2 = after.get(f); if ((v1 == null &amp;&amp; v2 != null) || (v1 != null &amp;&amp; (!v1.equals(v2)))) &#123; diff.add(f); &#125; &#125; return diff; &#125; return null; &#125;&#125; 二、postgresql1、文档 Debezium connector for PostgreSQL :: Debezium Documentation 2、使用限制 pg 版本为 9.6 及以上版本 pg 设置 wal_level 为 logical pg 安装了逻辑复制解码插件。最好使用 pgoutput 插件（10+ 版本默认安装，其他版本需要额外安装），低版本的 pg 若无法使用 pgoutput，则可以尝试使用 wal2json 插件（已被 debezium 新版本弃用）。 只能作用在 primary 服务器上，即无法捕获从服务器的变动 只支持 UTF-8 字符集的服务器 3、支持的事件 DML（read、insert、update、delete、truncate），不支持捕获 DDL 事件。 4、Replica identity PostgreSQL: Documentation: 14: ALTER TABLE Replica identity：即规定需要记录到 wal 日志中的信息，用于识别被更新或者被删除的行。有以下三种模式可选。 DEFAULT，只记录主键的旧值 USING INDEX index_name，当变更数据字段包括在这个索引中，则记录旧值 FULL，记录这一行中所有字段的旧值 NOTHING，不记录任何值 以上四种模式，常用 DEFAULT 和 FULL，这两种有各自的使用场景。 当不关心旧数据具体是哪个字段变化时，可以使用 DEFAULT，需要知道具体哪个字段变化时，需要使用 FULL。 修改该配置的方法如下， 且需要对每个表进行修改。 1ALTER TABLE t1 REPLICA IDENTITY FULL; 5、stream api 使用 123456789101112131415161718192021222324252627282930package net.yury.main;import com.fasterxml.jackson.databind.node.ObjectNode;import com.ververica.cdc.connectors.postgres.PostgreSQLSource;import net.yury.serialize.DBDeserialization;import net.yury.sink.PrintSink;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.source.SourceFunction;public class Demo11PostgresCDC &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); SourceFunction&lt;ObjectNode&gt; pgSource = PostgreSQLSource.&lt;ObjectNode&gt;builder() .hostname(&quot;192.168.141.141&quot;) .port(5432) .database(&quot;postgres&quot;) .schemaList(&quot;public&quot;) .tableList(&quot;public.t1&quot;) .username(&quot;postgres&quot;) .password(&quot;postgres&quot;) .deserializer(new DBDeserialization()) .decodingPluginName(&quot;pgoutput&quot;) .build(); env.addSource(pgSource) .addSink(new PrintSink&lt;ObjectNode&gt;()) .setParallelism(1); env.execute(&quot;Print postgresql Snapshot + Binlog&quot;); &#125;&#125; 6、sql api 使用 7、复制槽 123456select * from pg_catalog.pg_replication_slots; # 复制槽状态select * from pg_catalog.pg_stat_replication; # 逻辑复制状态select * from pg_catalog.pg_replication_origin_status; # 逻辑复制数据源状态select * from pg_catalog.pg_replication_origin; # 逻辑复制数据源select pg_drop_replication_slot(&#x27;flink&#x27;); # 删除复制槽","categories":[{"name":"bigdata","slug":"bigdata","permalink":"https://yury757.github.io/categories/bigdata/"}],"tags":[]},{"title":"clickhouse","slug":"bigdata/clickhouse/clickhouse","date":"2021-11-20T16:00:00.000Z","updated":"2022-01-02T06:53:14.626Z","comments":true,"path":"bigdata/clickhouse/clickhouse/","link":"","permalink":"https://yury757.github.io/bigdata/clickhouse/clickhouse/","excerpt":"","text":"clickhouse版本：21.7.3.14 一、介绍1、简介基于列存储的数据库，使用C++编写，主要用于在线分析处理查询（OLAP），能够使用SQL查询实时生成分析报告。 列式存储的优点： 对于列的聚合、计数等统计操作由于行式数据库 由于某一列的数据类型是一样的，在数据压缩上效率更高，压缩比更大，缓存cache也有更大的发挥空间 列式存储的缺点：插入、更新速度比行式数据库更慢 2、高吞吐写入能力clickhouse采用类LSM Tree的结构，数据写入后定期在后台compation。clickhouse在导入数据时全部都是顺序append写入，写入后数据段不可更改，在后台compation时也是多个段merge sort后顺序写回磁盘。顺序写的特性，充分利用了磁盘的吞吐能力。 3、数据分区和线程级并行clickhouse将数据划分为多个partition，每个partition再进一步划分为多个index granularity（索引粒度），然后通过多个线程分别处理其中一部分来实现并行数据处理。这种设计下，单条query就可以利用整机所有的CPU资源。对于大量数据的查询也能够化整为零的并行处理。 缺点：由于一条SQL就会占用所有cpu，因此对于qps高的业务并不适合。 4、使用场景不适用于初始数据存储，而适用于最后的宽表存储，用来查询用。 适用于clickhouse的业务：具有复杂统计逻辑的查询sql，需要查询大量数据的sql，并发量低 适用于hbase的业务：业务很简单的查询，一般就是key-value一一对应的查询，并发量高 clickhouse还有一个特点就是，单表查询速度及其快，但是多表join操作比较慢，在了解join原理后，可以通过优化sql来优化join速度。 二、安装部署1、准备工作（1）ulimitlinux取消一些系统资源限制： 12345678910111213141516171819# 查看系统资源限制ulimit -a# open files (-n) 1024# max user processes (-u) 7625# 修改系统资源限制sudo vi /etc/security/limits.conf# 加上以下配置* soft nofile 65535* hard nofile 65535* soft nproc 131072* hard nproc 131072# 有些linux系统在/etc/security/limit.d目录下面还有20-npric.conf或90-npric.conf# 这两个配置会把limits.conf配置覆盖了，所以如果有的话这两个也要加上以上配置sudo vi /etc/security/limits.d/20-nproc.conf# 修改之后不需要重启，重新登录该用户即可，再查看ulimit -a是否修改成功 （2）SELINUXsentos取消SELINUX（linux的security enforce） SELINUX并不是安装里所有linux都会有，没有SELINUX不需要执行以下修改。 12345678910# 查看security enforegetenforcesudo vi /etc/selinux/config# 将SELINUX修改为disabledSELINUX=disabled# 重启永久生效，输入以下命令临时生效setenforce 0 （3）版本clickhouse版本更新比较快 20.6.3 新增explain，类似于MySQL的explain 20.8 新增同步MySQL功能等 我们使用的版本是21.7.3.14 2、下载安装下载地址：Index of /clickhouse/deb/stable/main/ 12345678910111213cd /home/yury/clickhousewget https://repo.clickhouse.com/deb/stable/main/clickhouse-common-static_21.7.3.14_amd64.debwget https://repo.clickhouse.com/deb/stable/main/clickhouse-client_21.7.3.14_all.debwget https://repo.clickhouse.com/deb/stable/main/clickhouse-server_21.7.3.14_all.deb# 下面这个包是带有调试信息的包，可以不装wget https://repo.clickhouse.com/deb/stable/main/clickhouse-common-static-dbg_21.7.3.14_amd64.debsudo dpkg -i *.deb# 一路yyy，最后要输入一个默认用户的密码，可以设置密码，也可以直接回车不设置密码# 安装成功后，软件被安装到目录下，lib目录在/var/lib/clickhouse/ 3、常用路径bin目录：/usr/bin/ 服务器安装目录：/etc/clickhouse-server/ 客户端安装目录：/etc/clickhouse-client/ 配置文件目录：/etc/clickhouse-server/，所有其他配置目录都可以在这个配置文件中设置 日志目录：/var/log/clickhouse-server/clickhouse-server.log 报错日志目录：/var/log/clickhouse-server/clickhouse-server.err.log 依赖目录：/var/lib/clickhouse/ 数据目录：/var/lib/clickhouse/，即config.xml配置文件中path标签下 临时文件目录：/var/lib/clickhouse/tmp/，即config.xml配置文件中tmp_path标签下 4、配置config.xml中的配置是服务器配置，比如上面说的数据目录、日志目录都在这个配置中，而user.xml则是程序运行参数配置，如cpu、内存在这个里面配置。 config.xml配置文档：Server Settings | ClickHouse Documentation user.xml配置文档：Settings | ClickHouse Documentation 修改一个配置，使得其他ip地址也可以访问clickhouse服务器 1234sudo vi /etc/clickhouse-server/config.xml# 将下面这个配置取消注释&lt;listen_host&gt;::&lt;/listen_host&gt; 此外再把tcp_port端口改为9003，因为默认的9000端口很容易被其他应用使用，比如hadoop就使用了9000端口。此外用，clickhosue-client连接时用tcp连接，而使用其他连接，比如datagrip或java，都是用的http连接，注意端口的使用。 1&lt;tcp_port&gt;9003&lt;/tcp_port&gt; 5、简单使用123456789101112131415161718# 启动sudo clickhouse start# 查看服务器运行状态sudo clickhouse status# /var/run/clickhouse-server/clickhouse-server.pid file exists and contains pid = 7116.# The process with pid = 7116 is running.# 关闭sudo clickhouse stop# 重启sudo clickhouse restart# 常加-m参数，这样可以在sql语句中使用换行，否则要用分号分隔# -h参数为服务器host，默认是localhost# --port参数为tcp端口，默认是9000clickhouse-client -m -h myubuntu1 --port 9003 clickhouse-client还有一个快捷方式的参数query： 123clickhouse-client -m -h 192.168.141.141 --port 9003 --query &quot;show databases;&quot;defaultsystem clickhouse语法比较像MySQL，在某些语法上clickhouse区分大小写，比如数据类型UInt、String，以及表引擎MergeTree等等。 12345678910111213show databases;use default;show tables;create table test2( id UInt32, create_time Datetime default now(), name String, age UInt8, money Decimal(24,6) TTL create_time + interval 10 SECOND)engine=MergeTreeprimary key(id)order by(id, name); 使用shell命令快速写一个批量插入100万条数据sql文件。 12345for i in &#123;1..1000&#125;; doecho -n &quot;insert into default.test2(id, name, age, money) values (1, &#x27;小明&#x27;, 14, 2000)&quot; &gt;&gt; test.sql;for j in &#123;1..1000&#125;; do echo -n &quot;,($&#123;i&#125;$&#123;j&#125;, &#x27;小明$&#123;i&#125;$&#123;j&#125;&#x27;, $&#123;i&#125;$&#123;j&#125;, $&#123;i&#125;$&#123;j&#125;2000)&quot; &gt;&gt; test.sql; done;echo &quot;;&quot; &gt;&gt; test.sql;done; 执行sql文件（速度贼快） 1clickhouse-client -m -h 192.168.141.141 --port 9003 --multiquery &lt; ./test.sql 执行几个group查询（速度贼快） 注意，这里substring也是按照字节数来算长度的。 123select substring(name, 1, 6), sum(age) from test2 group by substring(name, 1, 6);select substring(name, 7, 1), sum(age) from test2 group by substring(name, 7, 1);select id % 2 as flag, sum(age) from test2 group by (id % 2); 三、数据类型文档：Introduction | ClickHouse Documentation 1、整形（1）有符号整型 clickhouse类型 范围 对应MySQL类型 Int8 -128 : 127 (2^7) tinyint Int16 -32768 : 32767(2^15) smallint Int32 -2147483648 : 2147483647 (2^31) int Int64 -9223372036854775808 : 9223372036854775807 (2^63) bigint Int128 -2^127 : 2^127-1 无 Int256 -2^255 : 2^255-1 无 （2）无符号整形 UInt8、UInt16、UInt32、UInt64，UInt128、UInt256，范围分别是0 : (2^n)-1。 128位和256位的一般用不到。 clickhouse没有布尔类型，官方建议用UInt8来存储，0代表false，1代表true。 2、浮点型 clichouse类型 对应MySQL类型 Float32 float Float64 double 3、Decimal clickhouse类型 整数位+小数位 小数位 对应MySQL类型 Decimal(P,S) P S decimal(P,S) Decimal32(S) 9 S decimal(9,S) Decimal64(S) 18 S decimal(18,S) Decimal128(S) 38 S decimal(38,S) Decimal128(S) 76 S decimal(76,S) 不同位数长度的Decimal进行运算时，最终结果的位数长度是最大的那个。 4、字符串类型 clickhouse 字节长度 对应MySQL类型 String 无限制 varchar、所有text、所有blob FixedString(N) N char(N) FixedString(N)类型，这个N是字节长度，而不是字符长度。当存入字符的字节长度小于N时，会用空字节（\\0）补齐。一般很少用FixedString(N)，就像MySQL很少使用char(N)一样。 clickhouse没有编码的概念，即它存储字符串时是以二进制的形式存储。clickhosue在计算长度时，length函数是计算编码后字节的长度，lengthUTF8函数才是计算字符的长度，且只有一个计算UTF8编码的函数，因此服务器一定要使用utf-8编码。 clickhouse还有一个专门的存储UUID的类型：UUID，以及生成UUID的函数generateUUIDv4()。 1CREATE TABLE t_uuid (x UUID, y String) ENGINE=TinyLog 5、日期类型 clickhouse类型 字节长度 时间范围 对应MySQL类型 Date 2 1970-01-01至2148-12-31 Date32 1925-01-01至2283-11-11 DateTime([timezone]) 1970-01-01 00:00:00至2105-12-31 23:59:59 timestamp DateTime64(precision, [timezone]) 1925-01-01 00:00:00至2283-11-11 23:59:59 Interval 无，是一个时间长度含义 interval timezone是指时区，默认使用配置文件中设置的时区，或者操作系统时区。 precision是指秒后面的时间精度。 Interval的使用和MySQL的interval类型一样使用。 1234select now() + interval 4 DAY + interval 3 HOUR;┌─plus(plus(now(), toIntervalDay(4)), toIntervalHour(3))─┐│ 2021-11-26 01:15:52 │└────────────────────────────────────────────────────────┘ 6、枚举类型 clickhouse类型 枚举值个数 MySQL类型 Enum8（别名：Enum） 256 enum Enum16 65536 enum 12345CREATE TABLE t_enum( x Enum(&#x27;hello&#x27; = 1, &#x27;world&#x27; = 2))ENGINE = TinyLog clickhouse和MySQL在存储枚举值时，都是存储的对应的数值，而clickhouse是手动设置枚举值对应的数字值，MySQL则是系统设定的。 7、LowCardinality1LowCardinality(data_type) 该数据类型，将存储的值，设置字典索引，实际存储则存储其索引数字即可。 比如LowCardinality(String)类型字段，在insert了“小明”，“小红”，“小光”三个字符时，首先会创建一个字典，将这三个字符写入字典，如下： 12345&#123; 1: &quot;小明&quot;, 2: &quot;小红&quot;, 3: &quot;小光&quot;,&#125; key为索引序号，value为实际值。而实际存储在数据文件中，则是存储的其索引。 当value有很多重复值时，这种方式不仅可以节省很多存储空间，还可以加快读取速度，以及对该字段进行过滤、分组和某些查询的速度。 值得一提的是，微软office2007版本的xlsx类型文件底层的字符串格式的存储也是采用这种方式，专门使用一个sharedString.xml来存储所有字符串，作为一个字典，而在主体的存储文件中使用索引。 但是这种类型，写入速度会比一般普通类型写入慢一些，因此特别不建议在重复度很低的字段上使用这个类型。 这种类型和Enum比较类似，但是LowCadinality更加灵活，不仅可以存储String，还可以存储Date、DateTime，只要重复度很高，则可以使用这种类型。 8、Nullable(T)Nullable(T)，可为空的类型，如Nullable(Int8)是可为空值的Int8 使用Nullable会对性能产生影响，业务中可以用一些特殊字符或无意义的值来填充null，从而避免使用Nullable类型。 原因：1、Nullable会单独存一个文件；2、null无法使用索引。 注意：每种数据类型都会有自带的默认值，且不是null，如Int8的默认值是0，String的默认值是空字符串，DateTime的默认值是1970-01-01 08:00:00。如果需要为null，则需要使用Nullable(T)类型，Nullable的默认值则是null。 123456789use default;create table test1(a Int8, b String, c DateTime, d Nullable(Int8), e Nullable(String) , f Nullable(DateTime))engine=TinyLog;insert into test1 values(1, &#x27;nihao&#x27;, &#x27;2020-12-31&#x27;, 1, &#x27;nihao&#x27;, &#x27;2020-12-31&#x27;);insert into test1 values(null, null, null, null, null, null);select * from test1;┌─a─┬─b─────┬───────────────────c─┬────d─┬─e─────┬───────────────────f─┐│ 1 │ nihao │ 2020-12-31 00:00:00 │ 1 │ nihao │ 2020-12-31 00:00:00 ││ 0 │ │ 1970-01-01 08:00:00 │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │└───┴───────┴─────────────────────┴──────┴───────┴─────────────────────┘ 判断是否为null的格式如下，如果不是Nullable类型，则没有.null这个字段，会报错。 1234select * from test1 where d.null = 1;┌─a─┬─b─┬───────────────────c─┬────d─┬─e────┬────f─┐│ 0 │ │ 1970-01-01 08:00:00 │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │└───┴───┴─────────────────────┴──────┴──────┴──────┘ 9、其他类型Array(T)，数组 AggregateFunction和SimpleAggregateFunction，比较重要，可以在表引擎那里再理解 Nested，类似于c++的struct Tuple，即可以存不同数据类型的Array Expression，存储了lamdba表达式 Set，集合，不可重复 Nothing，官方解释，此数据类型的唯一目的是表示不需要值的情况，好像没啥用 IPV4，ipv4专用类型，基于UInt32存储 IPV6，ipv6专用类型 GEO，坐标类型，包括Point、Ring、Polygon、MultiPolygon Map(K, V)，映射表类型 10、特殊值此外clickhouse还有一些特殊值，如Inf、-Inf和NaN，即infinity、negative infinity和非数字类型。 1234select 1 / 0 as a, -1 / 0 as b, 0 / 0 as c;┌───a─┬────b─┬───c─┐│ inf │ -inf │ nan │└─────┴──────┴─────┘ 11、相关函数 函数名 含义 toTypeName(field) 获field字段的类型 cast(value, T) or cast(value as T) 将value强制转换为T类型 extract(part from date) 从日期中获取年、月、日 uniq(field) 取distinct 123456789select cast(&#x27;1&#x27;, &#x27;Int8&#x27;) as x, toTypeName(x) as type;┌─x─┬─type─┐│ 1 │ Int8 │└───┴──────┘select extract(DAY from now());┌─toDayOfMonth(now())─┐│ 21 │└─────────────────────┘ 四、表引擎文档：Introduction | ClickHouse Documentation 表引擎决定了： 数据存储的方式和位置，写到哪里以及从哪里读取数据 支持哪些查询，以及如何支持（有一些特殊的查询需要特殊的表引擎才能支持） 并发数据访问 索引的使用 是否可以使用多线程执行（有些引擎，在查询一条sql时会多线程执行） 数据复制参数 表引擎有集成引擎、日志系列引擎、MergeTree系列引擎和一些特殊引擎。 1、集成引擎集成引擎主要是为了继承其他组件而使用的，比如集成MySQL、jdbc、kafka等。集成的本质就是，将MySQL等源数据和clickhouse表做一层映射，然后就可以直接用clickhouse查询源数据。 2、日志系列引擎日志引擎是为了需要快速写一些小表（小于100万）而使用的引擎，就像名字一样，多用于日志存储。 如TinyLog，以列文件的形式保存在磁盘上，不支持索引，没有并发控制。一般保存少量数据，生产环境很少使用。 3、特殊引擎很多，具体见官方文档。 如Memory，内存引擎，将数据以未经压缩的形式直接存储在内存中，不支持索引，读写操作不会阻塞，简单查询性能非常高！ 4、MergeTree系列（重要！！）文档：Introduction | ClickHouse Documentation （1）MergeTree合并树这种表引擎在建表时，必须要加order by。 1234567891011121314151617181920CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2], ... INDEX index_name1 expr1 TYPE type1(...) GRANULARITY value1, INDEX index_name2 expr2 TYPE type2(...) GRANULARITY value2, ... PROJECTION projection_name_1 (SELECT &lt;COLUMN LIST EXPR&gt; [GROUP BY] [ORDER BY]), PROJECTION projection_name_2 (SELECT &lt;COLUMN LIST EXPR&gt; [GROUP BY] [ORDER BY])) ENGINE = MergeTree()ORDER BY expr[PARTITION BY expr][PRIMARY KEY expr][SAMPLE BY expr][TTL expr [DELETE|TO DISK &#x27;xxx&#x27;|TO VOLUME &#x27;xxx&#x27; [, ...] ] [WHERE conditions] [GROUP BY key_expr [SET v1 = aggr_func(v1) [, v2 = aggr_func(v2) ...]] ] ][SETTINGS name=value, ...] 表底层存储文件夹和文件介绍： 分区是按文件夹存储的，一个分区一个文件夹，文件夹的名称为分区id_最小分区块编号_最大分区块编号_合并层级 分区id生成规则： 没有设置分区，则默认生成一个all目录作为数据分区 整形分区，以该整形值的字符串形式作为分区id 日期分区，分区id为日期的yyyymmdd形式 其他分区，如String、Float等，以其128位hash值为分区id 最小分区块编号：分区的最小分区编号，适用于分区合并 最大分区块编号：分区的最大分区编号，适用于分区合并 合并层级：被合并的次数。插入数据时，并不会直接将数据插入到对应分区中，而是会生成一个临时分区，等服务器空闲或者一段时间后，再将临时分区合并到原有的分区文件中。和hbase的regionserver类似 通过手动执行optimize table xxx final可以手动进行数据合并。 目录文件内各文件介绍： bin文件：数据文件 mrk文件：标记文件，标记文件在idx索引文件和bin文件之间起到了桥梁作用，一般记录列的offset，用于加速查询。以mrk2结尾的文件，表示该表启动了自适应索引间隔。 primary.idx文件：主键索引文件，用于加快查询效率 minmax_create_time.idx：分区键的最大值最小值 checksum.txt：校验文件，用于校验各个文件的正确性。存放各个文件的size和hash值。 count.txt：记录了该表的总数据量 columns.txt：记录了该表的列信息 default_compression_codec.txt：记录了数据文件中使用的压缩编码器 partition.dat：记录了分区信息 主键索引 clickhouse的主键索引并不是唯一索引。主键索引是稀疏索引，即并不是将这一列的所有值建索引，而是一部分值，查找时通过类似于二分查找的方式确定数据所在区间，再扫描这个区间找到对应的值。 索引有一个index granularity，即索引稀疏粒度，即稀疏索引记录值时每次跳过的行数，默认值为8192。粒度越细，索引数据量存储的越大，查询效率越高；粒度越粗，索引数据量越小，查询效率越低。当很多重复值时，需要适当提高粒度。 order by（MergeTree系列引擎最重要的字段） 主键索引是稀疏索引，这种索引的查找方式需要排序，因此表必须有一个字段进行了排序，这就是为什么order by是MergeTree引擎的必选信息。主键字段必须是order by的前缀字段，原因和MySQL可以使用最左前缀字段作为索引原因一样。 二级索引（数据跳跃索引） 老版本需要设置set allow_experimental_data_skipping_indices = 1开启。 v20.1.2.4及以后，这个参数已经被删除了 二级索引的原理：以索引粒度为单位，记录每个区间的最大值和最小值，当以这个字段为条件查询时，只需要比较值是否落在这个区间就行（比较两次），若落在这个区间内，再做区间扫描，否则直接跳过。 TTL 数据过期时间，用于管理该数据的生命周期，到期删除。 有字段级别的TTL和表级别的TTL。字段级别只是删除这一个字段，表级别是删除整行数据。 （2）ReplacingMergeTree1234567891011CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ...) ENGINE = ReplacingMergeTree([ver])[PARTITION BY expr][ORDER BY expr][PRIMARY KEY expr][SAMPLE BY expr][SETTINGS name=value, ...] 完全继承了MergeTree，只是多了一个去重功能，根据order by字段进行去重。 去重要删除哪些数据：建表语句中ReplacingMergeTree([ver])的ver字段值最大的那一条数据被保留，其他数据被删除。没指定这个字段，则按插入顺序来，保留最后的一条数据，类似于upsert。 去重时机：并不是实时去重，而是在合并分区时进行去重，即保证最终一致性。 去重范围：在同一分区内去重，并不是全表内去重。 这个引擎在实际生产环境中最常用，通常ver字段都是业务中的插入时间字段，或者其他数据版本控制字段。 示例： 1234567891011121314CREATE TABLE default.test3( `id` UInt32, `name` String, `money` Decimal(18, 6))ENGINE = ReplacingMergeTreeORDER BY (id, name)SETTINGS index_granularity = 8192;insert into test3 values (1, &#x27;小明&#x27;, 1000);insert into test3 values (1, &#x27;小明&#x27;, 2000);optimize table test3;select * from test3; （3）SummingMergeTree12345678910CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ...) ENGINE = SummingMergeTree([columns])[PARTITION BY expr][ORDER BY expr][SAMPLE BY expr][SETTINGS name=value, ...] 在MergeTree的基础上，提供了“预聚合”的功能，且根据order by字段进行sum聚合。 聚合后其他字段怎么办：对columns字段进行sum，其他字段保留最早的那一条，即最早插入的那一条。 聚合范围：在同一分区内进行聚合。 聚合时机：并不是实时聚合，而是在合并分区时进行聚合。 5、ReplacatedMergeTree系列该系列表引擎用于副本备份，MergeTree系列的所有子引擎都有对应的ReplacetedMergeTree引擎，如ReplacetedReplacingMergeTree等。 五、SQL语法只介绍和标准sql不一样的地方 1、Update和Deleteclickhosue提供update和delete的功能，但没有这两个关键字，即不能直接使用delete或update语句，而是提供了一类称为Mutation的查询，是Alter的一种。 12345-- 删除一条数据alter table test2 delete where id = 10000;-- 更新一条数据alter table test2 update age = 99 where id = 1; 和普通的OLTP数据库不一样，Mutation语句是一种很重的操作，而且不支持事务。原因在于每次操作都要放弃目标数据的原有分区，重新建分区，旧分区被打上逻辑上的失效标记，只有分区合并的时候，才会删除旧分区旧数据。因此要尽量做批量的变更，避免做频繁的小变更。 实现高性能的update和delete思路： 表结构额外新增两个字段：isvalid，0表示无效，1表示有效；version，表示数据版本号，最大为最新数据 更新：插入一条version = max(version) + 1的数据即可 删除：插入一条isvalid = 0的数据即可 查询：每次查询条件都要加上version = max(version) and isvalid = 1 问题：数据膨胀，需要定期清理过期数据，并对数据分区做合并 2、Join1select test2.id, sum(test3.money) from test2, test3 where test2.id = test3.id group by test2.id; clickhouse的join原理：将右表（test3）加载到内存中，再和左表一条一条匹配。 因此这里最好不要将大表写到右边，这一点和MySQL的“小表驱动大表”不一样。 3、函数窗口函数，21.7.3.14版本还处于实验中，需要使用的话，要将一个设置打开： 1set allow_experimental_window_functions = 1; 自定义函数，不支持。 multiif(cond_1, then_1, cond_2, then_2, …)，类似于case when。 4、groupgroup增加了with rollup、with cube、with totals用于统计不同维度的值。 1234567891011121314151617181920212223-- rollup是从右至左依次减少一个维度进行统计group A, B with rollup = group by A, Buniongroup by A, nulluniongroup by null, null-- cube是排列组合的维度统计group by A, B with cube =group by A, Buniongroup by A, nulluniongroup by B, nulluniongroup by null, null-- total是额外加上统计总计值group by A, B with totals =group by A, Buniongroup by null, null 示例： 12345select id % 2 as flag, substring(name, 7, 1) as flag2, sum(age) from test2 group by id % 2, substring(name, 7, 1) with rollup;select id % 2 as flag, substring(name, 7, 1) as flag2, sum(age) from test2 group by id % 2, substring(name, 7, 1) with cube;select id % 2 as flag, substring(name, 7, 1) as flag2, sum(age) from test2 group by id % 2, substring(name, 7, 1) with totals; 六、副本写入（备份、高可用）clickhouse副本类似于MySQL的replicate，备份用。对应的表引擎为ReplicatedMergeTree。 依赖zookeeper，没有主次之分。 配置的两种方式： 1、修改/etc/clickhouse/config.xml配置文件中的zookeeper配置 1234567891011121314&lt;zookeeper&gt; &lt;node&gt; &lt;host&gt;example1&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node&gt; &lt;host&gt;example2&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node&gt; &lt;host&gt;example3&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt;&lt;/zookeeper&gt; 2、在/etc/clickhouse-server/config.d/目录下新增一个metrika.xml文件，按照config.xml中zookeeper的配置样式写入自己的配置，再添加一个include_from标签和一个带有incl属性的zookeeper标签，引入这个配置文件。该文件的配置会覆盖config.xml中zookeeper的配置。 以这种方式创建xml文件不要忘了用户权限，将文件owner设置为clickhouse:clickhouse 1chown clickhouse:clickhouse metrika.xml 12345678910111213141516171819&lt;!-- metrika.xml --&gt;&lt;zookeeper&gt; &lt;node&gt; &lt;host&gt;example1&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node&gt; &lt;host&gt;example2&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node&gt; &lt;host&gt;example3&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt;&lt;/zookeeper&gt;&lt;!-- config.xml --&gt;&lt;zookeeper incl=&quot;zookeeper-servers&quot; optional=&quot;true&quot;/&gt;&lt;include_from&gt;/etc/clickhouse-server/config.d/metrika.xml&lt;/include_from&gt; 七、分片集群副本是指备份，每个服务器都有全量数据（就像复制了一个服务器一样，所以叫replicate）。 分片集群是指分布式，将一份数据切片，分布到不同服务器上，每次查询需要将任务分发到各个服务器中，最后汇总结果（类似于map-reduce）。 分片集群对应的表引擎为Distributed。 1、集群写入以3分片，2副本为例。 distribute hdp1类似于一个master（以下为了方便，简称为master），并不实际存储数据，而是起一个控制器和数据分发的作用。下面的hdp1-hdp6类似于worker，存储数据。客户端发送写入命令，master将接收到的数据发送给下面的worker写入数据。 问题：每个副本都要由master亲自分发数据吗？ internal_replication参数， true，表示master只将数据分发给分片，每个副本的数据写入有对应的分片进行内部复制 false，表示每个副本的数据写入也由master亲自分发，而不是由分片内部复制 一般来说都会把这个参数设置为true，原因： 副本服务器也要master来分发的话，会对master造成压力，写入效率会降低 对副本的分发过程中如果出现异常，则可能造成分片和对应的副本数据不一致 每一层只负责自己的事，master是为分布式而产生了，并不是为副本而产生的，因此master不太应该管副本的备份工作。 2、集群读取 errors_count，即读取时发生错误的次数，每次发生错误会记录在对应的服务器中，读取时会选择错误次数更小的副本读取。 3、配置同样可以将这个配置写在默认配置文件config.xml中，也可以在外部文件中加配置再在config.xml中引入。 在/etc/clickhouse-server/config.xml中加上 1&lt;include_from&gt;/etc/clickhouse-server/config.d/metrika-shard.xml&lt;/include_from&gt; /etc/clickhouse-server/config.d/metrika-shard.xml配置如下： 注意同样别忘了文件的用户权限。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;remote_servers&gt; &lt;perftest_1shards_3replicas&gt;&lt;!-- 集群名称，可以修改 --&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;example-perftest01j.yandex.ru&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;host&gt;example-perftest02j.yandex.ru&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;example-perftest03j.yandex.ru&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;host&gt;example-perftest04j.yandex.ru&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;example-perftest05j.yandex.ru&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;host&gt;example-perftest06j.yandex.ru&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/perftest_1shards_3replicas&gt;&lt;/remote_servers&gt;&lt;zookeeper&gt;&lt;!-- 有几台服务器就需要几台zookeeper，按照以上需求，要写六台，这里省略 --&gt; &lt;node&gt; &lt;host&gt;zoo01.yandex.ru&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node&gt; &lt;host&gt;zoo02.yandex.ru&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node&gt; &lt;host&gt;zoo03.yandex.ru&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt;&lt;/zookeeper&gt;&lt;macros&gt; &lt;shard&gt;01&lt;/shard&gt; &lt;replica&gt;rep_1_1&lt;/replica&gt;&lt;/macros&gt; macros用于建表时识别分片和副本，shard为分片名称，replica为副本名称，这个配置每台服务器上都要单独配置。 可以自定义配置，这里配置的含义是：rep_&#123;a&#125;_&#123;b&#125;，a表示分片编号，b表示副本编号。 123456789101112131415161718192021222324&lt;macros&gt;&lt;!-- 第1个分片，第1个副本 --&gt; &lt;shard&gt;01&lt;/shard&gt; &lt;replica&gt;rep_1_1&lt;/replica&gt;&lt;/macros&gt;&lt;macros&gt;&lt;!-- 第1个分片，第2个副本 --&gt; &lt;shard&gt;01&lt;/shard&gt; &lt;replica&gt;rep_1_2&lt;/replica&gt;&lt;/macros&gt;&lt;macros&gt;&lt;!-- 第2个分片，第1个副本 --&gt; &lt;shard&gt;02&lt;/shard&gt; &lt;replica&gt;rep_2_1&lt;/replica&gt;&lt;/macros&gt;&lt;macros&gt;&lt;!-- 第2个分片，第2个副本 --&gt; &lt;shard&gt;02&lt;/shard&gt; &lt;replica&gt;rep_2_2&lt;/replica&gt;&lt;/macros&gt;&lt;macros&gt;&lt;!-- 第3个分片，第1个副本 --&gt; &lt;shard&gt;03&lt;/shard&gt; &lt;replica&gt;rep_3_1&lt;/replica&gt;&lt;/macros&gt;&lt;macros&gt;&lt;!-- 第3个分片，第2个副本 --&gt; &lt;shard&gt;03&lt;/shard&gt; &lt;replica&gt;rep_3_2&lt;/replica&gt;&lt;/macros&gt; 默认配置文件config.xml中官方也给我们配好了几个示例集群的配置，可以参考上面的配置以及配置说明。 1234567891011121314myubuntu1 :) show clusters;SHOW CLUSTERSQuery id: 1d87a0a9-9e49-498d-b097-c22a23530d2e┌─cluster──────────────────────────────────────┐│ test_cluster_two_shards ││ test_cluster_two_shards_internal_replication ││ test_cluster_two_shards_localhost ││ test_shard_localhost ││ test_shard_localhost_secure ││ test_unavailable_shard │└──────────────────────────────────────────────┘ 4、使用（1）建表先建本地表： 即创建之前说的worker，存储数据用的本地表。 123456789101112131415create table test_cluster_table on cluster test_shard_localhost( id UInt8, name String, create_time Datetime default now())engine = ReplicatedMergeTree(&#x27;clickhouse/tables/&#123;shard&#125;/test_cluster_table&#x27;, &#x27;&#123;replica&#125;&#x27;)order by (id, name);Query id: 86b73340-2c9d-42a1-97e2-f99d8d388f0e┌─host────────────┬─port─┬─status─┬─error─┬─num_hosts_remaining─┬─num_hosts_active─┐│ 192.168.141.141 │ 9003 │ 0 │ │ 0 │ 0 │└─────────────────┴──────┴────────┴───────┴─────────────────────┴──────────────────┘1 rows in set. Elapsed: 0.111 sec. test_shard_localhost为集群名称 {shard}和{replica}都是配置文件中macros导入，不用自己填。 再创建分布式表： 即创建之前说的master 1234567891011121314create table test_cluster_table_all on cluster test_shard_localhost( id UInt8, name String, create_time Datetime default now())engine=Distributed(test_shard_localhost, default, test_cluster_table, hiveHash(id));Query id: 69a6f496-e916-47ac-ae0b-636c9244bc90┌─host────────────┬─port─┬─status─┬─error─┬─num_hosts_remaining─┬─num_hosts_active─┐│ 192.168.141.141 │ 9003 │ 0 │ │ 0 │ 0 │└─────────────────┴──────┴────────┴───────┴─────────────────────┴──────────────────┘1 rows in set. Elapsed: 0.114 sec. test_shard_localhost为集群名称 default为数据库名 test_cluster_table为刚才创建的本地表名 hiveHash(id)，hiveHash表示采用什么算法分片，id表示用哪个字段分片。 （2）使用分布式表不存储数据，为每个本地表的逻辑汇总表。 1234567-- 从分布式表插入数据，本地表可以看到insert into test_cluster_table_all values(1, &#x27;小明&#x27;, now());select * from test_cluster_table;-- 从本地表插入，从分布式表也可以看得到insert into test_cluster_table values(2, &#x27;小红&#x27;, now());select * from test_cluster_table_all; 本地表只能看到本服务器上存储的数据分片，无法看到其他服务器上存储的数据分片，而分布式表可以看到所有数据。 因此一般不用将本地表暴露给用户写入，统一从分布式表做增删改查。 八、进阶语法1、explain20.6.3.28及以后版本才有explain功能。 官方文档：EXPLAIN | ClickHouse Documentation 1EXPLAIN [AST | SYNTAX | PLAN | PIPELINE] [setting = value, ...] SELECT ... [FORMAT ...] EXPLAIN SYNTAX …可以看到系统给你优化后的语法。 比如上面那个join的sql，通过系统优化后的sql语句如下。 1234567891011121314151617explain syntax select test2.id, sum(test3.money) from test3, test2 where test2.id = test3.id group by test2.id;┌─explain─────────────────────┐│ SELECT ││ id, ││ sum(test3.money) ││ FROM test2 ││ ALL INNER JOIN ││ ( ││ SELECT ││ id, ││ money ││ FROM test3 ││ ) AS test3 ON id = test3.id ││ WHERE id = test3.id ││ GROUP BY id │└─────────────────────────────┘ 但这里优化后的结果不对，是因为id没有加上test2表名，并且where条件也是多余的，手动修改为如下sql，对比原始sql执行速度，优化后的sql性能确实提升了一些。 千万不要盲目相信explain syntax后的结果，可能会让你得不偿失。 123456789101112SELECT test2.id, sum(test3.money)FROM test2ALL INNER JOIN( SELECT id, money FROM test3) AS test3 ON test2.id = test3.idGROUP BY test2.id; 老版本没有explain语法，要查看执行计划，可以在进入客户端时，加上--send_logs_level=trade &lt;&lt;&lt; &quot;sql&quot;，然后去看日志即可。 2、optimize1OPTIMIZE TABLE [db.]name [ON CLUSTER cluster] [PARTITION partition | PARTITION ID &#x27;partition_id&#x27;] [FINAL] [DEDUPLICATE [BY expression]] optimize功能是触发一个表数据的合并。只能用于MergeTree系列表引擎、Buffer表引擎和MaterializedView表引擎。 触发数据合并的意思是，例如在一个ReplacingMergeTree表执行，则会执行一个任务，按order by指定的字段去重；又例如在SummingMergeTree表执行，则会执行一个任务，按照order by指定字段进行聚合，对SummingMergeTree指定字段进行sum计算，其他字段保留最新数据。即将数据初始化，或重新整理分区数据。 如果指定了FINAL，则会强制执行数据合并，即使所有数据都在一个分区，或者已经有并发的合并正在进行。 九、常用配置1、CPU 位置 配置 说明 user.xml background_pool_size 表引擎相关的后台线程池大小，常用于merge任务。默认值为16，允许的情况下可以调整称服务器的逻辑线程数据。 user.xml background_schedule_pool_size 后台任务线程池大小，常用于副本数据备份、kafka流、DNS缓存更新。默认值128。 user.xml background_distributed_schedule_pool_size 后台任务线程池大小，常用于分布式数据发送。默认值为16。 config.xml max_concurrent_queries 最大并发请求数，默认设置为100，建议不超过300。 user.xml max_threads 单个查询使用的最大线程数，默认值为cpu物理核心数。这个值越低，查询时占用的内存越低。 2、内存 位置 配置 说明 user.xml max_memory_usage 单次查询使用的最大内存。默认值为10G左右。 user.xml max_bytes_before_external_groupby 当使用groupby时内存占用超过最大内存时，将数据写入磁盘做缓存进行groupby时大小。一般可以设置为max_memory_usage的一半。 user.xml max_bytes_before_external_sort 当使用sort时内存占用超过最大内存时，将数据写入磁盘作为临时缓存进行sort时的大小。一般可以设置为max_memory_usage的一半。 config.xml max_table_size_to_drop drop table时，允许删除的表的最大大小，表大小超过这个值时删除表不被允许。设置为0时，认为没有限制。 十、查询优化1、单表优化（1）prewhere只能用于MergeTree系列表引擎。 prewhere执行时，会直接扫描过滤条件中的列，将数据过滤后，再将需要的字段补全。 当查询列明显多于过滤列时，可以使用prewhere，降低IO。 optimize_move_to_prewhere参数是在有需要的时候自动将where优化成prewhere，默认是打开的。即这个参数打开时，不需要手动写prewhere，系统会在有需要的时候自动优化成prewhere去查询。 部分情况下，不会自动优化成prewhere： 使用常用表达式 使用默认值的alias类型的字段 包含了arrayJOIN，globalIn，globalNotIn，indexHint的查询 select字段和where字段相同 使用了主键字段 有需要时，还是自己手动使用prewhere好，别太依赖系统的优化。 （2）sample只能用于MergeTree系列表引擎。 采样。不会对所有数据执行查询，而是对特定部分数据（样本）进行查询。 且这个样本并不是严格精确的数据量。 1234567891011121314-- 约10%样本查询select *from testsample 0.1;-- 当比例值远大于1时，这个含义便会转换为数据条数据，如这里采样至少10000条select *from testsample 10000;-- 加上offset表示跳过前面部分数据select *from testsample 0.1 offset 0.5 和limit的区别在于，limit是严格精确的数据量，而sample并不是严格的数据量。同时sample可以用在where、group和orderby前面。 123456789SELECT Title, count() * 10 AS PageViewsFROM hits_distributedSAMPLE 0.1WHERE CounterID = 34GROUP BY TitleORDER BY PageViews DESC LIMIT 1000; 在一些特殊情况下，可以使用sample： 业务需要查询延迟很低，但无法通过优化来降低延迟，且对具体结果的精确性不太讲究，可以使用近似结果 只想大致查看数据的分布以及数据质量 （3）orderbyorder by不要单独使用，结合where和limit一起使用。 一般排序后很少需要完整排序的结果，因此可以加一个limit。加limit和不加limit的效率还是有些差距的。 （4）虚拟列clickhouse中尽量不要使用虚拟列，很消耗性能。可以在服务端或者其他地方处理数据，不要在clickhouse中使用虚拟列来处理数据。 12select a/b from test;select a, b from test; 2、多表关联数据集： 123456789curl -O https://datasets.clickhouse.com/hits/partitions/hits_v1.tartar xvf hits_v1.tar -C /var/lib/clickhouse # path to ClickHouse data directory# check permissions on unpacked data, fix if requiredcurl -O https://datasets.clickhouse.com/visits/partitions/visits_v1.tartar xvf visits_v1.tar -C /var/lib/clickhouse # path to ClickHouse data directory# check permissions on unpacked data, fix if requiredsudo service clickhouse-server restart clickhouse建表时没有like语法，因此使用以下语法创建一个张和hits_v1一样表结构的表。 1234567create table hits_testENGINE = MergeTreePARTITION BY toYYYYMM(EventDate)ORDER BY (CounterID, EventDate, intHash32(UserID))SAMPLE BY intHash32(UserID)SETTINGS index_granularity = 8192as select * from datasets.hits_v1 where 1 = 0; （1）用in代替join但是in的使用场景很有限，而且使用时要特别注意查询结果，因为join是笛卡尔积，右表有重复数据的话，结果集可能不止一条，而使用in的话，右表有重复数据对于左表来说是没关系的。 （2）大表在左，小表在右和MySQL或其他行式数据库不一样，clickhouse在使用join时，需要将大表作为主表，小表作为被关联的表。 1234567-- 内存不足，直接报错insert into hits_testselect a.* from datasets.visits_v1 b join datasets.hits_v1 a on a.CounterID = b.CounterID where a.CounterID &gt; 100000;-- 3.751秒，可以执行insert into hits_testselect a.* from datasets.hits_v1 a join datasets.visits_v1 b on a.CounterID = b.CounterID where a.CounterID &gt; 100000; （3）先过滤再关联对于查询大数据量的sql，先过滤再关联，可以减少扫描的数据量，提升一点效率。即先对某一个表写where条件，形成一个子查询，再用子查询来关联，效率会比直接关联然后统一写where条件来的高。 使用先过滤再关联还有一个好处在于，如果过滤条件是右表，则可以减少将数据加载到内存的量。 123456789101112131415-- 1.999秒，效率低insert into hits_testselect a.* from datasets.hits_v1 a join datasets.visits_v1 b on a.CounterID = b.CounterID where a.EventDate = &#x27;2014-03-17&#x27;;-- 1.772秒，效率高insert into hits_testselect a.* from (select * from datasets.hits_v1 where EventDate = &#x27;2014-03-17&#x27;) a join datasets.visits_v1 b on a.CounterID = b.CounterID;-- 2.509秒，占用内存峰值1.91Ginsert into hits_testselect a.* from datasets.hits_v1 a join datasets.visits_v1 b on a.CounterID = b.CounterID where b.StartDate = &#x27;2014-03-17&#x27;;-- 2.478秒，占用内存峰值1.7Ginsert into hits_testselect a.* from datasets.hits_v1 a join (select CounterID from datasets.visits_v1 where StartDate = &#x27;2014-03-17&#x27;) b on a.CounterID = b.CounterID; （4）分布式表用GLOBAL以3分片为例，hits_v1表为分布式表，visits_v1为普通表，以下查询为例。 1select * from (select * from hits_v1 perwhere CounterID &gt; 1000) a join visits_v1 b on on a.CounterID = b.CounterID; 分布式表在join时，每台服务器都会将右表加载到各自服务器的内存中，然后进行匹配。不使用GLOBAL时，需要查3次右表；使用GLOBAL，可以只进行1次查询，并分发到其他节点。如果此时右表也是分布式表的话，不适用GLOBAL就会查9次；使用GLOBAL只需要查3次。这就是查询放大。 （5）使用字典表Dictionary。 https://clickhouse.com/docs/en/sql-reference/dictionaries/internal-dicts/ 3、其他优化（1）写入数据时先排序。因为写入数据时，数据不会直接分配到实际所在的那个分区，而是会先临时放在一个新的分区。无序的数据会产生大量的新分区，merge时会产生性能问题。 （2）关注CPUcpu负载在50%时，会对查询性能会产生影响；cpu负载超过70%时，会出现大范围超时情况。因此不要以为在单次查询时性能很好就以为在实际生产环境中没有问题，放到生产环境中的查询sql要尽量优化好。 十一、一致性clickhouse只能保证最终一致性！如ReplacingMergeTree的删除重复数据的功能，只会在进行数据合并时进行，即在不确定的时候后台进行。因此平时使用时，不能保证没有重复数据。问题：如何保证查询时数据的一致性。 123456789101112131415161718192021222324252627-- 创建表create table test_replacing_mt( user_id UInt64, score String, deleted UInt8 default 0, create_time DateTime default toDateTime(0))engine=ReplacingMergeTree(create_time)order by user_id;-- 写入数据insert into test_replacing_mt(user_id, score)with ( select [&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;, &#x27;G&#x27;]) as dictselect number as user_id, dict[rand()%7+1] as scorefrom numbers(10000000);-- 修改数据（其实还是写入数据）insert into test_replacing_mt(user_id, score, create_time)with ( select [&#x27;AA&#x27;, &#x27;BB&#x27;, &#x27;CC&#x27;, &#x27;DD&#x27;, &#x27;EE&#x27;, &#x27;FF&#x27;, &#x27;GG&#x27;]) as dictselect number as user_id, dict[rand()%7+1] as score, now() as create_timefrom numbers(500000);-- 查询数据量，为10500000条，并没有立即去重select count(1) from test_replacing_mt; 1、手动optimize（错误）每次执行完insert都手动进行optimize。 绝对不可取！！手动optimize只能空闲时间进行。 2、手动通过sql实现通过设置一些特殊字段，如deleted控制该条数据是否被删除掉了，create_time控制该条数据的创建时间。 然后查询create_time为最新的，且deleted为0的数据，即可实现手动去重。 12345678select user_id, argMax(score, create_time) as score, argMax(deleted, create_time) as deleted, max(create_time) as ctimefrom test_replacing_mtgroup by user_idhaving deleted = 0 以上面sql做一个普通视图，并在这个视图上查询数据，则可以查询到最新的有效数据。 12345678910111213create view test_replacing_mt_view asselect user_id, argMax(score, create_time) as score, argMax(deleted, create_time) as deleted, max(create_time) as ctimefrom test_replacing_mtgroup by user_idhaving deleted = 0;-- 在视图上进行查询select count(1) from test_replacing_mt_view;select * from test_replacing_mt_view where user_id == 100; 删除数据则通过插入一条deleted = 0 and create_time = now()的数据 1234567insert into test_replacing_mt values(100, &#x27;AA&#x27;, 1, now());-- 再次查询test_replacing_mt_view，发现没有数据返回select * from test_replacing_mt_view where user_id == 100;-- 9999999条数据select count(1) from test_replacing_mt_view; 注： argMax(field1, field2)：按照field2的最大值取field1的值。 3、通过FINAL查询在sql语句中写final只支持ReplacingMergeTree和SummingMergeTree，因为这两个表引擎在合并数据时需要聚合，普通的MergeTree表引擎合merge时不需要聚合。 final的作用是根据order by指定的字段，查询版本号最新的一条数据。 20.5.2.7-stable版本之前是单线程进行，速度很慢，老版本不建议使用。后面新版本支持多线程，并且可以通过max_final_threads参数控制单个final查询的线程数。 123456789-- 当然deleted还是要加的select * from test_replacing_mt final where user_id = 100 and deleted = 0;-- 如果不加deleted字段，可以看到最新版本的数据是那条deleted为1的数据select * from test_replacing_mt final where user_id = 100;┌─user_id─┬─score─┬─deleted─┬─────────create_time─┐│ 100 │ AA │ 1 │ 2021-12-11 13:27:54 │└─────────┴───────┴─────────┴─────────────────────┘ 4、结论由于clickhouse对update和delete的操作不友善，因此在实际生产环境中最好加上这么几个字段： create_time：数据创建时间 deleted：该数据是否被删除了，1代表是，0代表没有删除 或者使用 isvalid：该条数据是否有效，1代表有效数据，0代表无效数据，即和deleted作用相反。 如果是新版本，则可以在使用final查询和自定义视图的方式之间权衡，对比两种方式的效率以及对数据库造成的压力，选择最适合的方式。注意并不一定是所有表都要用一种方式，可能有些数据使用自定义视图合适，而有些数据使用final合适。 如果是旧版本，final无法设置多线程，导致效率很低，则还是选择自定义视图的方式来实现吧。 其次，每天在空闲时间定时进行数据合并。 最后，如果以上方式效率都很低，但是某个业务对于查询出来的数据的准确定并不讲究，则可以不进行去重。 十二、物化视图（MaterializedView）普通视图只保存查询逻辑，并不保存数据。而物化视图会保存数据，即真正创建一张隐藏表来存储数据，当原始表数据插入数据时，新数据会按照物化视图的逻辑生成最新的数据。 优点：快！当需要查询一些聚合的操作时，使用原始数据进行聚合查询可能会比较慢，但是可以通过物化视图将聚合数据加载到一张表中，然后直接查物化视图中的数据，速度就可想而知很快了。 1CREATE MATERIALIZED VIEW [IF NOT EXISTS] [db.]table_name [ON CLUSTER] [TO[db.]name] [ENGINE = engine] [POPULATE] AS SELECT ... TO：隐藏表名，如果不加的话，默认为.inner_id.xxxxxxxx POPULATE：创建后会对数据进行初始化，即执行视图逻辑，并将所有数据写入隐藏表。生产环境不建议加，如果需要历史数据，可以手动insert。 1、使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657-- 创建原始数据表create table test_score( user_id UInt64, subject String, score String, isvalid UInt8 default 1, create_time DateTime default toDateTime(0))engine=ReplacingMergeTree(create_time)order by (user_id, subject);-- 写入初始化数据insert into test_score(user_id, subject, score, create_time)with( select range(1000)) as user_dict,( select [&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;, &#x27;G&#x27;]) as score_dict,( select [&#x27;语文&#x27;, &#x27;数学&#x27;, &#x27;英语&#x27;]) as subject_dict,( select [&#x27;2021-12-11 15:00:00&#x27;, &#x27;2021-12-10 15:00:00&#x27;, &#x27;2021-12-09 15:00:00&#x27;]) as ctime_dictselect user_dict[rand()%1000+1] as user_id, subject_dict[rand()%3+1] as subject, score_dict[rand()%7+1] as score, ctime_dict[rand()%3+1] as create_timefrom numbers(100000)order by user_id, subject;-- 创建物化视图，业务逻辑：将每个user_id的成绩记录成json格式，写入score_json字段create materialized view test_score_mvengine=ReplacingMergeTree(create_time)order by (user_id)as select user_id, score_json, isvalid, create_timefrom ( select user_id, &#x27;&#123;&#x27; || arrayStringConcat(groupArray(&#x27;&quot;&#x27; || subject || &#x27;&quot;:&quot;&#x27; || score || &#x27;&quot;&#x27;), &#x27;,&#x27;) || &#x27;&#125;&#x27; as score_json, 1 as isvalid, max(ctime) as create_time from ( select user_id, subject, argMax(score, create_time) as score, argMax(isvalid, create_time) as isvalid, max(create_time) as ctime from test_score group by user_id, subject having isvalid = 1 ) as tmp1 group by user_id) as tmp2; 记录这个创建时间，初始化数据时可以通过筛选小于这个创建时间来初始化。 由于内存限制，初始化时，可以一批一批来 1234567891011121314clickhouse-client -m -h 192.168.141.141 --port 9003 --query &quot;select user_id from test_score group by user_id order by user_id format CSV;&quot; &gt; test_score.csvwhile read user_id; do echo &quot;$&#123;user_id&#125;&quot;; clickhouse-client -m -h 192.168.141.141 --port 9003 --query &quot;\\insert into test_score_mv \\select \\ user_id, \\ &#x27;&#123;&#x27; || arrayStringConcat(groupArray(&#x27;\\&quot;&#x27; || subject || &#x27;\\&quot;:\\&quot;&#x27; || score || &#x27;\\&quot;&#x27;), &#x27;,&#x27;) || &#x27;&#125;&#x27; as score_json, \\ 1 as isvalid, \\ max(create_time) as create_time \\from test_score \\final \\where user_id = $&#123;user_id&#125; \\group by user_id \\&quot;; done &lt; test_score.csv; 测试插入新数据到hits_test： 123456789101112131415161718-- 测试插入前，可以查询历史数据是否成功插入到物化视图中select * from test_score_mv where user_id = 0 and subject = &#x27;语文&#x27;;┌─user_id─┬─score_json────────────────────────────┬─isvalid─┬─────────create_time─┐│ 0 │ &#123;&quot;数学&quot;:&quot;EE&quot;,&quot;语文&quot;:&quot;EEE&quot;,&quot;英语&quot;:&quot;F&quot;&#125; │ 1 │ 2021-12-11 17:54:16 │└─────────┴───────────────────────────────────────┴─────────┴─────────────────────┘-- 插入一条数据insert into test_score values (0, &#x27;数学&#x27;, &#x27;EEE&#x27;, 1, now());-- 再次查询物化视图，是有数据的，但是只有最新数据的group。select * from test_score_mv where user_id = 0 and subject = &#x27;语文&#x27;;┌─user_id─┬─score_json─────┬─isvalid─┬─────────create_time─┐│ 0 │ &#123;&quot;数学&quot;:&quot;EEE&quot;&#125; │ 1 │ 2021-12-11 18:31:53 │└─────────┴────────────────┴─────────┴─────────────────────┘┌─user_id─┬─score_json────────────────────────────┬─isvalid─┬─────────create_time─┐│ 0 │ &#123;&quot;数学&quot;:&quot;EE&quot;,&quot;语文&quot;:&quot;EEE&quot;,&quot;英语&quot;:&quot;F&quot;&#125; │ 1 │ 2021-12-11 17:54:16 │└─────────┴───────────────────────────────────────┴─────────┴─────────────────────┘ 测试更新数据（实际业务场景中不建议用update语句，这里只用作测试）。 1234567891011optimize table test_score;alter table test_score update score = &#x27;AAA&#x27; where user_id = 0;-- 发现物化视图数据并没有修改select * from test_score_mv where user_id = 0;┌─user_id─┬─score_json─────┬─isvalid─┬─────────create_time─┐│ 0 │ &#123;&quot;数学&quot;:&quot;EEE&quot;&#125; │ 1 │ 2021-12-11 18:31:53 │└─────────┴────────────────┴─────────┴─────────────────────┘┌─user_id─┬─score_json────────────────────────────┬─isvalid─┬─────────create_time─┐│ 0 │ &#123;&quot;数学&quot;:&quot;EE&quot;,&quot;语文&quot;:&quot;EEE&quot;,&quot;英语&quot;:&quot;F&quot;&#125; │ 1 │ 2021-12-11 17:54:16 │└─────────┴───────────────────────────────────────┴─────────┴─────────────────────┘ 测试删除数据（实际业务场景中不建议用update语句，这里只用作测试）。 12345678910alter table test_score delete where user_id = 0;-- 发现物化视图数据同样没有修改select * from test_score_mv where user_id = 0;┌─user_id─┬─score_json─────┬─isvalid─┬─────────create_time─┐│ 0 │ &#123;&quot;数学&quot;:&quot;EEE&quot;&#125; │ 1 │ 2021-12-11 18:31:53 │└─────────┴────────────────┴─────────┴─────────────────────┘┌─user_id─┬─score_json────────────────────────────┬─isvalid─┬─────────create_time─┐│ 0 │ &#123;&quot;数学&quot;:&quot;EE&quot;,&quot;语文&quot;:&quot;EEE&quot;,&quot;英语&quot;:&quot;F&quot;&#125; │ 1 │ 2021-12-11 17:54:16 │└─────────┴───────────────────────────────────────┴─────────┴─────────────────────┘ clickhouse的物化视图更像是触发器，且只对新增的部分数据有效，对历史数据，或update操作或delete操作都是无效的。物化视图功能有限，对于上面这种业务逻辑，是不适用的。 2、简单聚合业务简单聚合，如sum、max、min、count等业务，可以使用物化视图来实现。 物化视图使用的隐藏表不使用默认的，而是我们自己定义的表，具体如下： 业务介绍：统计每个设备出现的次数和最大值、最小值、平均值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647-- 基础数据表CREATE TABLE counter ( when DateTime DEFAULT now(), device UInt32, value Float32) ENGINE=MergeTreePARTITION BY toYYYYMM(when)ORDER BY (device, when);-- 物化视图物理表，以天为单位做预聚合CREATE TABLE counter_daily ( day DateTime, device UInt32, count UInt64, max_value_state AggregateFunction(max, Float32), min_value_state AggregateFunction(min, Float32), avg_value_state AggregateFunction(avg, Float32))ENGINE = SummingMergeTree()PARTITION BY tuple()ORDER BY (device, day);-- 物化视图CREATE MATERIALIZED VIEW counter_daily_mvTO counter_dailyAS SELECT toStartOfDay(when) as day, device, count(*) as count, maxState(value) AS max_value_state, minState(value) AS min_value_state, avgState(value) AS avg_value_stateFROM counterWHERE when &gt;= toDate(&#x27;2019-01-01 00:00:00&#x27;)GROUP BY device, dayORDER BY device, day;-- 查询SELECT device, sum(count) AS count, maxMerge(max_value_state) AS max, minMerge(min_value_state) AS min, avgMerge(avg_value_state) AS avgFROM counter_daily_mvGROUP BY deviceORDER BY device ASC; 物化视图的物理表使用的是SummingMergeTree表引擎，且定义三个状态函数。 物化视图则像一个触发器，将新插入的数据转换为以天为单位的状态写入物理表中。 最后查询时需要指定对应的聚合类型，获取对应的聚合值。 3、应用场景 历史状态可用的业务，比如上面说的sum等聚合。但是对于历史状态不可用的聚合，比如中位数、方差等，无法通过简单使用物化视图或预聚合来实现。 数据过滤 当作触发器来用 流数据处理 重排序。即基础数据表只能有一个排序字段，但是如果某个业务想要以其他字段排序或group查询效率很低，则可以新建一个以其他字段为order by的物化视图，再将基础数据表的指定数据插入到物化视图中。这种玩法本质是为了做两张表。且一张表是另外一个张表的子集，则可以通过一个sql往两张表写入数据。 十三、MaterializeMySQL引擎MaterializeMySQL引擎是一个库引擎，作用是直接将MySQL的数据变化通过流的形式同步到clickhouse中，底层原理和flinkcdc、canal一样都是基于binlog实现的。 MySQL配置： 123456default-authentication-plugin=mysql_native_password # 需要用这种密码验证方式# 如果配置了主从，则要加上以下配置gtid-mode=on # 主从切换时保证数据一致性enforce-gtid-consistency=1 # 强一致性log-slave-updates=1 # 从服务器日志记录 clickhouse配置： 1set allow_experimental_database_materialize_mysql=1; -- 当前版本这个配置是关闭的 创建对应的数据库： 12345678-- MySQL中创建新数据库和表create database testck;create table testck.test1( seq bigint auto_increment primary key)default charset=utf8mb4;-- clickhouse创建以下数据库create database testck engine=MaterializeMySQL(&#x27;192.168.141.141:3306&#x27;, &#x27;testck&#x27;, &#x27;root&#x27;, &#x27;root&#x27;); 注意，通过MaterializeMySQL同步的数据库表必须要有主键，不然会报错。 如果你的的密码认证插件是从caching_sha2_password临时修改成mysql_native_password的话，你还需要修改mysql.user表中的plugin字段修改为mysql_native_password。不然查询clickhouse的数据时会报错： 1Code: 100. DB::Exception: Received from myubuntu1:9003. DB::Exception: Access denied for user root. 同步后，会自动生成对应的clickhouse表，如下。 MySQL的主键 = clickhouse的order by，并且自动加上了_sign和_version字段。 _sign字段：数据是否有效的标识，1代表有效数据，-1代表无效数据，即被删除了的数据。 _version字段：数据版本号，每次执行MySQL的sql语句，转换为clickhouse的一条sql语句插入的数据版本号都是相同的。clickhouse内部应该维护了一个最大版本号，最新sql语句写入的数据会使用这个最大版本号，使用完之后再让这个最大版本号+1。 12345678910111213┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐│ CREATE TABLE testck.test1( `seq` Int64, `_sign` Int8 MATERIALIZED 1, `_version` UInt64 MATERIALIZED 1, INDEX _version _version TYPE minmax GRANULARITY 1)ENGINE = ReplacingMergeTree(_version)PARTITION BY intDiv(seq, 18446744073709551)ORDER BY tuple(seq)SETTINGS index_granularity = 8192 │└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ MySQL的增删改操作都会通过一定的方式转换为clickhouse的insert操作。 MySQL insert = clickhouse的insert with _sign = 1的语句。 MySQL update = clickhouse的insert with _sign = -1和insert with _sign = 1的两条语句。 MySQL delete = clickhouse的insert with _sign = -1的语句。 十四、常见问题常见问题排查 (aliyun.com) 十五、监控和备份1、监控Promethseus + Grafana Promethseus：promethseus.io Grafana：Download Grafana | Grafana Labs 2、备份手动： 12345-- 备份到freeze目录下ALTER TABLE table_name FREEZE [PARTITION partition_expr] [WITH NAME &#x27;backup_name&#x27;]-- 从detach目录下恢复ALTER TABLE table_name ATTACH PARTITION|PART partition_expr 自动： GitHub - AlexAkulov/clickhouse-backup: Tool for easy ClickHouse backup and restore with cloud storages support（小心使用，可能有版本兼容问题。） N、一些不同寻常的点（坑）（1）普通类型的默认值不为null，都有各自的默认值，具体见Nullable类型说明。 （2）clickhouse的primary key是可以重复的，有需要的话得手动将其设置为unique。 （3）ReplacingMergeTree的聚合并不是实时的，每次查询都需要手动聚合去重。","categories":[{"name":"bigdata","slug":"bigdata","permalink":"https://yury757.github.io/categories/bigdata/"}],"tags":[]},{"title":"设计模式","slug":"java/设计模式","date":"2021-11-05T16:00:00.000Z","updated":"2022-05-29T15:40:39.397Z","comments":true,"path":"java/设计模式/","link":"","permalink":"https://yury757.github.io/java/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"一、分类 创建型模式：将对象的创建和使用分离 单例、原型、工厂方法、抽象工厂、建造者 结构性模式：用于描述如何将类和对象按某种布局组成更大的结构 代理、适配器、桥接、装饰、外观、享元、组合 行为型模式：用于描述类或对象之间怎样相互协调共同完成单个对象无法单独完成的操作 模板方法、策略、命令、职责链、状态、观察者、中介者、迭代器、访问者、备忘录、解释器 二、创建者模式1、单例模式（1）饿汉式1234567public class Singleton&#123; private static final Singleton singleton = new Singleton(); private Singleton() &#123; &#125; public Singleton getInstance() &#123; return singleton; &#125;&#125; （2）懒汉式-双重检查锁123456789101112131415161718192021222324public class Singleton implements Serializable &#123; private static volatile Singleton singleton; private synchronized Singleton() &#123; if (singleton != null) &#123; throw new RuntimeException(&quot;不能重复创建对象&quot;); &#125; // 创建对象的业务代码 // ... &#125; public static Singleton getInstance() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125; public Object readResolve() &#123; return getInstance(); &#125;&#125; （3）懒汉式-静态内部类原理：静态内部类只有在使用时才会被加载，不使用时不会加载该类的字节码，因此可用达到懒汉式的效果。 1234567891011121314151617public class Singleton &#123; private static boolean first = true; private synchronized Singleton() &#123; if (!first) &#123; throw new RuntimeException(&quot;不能重复创建对象&quot;); &#125; // 创建对象的业务代码 // ... first = false; &#125; public static Singleton getInstance() &#123; return Holder.singleton; &#125; private static class Holder &#123; private static final Singleton singleton = new Singleton(); &#125;&#125; （4）枚举方式枚举方式属于饿汉式，JVM加载后就会加载该类的字节码文件。 1234567public enum class Singleton &#123; singleton; private Singleton() &#123; &#125; public getInstance() &#123; return singleton; &#125;&#125; 2、工厂模式对象的创建不是直接创建，而是将你想创建的类型或属性交给工厂factory，由抽象工厂来创建对象并返回给你。这样你只和抽象工厂对象耦合，而和各种对象解耦。 常用场景：将需要创建的类型属性放入配置文件中，再在工厂类中加载对应指定配置文件中的属性，就可以完成对象的创建。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public static void main(String[] ... args) &#123; AnimalPark park = new AnimalPark(); AnimalFactory factory = new DogFactory(); Animal animal = park.getAnimal(factory); System.out.println(animal.getName());&#125;public class AnimalPark &#123; public Animal watchAnimal(AnimalFactory factory) &#123; return factory.getAnimal(); &#125;&#125;public abstract class Animal &#123; public getName();&#125;public interface AnimalFactory &#123; public Animal getAnimal();&#125;public class Dog extends Animal &#123; public getName() &#123; return &quot;dog&quot;; &#125;&#125;public class DogFactory implements AnimalFactory &#123; public getAnimal() &#123; return new Dog(); &#125;&#125;public class Cat extends Animal &#123; public getName() &#123; return &quot;cat&quot;; &#125;&#125;public class CatFactory implements AnimalFactory &#123; public getAnimal() &#123; return new Cat(); &#125;&#125;","categories":[{"name":"java","slug":"java","permalink":"https://yury757.github.io/categories/java/"}],"tags":[]},{"title":"kafka","slug":"bigdata/kafka/kafka","date":"2021-11-02T16:00:00.000Z","updated":"2022-10-07T12:42:28.450Z","comments":true,"path":"bigdata/kafka/kafka/","link":"","permalink":"https://yury757.github.io/bigdata/kafka/kafka/","excerpt":"","text":"kafka版本：2.12_2.8.1（2.12为scale版本，2.8.1为kafka版本，3.0以上的版本不需要依赖zookeeper） zookeeper版本：3.6.3 单机部署使用12345678910111213141516171819202122232425262728293031# 下载wget https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.8.1/kafka_2.12-2.8.1.tgz# 解压tar -xvzf kafka_2.12-2.8.1.tgz# 设置环境变量 sudo vi /etc/profileexport KAFKA_HOME=/home/yury/kafka_2.12-2.8.1export PATH=$&#123;PATH&#125;:$&#123;KAFKA_HOME&#125;/binsource /etc/profile# 以 /kafka_2.12-2.8.1 为根目录# 修改配置 vi server.properties 取下以下配置的注释，并修改称以下值zookeeper.connect=192.168.141.141:2181log.dirs=$&#123;KAFKA_HOME&#125;/tmp/kafka-logslisteners=PLAINTEXT://192.168.141.141:9092# 启动zookeeperskServer.sh startskServer.sh status # 查看状态：Mode: standalone# 写一个快速启动命令放到bin目录下 vi bin/start-kafka.sh 并写入$&#123;KAFKA_HOME&#125;/bin/kafka-server-start.sh $&#123;KAFKA_HOME&#125;/config/server.properties --daemon# 启动kafkastart-kafka.sh# jps命令3010 Jps2578 Kafka2046 QuorumPeerMain 快速上手1234567891011121314# 新增topicbin/kafka-topics.sh --bootstrap-server 192.168.141.141:9092 --create --topic test_topic# 查看topicbin/kafka-topics.sh --bootstrap-server 192.168.141.141:9092 --describe --topic test_topic# 删除topicbin/kafka-topics.sh --bootstrap-server 192.168.141.141:9092 --delete --topic test_topic# 生产数据bin/kafka-console-producer.sh --bootstrap-server 192.168.141.141:9092 --topic test_topic# 消费数据bin/kafka-console-consumer.sh --bootstrap-server 192.168.141.141:9092 --topic test_topic --from-beginning 概念1、broker一个broker即为一个kafka服务器。 2、partition分区分区，即一个topic会分为多个区域存储数据。","categories":[{"name":"bigdata","slug":"bigdata","permalink":"https://yury757.github.io/categories/bigdata/"}],"tags":[]},{"title":"Linux命令","slug":"linux/linux命令","date":"2021-10-31T16:00:00.000Z","updated":"2022-09-22T17:59:31.293Z","comments":true,"path":"linux/linux命令/","link":"","permalink":"https://yury757.github.io/linux/linux%E5%91%BD%E4%BB%A4/","excerpt":"","text":"安装1、刚安装好后的root用户的密码是随机的，需要修改密码： 1234sudo passwd[sudo] paasword for yury: # 然后输入当前用户yury的密码New password: # 输入root用户的新密码Retype new password: # 重新输入 2、修改主机名（如需要的话） 1sudo vi /etc/hostname 3、用户相关 1234567891011121314151617181920# 切换到root用户su# 新增一个名为XXX的用户useradd XXX# 为XXX用户设置密码passwd XXX# 退出root用户exit# 删除用户xxxsudo deluser xxx# 删除用户xxx及其home里面的文件夹sudo deluser --remove-home xxx# 修改用户名的流程很复杂，且容易使系统奔溃，因此在不是精通linux的情况下最好别修改用户名# 新增用户可以用命令adduser XXX，一套流程下来更方便 4、关机重启 123456789101112131415# 重启reboot# 关机poweroff# 关闭系统，不关闭电源halt# 或者使用shutdown --[reboot | halt | poweroff] [now | 20:35 | 10]# now是指立即操作，20:35是指在这个时间点操作，10是指10分钟后操作# 取消定时关机shutdown -c 前言1、sudo表示用管理员模式执行 2、命令后面若带有参数，一般短参数前加“-”，长参数前加“–”，但有些例外，看语言风格。 3、apt软件包默认下载路径为：/var/cache/apt/archives。apt软件包默认安装路径为：/usr/share 4、常用命令： clear sudo：允许本用户以其他用户（默认为超级管理员用户）的安全权限来运行某个命令或程序，即superuser do shell语法1、读取文本文件，循环每一行数据，将数据作为参数执行命令 1234567# 方法1for a in `cat hello.txt`; do echo &quot;$&#123;a&#125; 123&quot;; done;# 方法2while read a; do echo &quot;$&#123;a&#125; 123&quot;; done &lt; hello.txt;while read user_id; do echo &quot;$&#123;user_id&#125;&quot;; clickhouse-client -m -h 192.168.141.141 --port 9003 --query &quot;insert into test_score_mv select user_id, subject, argMax(score, create_time) as score, argMax(isvalid, create_time) as isvalid, max(create_time) as ctime from test_score where user_id = $&#123;user_id&#125; and create_time &lt;= &#x27;2021-12-11 15:01:00&#x27; group by user_id, subject&quot;; done &lt; test_score.csv; 系统相关信息12345678910111213141516171819# cpu和内存使用情况top# 查看内存free# 查看系统时区timedatectl status# 设置系统时区timedatectl set-timezone &quot;Asia/Shanghai&quot;# 查看linux内核版本cat /proc/versionuname -a# 查看linux系统版本lsb_release -acat /etc/issue apt相关apt search XXX：搜索软件包 apt show XXX：显示软件包详情 apt install XXX：安装某个软件 apt depends XXX：查询该包使用的依赖包 apt rdepends XXX：查看该包被哪些包依赖 apt remove XXX：卸载某个软件（保留配置文件） apt –purge remove XXX：卸载某个软件（删除配置文件） apt autoremove XXX：自动清理不再使用的依赖和库文件 apt list –upgradeable：显示可升级的软件包 apt list –installed：显示已安装的软件包 apt update：更新apt仓库包索引 apt upgrade：更新已安装的软件到最新版本 apt dist-upgrade：升级系统到最新版本 软件源镜像1234567891011# 备份软件源配置sudo cp -v /etc/apt/sources.list /etc/apt/sources.list.backup# 修改权限使得这个文件可以编辑sudo chmod 777 /etc/apt/sources.list# 修改sources.list文件，可以把里面的内容删光，重新写入以下内容即可deb http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu/ focal-security main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu/ focal-updates main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu/ focal-proposed main restricted universe multiverse 注意不同的ubuntu版本，sources.list里面的内容不一样，如ubuntu 20是focal，18和16又是另外的。 最后更新一下软件源仓库即可： 1sudo apt update 安装openssh-server 1sudo apt install openssh-server 安装了这个后就可以通过其他主机使用ssh命令远程连接这个服务器。 用户、主机相关恢复模式重启服务器，进入系统的时候按住shift，即可进入一个选项界面，依次选择： Unbuntu高级选项 recovery mode 进入恢复菜单后，可以根据需要选择。 修改root用户密码 先选择grub，进入之后按enter，然后回到这个界面，再选择root，进入root的shell命令行后，使用passwd root命令即可修改root用户的密码。 网络Ubuntu 17.10及以后的版本使用Netplan（/etc/netplan）作为网络管理工具。以前的版本使用ifconfig和/etc/network/interfaces这个配置文件配置网络。以下对Netplan进行配置。 1234# 进入到配置文件夹cd /etc/netplan# 进入后可能有一个或多个yaml文件，文件名视系统版本不定，我这里只有一个文件，因此直接配置这一个文件即可，如下 1234567891011# this is the network written by &#x27;subiquity&#x27;network: ethernets: ens33: addresses: - 192.168.141.142/24 gateway4: 192.168.141.1 nameservers: addresses: [223.5.5.5, 223.6.6.6] # 阿里域名服务器 search: [] version: 2 属性解释 version：版本 renderer：设备类型（如networkd），以上我这里没有出现 ethernets：配置网络，在我这里只有一个网络ens33，这是默认的网络。每个网络可以设置以下属性： dhcp4：使用dhcp服务器自动分配ip和dns，可以填yes或no，使用dhcp服务器后这里再设置ip好像就没用了，没试过 addresses：静态局域网地址，可以配置多个 gateway4：默认网关 nameservers：域名服务器，可以在这个属性下面的addresses属性里面设置多个域名服务器。search属性shows your search domains，不是很明白，可以不用设置。 保存后，应用修改。 1sudo netplan apply 环境变量相关12345678910111213141516171819202122232425# 列出所有已设置的环境变量env# 引用变量名为XXX的变量值，最好加上大括号，好习惯$&#123;XXX&#125;# 查看变量XXX的值echo $&#123;XXX&#125;# 设置环境变量，在相应的profile文件中添加环境变量# 1、设置用户环境变量，在对应的用户文件夹下的profilevi /home/yury/.profile# 在最下面增加以下代码，设置一个变量名为AAA1的环境变量，其值为BBB1export AAA1=BBB1# 2、设置系统环境变量，在/etc/profile下vi /etc/profileexport AAA2=BBB2# 在环境变量中使用引用，将这个路径添加到PATH环境变量末尾export PATH=$&#123;PATH&#125;:$&#123;JAVA_HOME&#125;/bin# 最后，重新加载用户或系统环境变量，即不需要重启就可以使环境变量生效source /home/yury/.profilesource /etc/profile 文件与目录相关cd /：跳转到根目录 cd /XXX：跳转到/XXX目录，父目录为root，而不是当前目录 cd XXX：跳转到当前目录下的XXX目录中 cd ：跳转到目录，~为/home/AAA，AAA为你的用户名 ls：列出该目录下的所有文件夹和文件 touch AAA.TXT：创建AAA.TXT文件 mkdir AAA：在当前目录下继续创建/AAA目录，创建目录时不能写成/AAA，而可以写成AAA/；且只能创建一层目录，不能mkdir AAA/BBB这样创建多层目录 rm AAA.TXT：删除AAA.TXT文件 rmdir AAA：删除AAA文件夹 rm -rf AAA：强制删除AAA文件夹 java相关（版本jdk8）使用apt安装java后安装路径：/usr/lib/jvm/java-8-openjdk-amd64（8为你的java版本，该文件夹名根据你安装的java包名来定）。注：把java安装文件放到/usr/local下面并设置好环境变量，则所有用户都可以使用这个java环境。 1234567891011# 配置JAVA_HOME环境变量export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64# 配置PATH环境变量export PATH=$PATH:$JAVA_HOME/bin# 使环境变量马上生效，不需要重启source ~/.bashrc# 切换java版本update-alternatives --config java 下载、解压123456# curl和wget都可以下载，可这样简单区分使用：curl用于较复杂的不仅仅是下载的web场景，而wget适用于快速且不用担心其他参数的下载curl https://github.com/ziyaddin/xampp/archive/master.zip -L -o MyFilename.zipwget https://github.com/ziyaddin/xampp/archive/master.zip# 解压tar xzvf xxxxxxxxx.tar.gz 服务管理1234567891011121314151617# 查看所有服务systemctl statuc# 查看某个服务的状态systemctl status mysql.service# 禁止开机启动d# 停止已启动的服务sudo systemctl stop mysql.service# 启动服务systemctl start mysql.service# 关闭防火墙systemctl stop firewalld.service 磁盘相关1234567891011121314151617181920212223242526272829303132333435363738394041# 查看磁盘情况df -hT# 进入磁盘分区情况fdisk -lDisk /dev/fd0: 1.4 MiB, 1474560 bytes, 2880 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: dosDisk identifier: 0x90909090Device Boot Start End Sectors Size Id Type/dev/fd0p1 2425393296 4850786591 2425393296 1.1T 90 unknown/dev/fd0p2 2425393296 4850786591 2425393296 1.1T 90 unknown/dev/fd0p3 2425393296 4850786591 2425393296 1.1T 90 unknown/dev/fd0p4 2425393296 4850786591 2425393296 1.1T 90 unknownDisk /dev/sda: 60 GiB, 64424509440 bytes, 125829120 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: gptDisk identifier: A006A120-A37D-4375-8C47-BF69799EA710Device Start End Sectors Size Type/dev/sda1 2048 4095 2048 1M BIOS boot/dev/sda2 4096 2101247 2097152 1G Linux filesystem/dev/sda3 2101248 41940991 39839744 19G Linux filesystem/dev/sda4 41940992 125829086 83888095 40G Linux filesystemDisk /dev/mapper/ubuntu--vg-ubuntu--lv: 19 GiB, 20396900352 bytes, 39837696 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytes# 对某个分区进行管理fdisk /dev/sda4 1、磁盘管理下的常用命令123456m # 显示帮助页面p # 查看分区列表d # 删除分区n # 新建分区q # 不保存直接退出w # 保存并退出 2、磁盘基本知识 platter：盘片，即图中一个个的盘，一个盘片有上下两个盘面 head：磁头，即图中在盘片上滑动的磁头，一个盘片有上下两个盘面，因此对应两个磁头 sector：扇区，相当于图中的一条一条的弧，是磁盘存取的基本单位，1个sector可以是512个byte或4096个byte。 track：磁道，相当于图中的一个圆 cylinder：磁柱，相当于图中的一个圆柱形（不同盘片同一磁道组成的圆柱形），磁柱数等于一个盘面上的磁道数。 block：数据块，是文件系统存取的最小单位，在Windows下如NTFS等文件系统中叫做簇；在Linux下如Ext4等文件系统中叫做块（block）。每个簇或者块可以包括2、4、8、16、32、64…2的n次方个扇区。 一个block最多仅能容纳一个文件（即不存在多个文件同一个block的情况）。如果一个文件比block小，他也会占用一个block，因而block中空余的空间会浪费掉。而一个大文件，可以占多个甚至数十个成百上千万的block。 磁盘容量 = 磁头数 * 磁柱（磁道）数 * 每个磁道的扇区数 * 每个扇区的字节数 3、新建分区流程本流程适用于vmware station对服务器进行扩容。 关闭服务器，在vmware station上的服务器设置中，将硬盘扩展到你需要的大小。 重新进入服务器，fdisk -l命令查看分区列表，找到磁盘/dev/sda，fdisk /dev/sda命令对该磁盘设备进行管理。 键入n命令新建分区，后面如果提示选择extended或者primary partition，则选择primary partition。 然后提示输入分区编号，一般使用提示的默认编号，或者直接回车。 然后提示输入First cylinder（或sector），即新建的分区的起始cylinder（或sector）编号为多少，一般使用默认（最小）即可。 最后提示输入Last cylinder（或sector），即新建的分区的截止cylinder（或sector）编号为多少，一般使用默认（最大）即可。 以上三步有需要可以按自己的需求来选择 键入w命令保存设置 fdisk -l，查看新分区是否生成： mkfs.ext4 /dev/sda4，命令格式化（格式化会清除数据）分区，sda4为你之前输入的新分区编号，ext4为文件系统类型 mkdir disk4，在根目录下创建一个空文件夹，mount /dev/sda4 /disk4，将新分区挂载到新建的文件夹下 echo &#39;/dev/sda4 /disk4 ext4 defaults 0 0&#39; &gt;&gt; /etc/fstab，将这个挂载动作写入一个文件中，保证每次启动服务器时会自动将这个分区挂载到这个disk4目录。 reboot，重启服务器，df -hT，查看磁盘情况 4、分区扩容流程不建议在Linux系统盘所在分区进行扩容，小心丢失数据。且必须有未分区的磁盘用于扩容。 df –hT和fdisk -l命令查看磁盘情况和分区情况 umount /dev/sda4，卸载需要扩容的磁盘挂载的分区 fdisk /dev/sda4，对需要扩容的磁盘分区进行管理 d，删除分区 n，新建分区，然后按新建分区的流程设置分区大小 w，保存设置 e2fsck -f /dev/sda4，检查扩容的分区是否ok resize2fs /dev/sda4，扩容 mount /dev/sda4 /disk4，挂载 df -hT和reboot，查看磁盘情况并重启服务器 service、systemctl相关首先我们得搞清楚你的linux服务器的system manager是谁。一般有SysVinit system manager和Systemd system manage这两种。 1pstree | head -n 5 # 运行这个命令，若输出`systemd`则说明是第二种，若输出`init`则说明是第一种。 1、systemd系统（1）列举所有service12345systemctl list-units --type=service # 查看所有正在运行、或者失败了的units servicesystemctl list-units --type=service --all # 查看所有untis servicesystemctl list-unit-files --type=service # 查看已安装了的所有service list-units和list-unit-files的区别，linux中man命令对他们的解释如下： list-units： List units that systemd currently has in memory. This includes units that are either referenced directly or through a dependency, units that are pinned by applications programmatically, or units that were active in the past and have failed. By default only units which are active, have pending jobs, or have failed are shown; this can be changed with option –all. If one or more PATTERNs are specified, only units matching one of them are shown. The units that are shown are additionally filtered by –type= and –state= if those options are specified. list-unit-files： List unit files installed on the system, in combination with their enablement state (as reported by is-enabled). If one or more PATTERNs are specified, only unit files whose name matches one of them are shown (patterns matching unit file system paths are not supported). 即一个是当前已经加载在内存中的，要么被直接引用或者依赖或者，被其他应用程序固定了，或者曾经启用过但是失败了。另外一个是系统中安装了的。 linux系统在查找已安装了的service时，会按照以下路径加载service： 12345678910111213141516171819202122232425262728System Unit Search Path:/etc/systemd/system.control/*/run/systemd/system.control/*/run/systemd/transient/*/run/systemd/generator.early/*/etc/systemd/system/*/etc/systemd/systemd.attached/*/run/systemd/system/*/run/systemd/systemd.attached/*/run/systemd/generator/*.../lib/systemd/system/*/run/systemd/generator.late/*User Unit Search Path:~/.config/systemd/user.control/*$XDG_RUNTIME_DIR/systemd/user.control/*$XDG_RUNTIME_DIR/systemd/transient/*$XDG_RUNTIME_DIR/systemd/generator.early/*~/.config/systemd/user/*/etc/systemd/user/*$XDG_RUNTIME_DIR/systemd/user/*/run/systemd/user/*$XDG_RUNTIME_DIR/systemd/generator/*~/.local/share/systemd/user/*.../usr/lib/systemd/user/*$XDG_RUNTIME_DIR/systemd/generator.late/* service的所有状态包括：active, inactive, activating, deactivating, failed, not-found, dead （2）注册service注册service即只要在对应的加载路径下放入xxx.service脚本即可，然后启动这个脚本即可。这个路径一般就用/etc/systemd/system/或者/lib/systemd/system/下面，比如MySQL的service脚本就是放在后者路径下。 12345678910111213141516[Unit]Description=Grafana ServerAfter=network-online.target[Service]Type=simple # 服务类型，如果执行程序是linux的可运行文件，则填入simple；如果执行命令是shell脚本方式，则填入forking。User=yury # 启动用户Group=yury # 启用的用户组# Restart=on-failure # 重启策略# RestartSec=30 # 重启时间RuntimeDirectory=/disk4/grafana/grafana-8.4.5/# ExecStart为启动命令ExecStart=/disk4/grafana/grafana-8.4.5/bin/grafana-server --homepath=/disk4/grafana/grafana-8.4.5/[Install]WantedBy=multi-user.target 然后运行systemctl enable grafana.service就可以设置grafana开启服务器后自动启动。 再比如zookeeper的.service脚本如下： 12345678910111213[Unit]Description=Zookeeper ServerAfter=network-online.target[Service]Type=forkingRuntimeDirectory=/disk4/zookeeper/zookeeper-3.6.3ExecStart=/disk4/zookeeper/zookeeper-3.6.3/bin/zkServer.sh startExecStop=/disk4/zookeeper/zookeeper-3.6.3/bin/zkServer.sh stopExecReload=/disk4/zookeeper/zookeeper-3.6.3/bin/zkServer.sh restart[Install]WantedBy=multi-user.target 但是这样的话，启动后就是以root用户运行，增加User又无法启动。所以目前暂时没找到以普通用户启动的方法。 2、SysVinit系统（1）列举所有service1service --status-all # 输出内容左侧的符号意思如下： + : means that the service is running; – : means that the service is not running at all; ? : means that Ubuntu was not able to tell if the service is running or not. 还一种列举所有service的方法是： 1ls -l /etc/init.d/*","categories":[{"name":"linux","slug":"linux","permalink":"https://yury757.github.io/categories/linux/"}],"tags":[]},{"title":"flink_data_warehouse","slug":"bigdata/flink_data_warehouse/flink_data_warehouse","date":"2021-10-29T16:00:00.000Z","updated":"2022-01-02T06:45:28.629Z","comments":true,"path":"bigdata/flink_data_warehouse/flink_data_warehouse/","link":"","permalink":"https://yury757.github.io/bigdata/flink_data_warehouse/flink_data_warehouse/","excerpt":"","text":"前言Flink：1.12.0 java：1.8 一、实时数仓简介1、分层介绍 ODS，数据采集层，采集原始数据、日志、业务数据等 DWD，对ODS层的数据进行初步处理，并根据业务逻辑进行分流 DIM，维度数据，存储一些元数据信息，放在hbase DWM，对于DWD数据对象进行进一步加工，常常是DWD和DIM进行关联，形成宽表 DWS，对DWM数据根据某个主题进行轻度聚合，进行主题宽表，放在clickhouse ADS，把DWS层数据根据可视化需求进行筛选聚合，不存储，而是直接查询，形成一个接口 2、实时计算和离线计算实时计算：输入数据可以以流的方式一个个输入并进行处理，即并不知道全部数据有多大，来一个处理一个。适合运算时间短、计算量级较小的业务。比如当前商品库存，当一个顾客下单后，需要实时对库存数据进行实时处理。 离线计算：计算之前已经知道全部的输入数据，且输入数据并不会发生变化。这种业务一般运算时间长、计算量较大，需要放在后台离线计算。比如计算上个月的网站访问量、去年某商品的销售额。 即席查询：临时的业务，可能是老板为了装逼临时分派给你的查询任务。 Presto：当场计算（基于内存速度块） Kylin：预计算（提前算好），多维分析（各个维度组合的结果都帮你算好） 3、需求 日常统计报表或分析图实时数据变化 实时数据展示 数据预警或提示 实时推荐系统 一、数据采集（ODS）ODS：Operation Data Store，数据准备区。 功能：采集原始数据、日志和业务数据，写入kafka kafka topic：ods_base_log 二、初步处理及分流（DWD）DWD：data warehouse details，细节数据层。 功能：主要对ODS数据层做一些数据清洗、规范化以及分流的操作。 数据清洗：剔除非法值、脏数据等 数据分流：按照不同的业务需求，将数据拆分，输出到下游的kafka的不同topic中。 重点：需要分流的逻辑一般以某种形式写在配置中，而不能在代码中写死。 而在实际工作中，这一层一般会通过搭一个平台来实现动态增加或减少分流逻辑。 实现原理就是将分流逻辑配置在数据库中，再将数据库中这个配置表通过flinkcdc生成一个广播流，将这个广播流和ods流连接合并，就可以根据配置实现动态分流。 三、维度数据（DIM）DIM：Dimension，维度数据，一般就是一些业务基本信息的数据，包含： 高基数维度数据：用户资料、商品资料等业务相关的基本信息。 低基数维度数据：配置表、数据字典等，如业务相关枚举值及其含义等。 问：维度数据为什么不放在redis，而放在hbsae？ 答：有些维度表会随时间扩大，比如用户信息，放redis太占内存，一些在可预见的未来不会膨胀的维度数据实际上是可以放redis的。（该问题一般不能从持久化的角度来回答，实际场景下，肯定会有数据库作为持久化存储） 问：维度数据为什么不直接取数据库，而是取hbase？ 答：数据库一般是业务本身要用来做增删改查的，大数据处理和分析模块再请求数据库，会对数据库造成压力。 四、数据中间层（DWM）DWM：Data WareHouse Middle，数据中间层，即从DWD到DWS中间，会有很多复用的部分，将这些可复用的部分的数据加工提出出来作为一层，避免重复劳动。","categories":[{"name":"bigdata","slug":"bigdata","permalink":"https://yury757.github.io/categories/bigdata/"}],"tags":[]},{"title":"JVM-1.8-memory-and-garbage-collect","slug":"java/JVM/jvm-1.8","date":"2021-09-19T16:00:00.000Z","updated":"2022-10-07T12:55:44.197Z","comments":true,"path":"java/JVM/jvm-1.8/","link":"","permalink":"https://yury757.github.io/java/JVM/jvm-1.8/","excerpt":"","text":"JVM版本：HotSpot 1.8 推荐书籍： 《The Java Virtual Machine Specification》（The Java® Virtual Machine Specification (oracle.com)） 《深入理解java虚拟机——JVM高级特性与最佳实践》 一、JVM简介1、JVM是什么狭义上来说，JVM是java运行的平台。 广义上来说，It is the component of the technology responsible for its hardware- and operating system independence, the small size of its compiled code, and its ability to protect users from malicious programs. 即JVM是一个操作系统或硬件与用户程序之间的一个接口或平台，这个接口可以使用户的程序与不同的操作系统或硬件独立开，只要程序运行在这个平台上，就可以对不同的操作系统或硬件进行相同的操作，就像一个虚拟的计算机，可以执行一系列的虚拟计算机指令。 虚拟机分为两类： 系统虚拟机，虚拟一个操作系统的运行环境（模拟硬件），如VMWare，是操作系统的运行环境，可以安装window、Linux等。 程序虚拟机，虚拟一个普通应用程序的运行环境（模拟软件），如JVM，是二进制字节码的运行环境。 java SE架构：Java Platform Standard Edition 8 Documentation (oracle.com)，JVM处于最底层，即java的运行环境。 2、JVM厂商JVM和JVM规范（JVM Specification）不一样，JVM规范是一套规范，并不是JVM本身，而JVM是基于这套规范的实现，java官网上的JVM只是Oracle（Sun）对JVM规范的一个实现版本，不过还有其他厂商实现的JVM，如： Oracle HotSpot（Java Downloads | Oracle）（里面有很多历史，可以了解一下） Microsoft OpenJDK（Microsoft Build of OpenJDK） Alibaba Dragonwell（开发者平台_开发者中心 (aliyun.com)） Azul OpenJDK（Azure Only Downloads - Azul | Better Java Performance, Superior Java Support） Red Hat OpenJDK（OpenJDK Download | Red Hat Developer） Amazon Corretto（Amazon Corretto-OpenJDK 的免费多平台发行版-AWS云服务） 12345678910111213141516171819# Oraclejava version &quot;13.0.2&quot; 2020-01-14Java(TM) SE Runtime Environment (build 13.0.2+8)Java HotSpot(TM) 64-Bit Server VM (build 13.0.2+8, mixed mode, sharing)# Microsoftopenjdk version &quot;11.0.12&quot; 2021-07-20OpenJDK Runtime Environment Microsoft-25199 (build 11.0.12+7)OpenJDK 64-Bit Server VM Microsoft-25199 (build 11.0.12+7, mixed mode)# Alibabaopenjdk version &quot;1.8.0_302&quot;OpenJDK Runtime Environment (Alibaba Dragonwell 8.8.8) (build 1.8.0_302-b01)OpenJDK 64-Bit Server VM (Alibaba Dragonwell 8.8.8) (build 25.302-b01, mixed mode)# 不同的Linux发行版会提供OpenJDK或其变体作为系统默认的JVM实现openjdk version &quot;1.8.0_292&quot;OpenJDK Runtime Environment (build 1.8.0_292-8u292-b10-0ubuntu1~18.04-b10)OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode) 3、跨语言平台JVM是一个跨语言的平台，只要对应的编译器按照一定的规范（JSR-292）能生成JVM可以识别的字节码文件，就可以运行其他语言的程序，而不仅仅是java。 因此java的强大之处并不在于java语言本身，而更在于JVM的强大。 4、JVM整体结构JVM主要分为三个区域： 类加载子系统 运行时数据区 执行引擎 5、java代码执行流程 java编译器（如javac）编译成.class字节码文件（前端编译器） 词法分析 语法分析 语法/抽象语法树 语义分析 注解抽象语法树 字节码生成器 JVM 类加载 字节码校验 解释器，逐行将字节码翻译成机器指令，解析执行 JIT编译器（后端编译器），对字节码整体进行编译再执行，区别在于JIT会缓存一些热点代码等，优化执行效率 注意：解释器和JIT编译器属于JVM的执行引擎下，可以共存，但是不会同时运行，JIT编译器和解释器只能选其中一种来执行，但是并不是在JVM的整个生命周期内只能选一种运行，而是可以切换运行，根据当前要执行的代码的特征，JVM会选择其中一种来执行。 6、class文件反编译通过javap命令可以对.class文件进行反编译，查看字节码指令。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192javap -v Demo01.classClassfile /D:/Adocument/Java/JVM/out/production/c1-memory-and-garbage-collect/net/yury/demo/Demo01.class Last modified 2021-9-20; size 580 bytes MD5 checksum fa60e763603ba1e3f645376ecc25e0ac Compiled from &quot;Demo01.java&quot;public class net.yury.demo.Demo01 minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPERConstant pool: #1 = Methodref #5.#23 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = Fieldref #24.#25 // java/lang/System.out:Ljava/io/PrintStream; #3 = Methodref #26.#27 // java/io/PrintStream.println:(I)V #4 = Class #28 // net/yury/demo/Demo01 #5 = Class #29 // java/lang/Object #6 = Utf8 &lt;init&gt; #7 = Utf8 ()V #8 = Utf8 Code #9 = Utf8 LineNumberTable #10 = Utf8 LocalVariableTable #11 = Utf8 this #12 = Utf8 Lnet/yury/demo/Demo01; #13 = Utf8 main #14 = Utf8 ([Ljava/lang/String;)V #15 = Utf8 args #16 = Utf8 [Ljava/lang/String; #17 = Utf8 a #18 = Utf8 I #19 = Utf8 b #20 = Utf8 c #21 = Utf8 SourceFile #22 = Utf8 Demo01.java #23 = NameAndType #6:#7 // &quot;&lt;init&gt;&quot;:()V #24 = Class #30 // java/lang/System #25 = NameAndType #31:#32 // out:Ljava/io/PrintStream; #26 = Class #33 // java/io/PrintStream #27 = NameAndType #34:#35 // println:(I)V #28 = Utf8 net/yury/demo/Demo01 #29 = Utf8 java/lang/Object #30 = Utf8 java/lang/System #31 = Utf8 out #32 = Utf8 Ljava/io/PrintStream; #33 = Utf8 java/io/PrintStream #34 = Utf8 println #35 = Utf8 (I)V&#123; public net.yury.demo.Demo01(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return LineNumberTable: line 3: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this Lnet/yury/demo/Demo01; public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=4, args_size=1 0: iconst_4 1: istore_1 2: iconst_3 3: istore_2 4: iload_1 5: iload_2 6: iadd 7: istore_3 8: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 11: iload_1 12: invokevirtual #3 // Method java/io/PrintStream.println:(I)V 15: return LineNumberTable: line 5: 0 line 6: 2 line 7: 4 line 8: 8 line 9: 15 LocalVariableTable: Start Length Slot Name Signature 0 16 0 args [Ljava/lang/String; 2 14 1 a I 4 12 2 b I 8 8 3 c I&#125;SourceFile: &quot;Demo01.java&quot; JVM是基于栈的架构，不同CPU架构不同，因此不能基于寄存器来设计。 7、JVM生命周期 启动：Java虚拟机的启动是通过引导类加载器（bootstrap class loader）创建一个初始类（initial class）来完成的，这个初始类是由虚拟机的具体实现来指定的，不同实现版本的JVM的类可能不一样。 执行：JVM执行的任务就是执行用户程序，即执行一个java程序，实际上是执行一个JVM进程，而用户程序只不过在这个进程上运行。 退出：程序正常终止、异常或错误而终止、操作系统错误终止、某个线程调用Runtime类或System类的exit方法或其他方法手动终止程序等都会导致程序退出，程序终止则JVM也退出。 二、类加载子系统类加载子系统（class loader subsystem）负责加载.class字节码文件。 类加载过程分为一下几个步骤：加载、链接、初始化。 1、加载 通过全限定类名获取定义此类的二进制流（本地文件，网络，动态代理，JSP生成） 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构 在内存中生成一个代表这个类的java.lang.Class对象（详见反射），作为方法区这个类的各种数据的访问入口 2、链接 验证：验证class字节码文件是否符合当前JMV规范，保证类被正确地加载并不会危害JVM本身。主要有：文件格式验证、元数据验证、字节码验证、符号引用验证。 准备：为变量分配内存，并设置该变量的默认值。用final static修饰的变量在编译成class文件时就会分配值，因而这种变量在这个阶段就会直接赋值。这里不会为示例变量分配初始化值，类变量会分配在方法区中，而实例变量是会随着对象一起分配到java堆中。 解析：将内存池中的符号引用转换为直接引用。即比如我们引用了一个java.Lang.String类，会产生一个对这个类的符号引用，等其他类都准备好了之后，将符号引用转换为地址引用这样的直接引用。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型等。 3、初始化这个初始化并不是初始化实例，而是类的初始化。 执行类构造器方法&lt;clinit&gt;()的过程。此方法不需要定义，是javac编译器自动收集类中的所有类变量的赋值动作和静态代码块中的语句合并而来。即执行所有static变量或static代码块相关的初始化操作，若没有这两类操作，则不会产生对应的&lt;clinit&gt;()方法。 初始化顺序是按照代码在源文件中的顺序执行 若该类具有父类，则必须先执行父类的&lt;clinit&gt;()方法，父类初始化完成后，才能初始化子类。 JVM必须保证一个类的&lt;clinit&gt;()方法在多线程中被同步加锁，保证只能被加载一次。 12345678910111213141516171819202122232425262728package net.yury.demo;public class Demo02 &#123; public static void main(String[] args) &#123; Runnable task = ()-&gt;&#123; System.out.println(Thread.currentThread().getName() + &quot;开始&quot;); AnotherClass deadThread = new AnotherClass(); System.out.println(Thread.currentThread().getName() + &quot;结束&quot;); &#125;; Thread thread1 = new Thread(task, &quot;thread-1&quot;); Thread thread2 = new Thread(task, &quot;thread-2&quot;); thread1.start(); thread2.start(); &#125;&#125;class AnotherClass &#123; static &#123; System.out.println(Thread.currentThread().getName() + &quot;正在初始化&quot;); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;初始化结束&quot;); &#125;&#125; 如以上代码执行时最多一个线程进入AnotherClass的static代码块去执行，要么是thread-1，要么是thread-2。 123456thread-1开始thread-2开始thread-1正在初始化thread-1初始化结束thread-1结束thread-2结束 4、类加载器（重点）按被加载的被的类的类型划分，类加载分为一下几种： 引导类加载器（Bootstrap ClassLoader） 扩展类加载器（Extension ClassLoader） 应用程序类加载器（Application ClassLoader） 用户自定义类加载器（意为用户自定义的类加载器，而不是用户自定义类的加载器，没有加载器这个东西，只有类加载器） 而按照JVM的标准，后面三种都叫用户自定义类加载器。所有直接或间接派生于ClassLoader的类加载器都是用户自定义类加载器。如扩展类加载器（ExtClassLoader）间接继承了ClassLoader。 Launcher类是JVM的一个入口应用，后面可以看到扩展了加载器和应用程序类加载器都是Launcher类里面的内部类。 （1）引导类加载器引导类加载器为C/C++编写，用于引导java的核心类库（JAVA_HOME/jre/lib/rt.jar、resources.jar、sun.boot.class.path路径下的内容），用于提供JVM启动运行自身需要的类。 扩展类加载器和应用程序类加载器也是一个类，因此引导类加载器用于还用于加载这两个特殊的类。 出于安全考虑，Bootstrap类加载器只加载包含java、javax、sun等开头的类。 引导类加载器加载的类，调用getClassLoader()方法返回null。 12345678910111213141516171819202122232425262728293031package net.yury.demo;import com.sun.net.ssl.internal.ssl.Provider;import sun.misc.Launcher;import java.net.URL;public class Demo03BootstrapClassLoader &#123; public static void main(String[] args) &#123; System.out.println(&quot;bootstrap引导类加载器加载的路径为：&quot;); URL[] urLs = Launcher.getBootstrapClassPath().getURLs(); for (URL urL : urLs) &#123; System.out.println(urL.toString()); &#125; ClassLoader classLoader = Provider.class.getClassLoader(); System.out.println(classLoader); &#125;&#125;/*bootstrap引导类加载器加载的路径为：file:/D:/Program%20Files%20(x86)/Java/jdk1.8.0_201/jre/lib/resources.jarfile:/D:/Program%20Files%20(x86)/Java/jdk1.8.0_201/jre/lib/rt.jarfile:/D:/Program%20Files%20(x86)/Java/jdk1.8.0_201/jre/lib/sunrsasign.jarfile:/D:/Program%20Files%20(x86)/Java/jdk1.8.0_201/jre/lib/jsse.jarfile:/D:/Program%20Files%20(x86)/Java/jdk1.8.0_201/jre/lib/jce.jarfile:/D:/Program%20Files%20(x86)/Java/jdk1.8.0_201/jre/lib/charsets.jarfile:/D:/Program%20Files%20(x86)/Java/jdk1.8.0_201/jre/lib/jfr.jarfile:/D:/Program%20Files%20(x86)/Java/jdk1.8.0_201/jre/classesnull*/ （2）扩展类加载器java语言编写，由sun.misc.Launcher$ExtClassLoader实现，继承于ClassLoader，该类由Bootstrap引导类加载器加载。 加载的类目录为指定的java.ext.dirs系统属性目录，若没指定则默认为JAVA_HOME/jre/lib/ext，当用户写了一个类放入这个目录下也会被加载。 123456789101112131415161718package net.yury.demo;import sun.security.ec.CurveDB;public class Demo04ExtClassLoader &#123; public static void main(String[] args) &#123; String property = System.getProperty(&quot;java.ext.dirs&quot;); System.out.println(property); ClassLoader classLoader = CurveDB.class.getClassLoader(); System.out.println(classLoader); &#125;&#125;/*D:\\Program Files (x86)\\Java\\jdk1.8.0_201\\jre\\lib\\ext;C:\\WINDOWS\\Sun\\Java\\lib\\extsun.misc.Launcher$ExtClassLoader@4b67cf4d*/ （3）应用程序类加载器应用程序类加载器也叫系统类加载器，java语言编写，由sun.misc.Launcher$AppClassLoader实现，继承于ClassLoader，该类由扩展类加载器加载。 加载的类的目录为指定的java.class.path系统属性目录。 该类加载器是java应用程序的默认类加载器，即java应用的类一般都是由这个类加载器加载的。 123456789101112package net.yury.demo;public class Demo05AppClassLoader &#123; public static void main(String[] args) &#123; ClassLoader classLoader = Demo05AppClassLoader.class.getClassLoader(); System.out.println(classLoader); &#125;&#125;/*sun.misc.Launcher$AppClassLoader@18b4aac2*/ 总结如下： （4）用户自定义类加载器不适用以上三种类加载器，而是用户自定义一个类加载器，来加载需要加载的类。 为什么需要自定义类加载器？ 隔离加载类 修改类的加载方式 扩展加载源 防止源码泄露 定义用户自定义类的两个方法： 继承ClassLoader类，重写findClass方法，JDK1.2之前要重写loadClass方法 继承URLClassLoader类，按照需求重写部分方法 将URLClassLoader类作为一个参考模板，模仿重写一个适应自己需求的一个类 （5）获取类加载器的方式 Class.getClassLoader() Thread.currentThread().getContextClassLoader() ClassLoader.getSystemClassLoader() DriverManager.getCallerCLassLoader() 5、双亲委派机制（重点）JVM对class文件采用的是按需加载的方式加载，即在要用到这个类时才会将这个类加载进内存中生成class对象。而加载某个类时，JVM加载类时采用的是双亲委派机制，是一种任务委派模式。 双亲委派机制原理如下： 当一个类加载器收到加载类的请求时，并不会自己去加载这个类，而是将这个请求委托给父类去加载 父类加载器收到加载请求后，会继续向上委托，最终到达Bootstrap ClassLoader 在这个递归的过程中，如果父类加载器完成了加载过程，则成功返回，否则才让子类加载器去完成类加载，知道这个类完成加载。 注：“父类加载器“的断句为”父 类加载器“，还是一个类加载器，而不是父类的类加载器。类加载器可以通过调用getParent()方法获取其父类加载器。 1234567/** * Returns the parent class loader for delegation. Some implementations may * use &lt;tt&gt;null&lt;/tt&gt; to represent the bootstrap class loader. This method * will return &lt;tt&gt;null&lt;/tt&gt; in such implementations if this class loader&#x27;s * parent is the bootstrap class loader. */public final ClassLoader getParent() &#123;&#125; 就像注释中说的，类加载器在任务委派方面是存在父子这种层次关系的，上面一节中讲到的不同类型的类加载器可以加载那些类，只是说明了它拥有加载这些类的能力，而不是说这些类一定由这个类加载器加载。而某个类最终由哪个类加载器加载，取决于可以加载这个类的最高级别的类加载器。 类加载器在任务委派方面的级别从高到低依次如下： Bootstrap ClassLoader Extension ClassLoader Application ClassLoader UserDefined ClassLoader 例如用户定义了一个java.lang.String类，而用户在使用这个类时，并不会找到用户定义的这个类，而是用了bootstrap classloader加载的rt.jar包下的java.lang.String类。因为通过双亲委派机制加载类时，加载任务不会直接进行，而是从最底层一直往上传递，最顶层的bootstrap classloader可以完成这个类的加载，于是直接完成了加载并返回class对象了，并不会将加载任务继续返还给子类加载器加载。代码略。 机制的优点： 避免类被重复加载 保护程序安全，防止核心api被随意篡改 6、识别同一个类JVM中识别两个对象是否属于同一个类，包含以下两个校验： 全限定类名是否相同 这两个对象的类是否由同一个类加载器加载的 7、类的主动使用和被动使用主动使用包括： 创建类实例 访问类或接口的静态变量，或对该静态变量进行赋值 调用类的静态方法 反射 初始化一个类的子类 JVM启动时被标明为启动类的类 JDK7开始提供的动态语言支持：java.lang.invoke.MethodHandler实例的解析结果 除了以上几种情况为主动使用，其他均为被动使用。类的被动使用不会导致类的初始化，即不会执行对应的&lt;clinit&gt;()方法。 三、运行时数据区1、组成 方法区（JDK1.8叫元空间metaspace，或堆外内存） JVM堆 程序计数器 本地方法栈 JVM栈 其中方法区和JVM堆的生命周期和JVM进程的生命周期一样，而程序计数器、本地方法栈、JVM栈的生命周期和程序线程的生命周期一样，每有一个线程， 就会有一个自身的程序计数器、本地方法栈、JVM栈。 2、线程在HotSpot JVM中，每个线程都与操作系统中的本地线程直接映射。java线程准备好时，操作系统中对应的本地线程也同时创建；java线程终止时，本地线程也会回收。 普通线程：用户应用程序需要执行一定的工作而创建的工作线程。JVM虚拟机在所有普通线程终止时自动终止。 守护线程：驻立在后台的线程，用于服务普通线程的线程，当JVM虚拟机准备终止时，守护线程才会终止。 HotSpot JVM中的守护线程主要有： JVM线程 周期任务线程 GC线程 编译线程 信号调度线程 3、程序计数器（PC寄存器）英文全称：Program Counter Register。它是对CPU的寄存器的一种抽象模拟。 PC寄存器用来存储指向下一条指令的地址，也即将要执行的指令代码。由执行引擎读取下一条指令。它是程序控制流的指示器，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。可以理解为数据库中的游标，或集合的迭代器。 线程私有，和线程的生命周期一致。 没有GC，且是唯一一个在JVM规范中没有规定任何OOM情况的区域。 1234567891011package net.yury.demo;public class Demo06PCRegister &#123; public static void main(String[] args) &#123; int i = 10; int j = 20; int k = i + j; String s = &quot;yury&quot;; System.out.println(s); &#125;&#125; 对于上面这段代码反编译后的指令如下： 问题1：为什么要用PC寄存器？ 因为CPU需要不断地在各个线程之间切换运行，需要有一个东西记录CPU需要运行的下一条指令的位置，不然CPU从其他线程切换回来不知道从哪里开始。 问题2：PC寄存器为什么是线程私有的？ 因为PC寄存器是记录每个线程的下一条指令的位置，和线程相关，因此要每个线程独享一份PC寄存器。如果设计成公用的，则需要将对应线程的id也记录进去（即一种key-value形式的数据结构存储），而在创建线程和销毁线程时还要对这个数据结构进行操作，又涉及并发问题，稍显麻烦。而PC寄存器的占用内存及其小，因此设计成和线程绑定，随着线程生命周期创建或销毁，使用起来更方便。 4、JVM栈英文全称：Java Virtual Machine Stacks。 栈是运行时的单位，而堆是存储的单位。栈解决程序运行的问题，即程序如何运行，或者说如何处理数据。堆解决的是数据存储的问题，即数据怎么放，放在那里。 JVM栈内部存储的是一个个的栈帧，对应着一次次的方法调用，当前线程每调用一个方法，则将该方法入栈，栈顶元素即为正在运行的方法，当前方法运行结束后则栈顶元素出栈。主管java程序的运行，每个栈帧中保存方法的局部变量、部分结果，并参与方法的调用和返回。 线程私有，和线程的生命周期一致。 不存在GC，但是存在OOM。 JVM允许JVM栈的大小是动态的或者固定不变的。若栈大小是固定的，则可能存在StackOverflowError异常；若栈大小是动态的，则可能存在OutOfMemoryError异常。 设置栈的大小：在JVM参数设置里面加上-Xss1024即可设置栈空间的大小；默认单位是bytes，加上k则以kb为单位，同样加上m则以mb为单位。 （1）栈帧栈帧是一个内存区块，是一个数据集，维系着方法执行过程中的各种数据集。 在一个活动线程中，一个时间点上只可能有一个活动的栈帧，即栈顶栈帧，对应着当前正在运行的方法。 1234567891011121314151617181920package net.yury.demo;public class Demo07JVMStackTest &#123; public static void main(String[] args) &#123; Demo07JVMStackTest demo = new Demo07JVMStackTest(); demo.test1(); &#125; public void test1()&#123; int a = 10; test2(); &#125; public void test2()&#123; int b = 20; test3(); &#125; public void test3()&#123; int c = 30; &#125;&#125; 如在运行以上代码时，通过debug手动控制程序的运行时，会发现每当进入一个方法时，下图的顶端方法就会变成当前方法。每执行完一个方法时，顶端方法就会移除。 注意：不同线程中的所包含的栈帧是不允许存在相互引用的，即不可能在一个栈帧中引用另外一个线程的栈帧。即不同栈之间的栈帧时隔离的。 返回类型 函数有两种返回方式，一是正常返回，而是异常返回。这两种返回方式都会导致栈帧被弹出。 正常返回时，使用return指令，返回值会被传给下一个栈帧，接着当前栈帧就被JVM丢弃。返回值为void其实也有一个return。 异常返回时，抛出的异常会传给下一个栈帧去处理，如果下一个栈帧没有处理该异常，则继续往前抛，直到有一个函数可以处理这个异常。 栈帧内部结构 局部变量表 操作数栈（或表达式栈） 动态链接（或指向运行时常量池的方法引用） 方法返回地址（或方法正常退出或异常退出的定义） 其他附加信息 （2）局部变量表局部变量表（local variables），定义为一个数字数组，主要用于存储方法参数和定义在方法体内的局部变量，如字节码指令中的store、load等指令都会对局部变量表的操作，这些数据类型包括基本数据类型、对象引用（reference），以及returnAddress类型。 局部变量表中最基本的存储单元是slot（变量槽）。32位以内的类型只占用一个slot（包括returnAddress类型），64位的类型（long和double）占用两个slot。 boolean、byte、short、char在存储之前都会被转换成int，占用一个slot float为占用一个字节，故占用一个slot long和double则占用两个slot 引用类型占用一个slot 局部变量表是和线程绑定的，不存在线程安全的问题。 局部变量表所需的容量大小是在编译器就确定下来了，并保存在方法的Code属性的locals数据项中，在方法运行期间是不会改变局部变量表的大小的。 局部变量表中的变量是重要的垃圾回收的根节点，只要被局部变量表中直接或间接引用的对象都不会被回收。 如下这段代码，在javap命令下展示的局部变量表如下所示。locals即为局部变量表的容量（数字数组的长度），L开头表示引用类型。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package net.yury.demo;public class Demo08Slot &#123; public static void main(String[] args) &#123; String res = test(args); &#125; public static String test(String[] args)&#123; long a = 1L; double b = 2; float c = 3F; int d = 3; short e = 4; byte f = 5; boolean g = true; char h = &#x27;a&#x27;; String i = &quot;123&quot;; return i + &quot;456&quot;; &#125;&#125;/*public static java.lang.String test(java.lang.String[]); descriptor: ([Ljava/lang/String;)Ljava/lang/String; flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=12, args_size=1 0: lconst_1 1: lstore_1 2: ldc2_w #3 // double 2.0d 5: dstore_3 6: ldc #5 // float 3.0f 8: fstore 5 10: iconst_3 11: istore 6 13: iconst_4 14: istore 7 16: iconst_5 17: istore 8 19: iconst_1 20: istore 9 22: bipush 97 24: istore 10 26: ldc #6 // String 123 28: astore 11 30: new #7 // class java/lang/StringBuilder 33: dup 34: invokespecial #8 // Method java/lang/StringBuilder.&quot;&lt;init&gt;&quot;:()V 37: aload 11 39: invokevirtual #9 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 42: ldc #10 // String 456 44: invokevirtual #9 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 47: invokevirtual #11 // Method java/lang/StringBuilder.toString:()Ljava/lang/String; 50: areturn LineNumberTable: line 8: 0 line 9: 2 line 10: 6 line 11: 10 line 12: 13 line 13: 16 line 14: 19 line 15: 22 line 16: 26 line 17: 30 LocalVariableTable: Start Length Slot Name Signature 0 51 0 args [Ljava/lang/String; 2 49 1 a J 6 45 3 b D 10 41 5 c F 13 38 6 d I 16 35 7 e S 19 32 8 f B 22 29 9 g Z 26 25 10 h C 30 21 11 i Ljava/lang/String;*/ 局部变量表在LocalVariableTable下， start：该局部变量开始生效的字节码行号 length：该局部变量开始生效的字节码行的数量，start + length一定等于该方法的总字节码行数 slot：占据的槽位的编号 name：局部变量名 signature：局部变量类型缩写 最上面Code：下面的locals即为局部变量表的大小，而stack为操作数栈的长度。 LineNumberTable这个表指的是源代码中行号（冒号左边）和字节码行号（冒号右边）的对应关系 注意：非静态方法，会把对自身对象的引用this放在第0个位置的slot处作为“局部变量”，甚至方法的参数都要放在这个“局部变量”后面。而静态方法中，不会将自身对象的应用放到局部变量表中，因此在静态方法中使用this会报错。如下： 123456789101112131415161718192021222324252627282930313233public String test1(long i) throws FileNotFoundException &#123; FileInputStream fileInputStream = new FileInputStream(&quot;&quot;); this.test2(); return this.name;&#125;/*public java.lang.String test1(long) throws java.io.FileNotFoundException; descriptor: (J)Ljava/lang/String; flags: ACC_PUBLIC Code: stack=3, locals=4, args_size=2 0: new #12 // class java/io/FileInputStream 3: dup 4: ldc #13 // String 6: invokespecial #14 // Method java/io/FileInputStream.&quot;&lt;init&gt;&quot;:(Ljava/lang/String;)V 9: astore_3 10: aload_0 11: invokevirtual #15 // Method test2:()V 14: aload_0 15: getfield #16 // Field name:Ljava/lang/String; 18: areturn LineNumberTable: line 21: 0 line 22: 10 line 23: 14 LocalVariableTable: Start Length Slot Name Signature 0 19 0 this Lnet/yury/demo/Demo09Method; 0 19 1 i J 10 9 3 fileInputStream Ljava/io/FileInputStream; Exceptions: throws java.io.FileNotFoundException*/ slot的重复利用 局部变量表中的slot槽位是可以重复利用的。当一个局部变量过了其作用域时，那么后面申明的局部变量可以利用前面过期的局部变量的slot槽位，以达到节省资源的作用。 如下，c和d利用了过期了的b的slot槽位。 123456789101112131415161718192021222324252627282930313233343536373839404142public void test3()&#123; int a = 0; &#123; long b = 100L; b = a; &#125; int c = 0; char d = &#x27;a&#x27;;&#125;/*public void test3(); descriptor: ()V flags: ACC_PUBLIC Code: stack=2, locals=4, args_size=1 0: iconst_0 1: istore_1 2: ldc2_w #4 // long 100l 5: lstore_2 6: iload_1 7: i2l 8: lstore_2 9: iconst_0 10: istore_2 11: bipush 97 13: istore_3 14: return LineNumberTable: line 31: 0 line 33: 2 line 34: 6 line 36: 9 line 37: 11 line 38: 14 LocalVariableTable: Start Length Slot Name Signature 6 3 2 b J 0 15 0 this Lnet/yury/demo/Demo09Method; 2 13 1 a I 11 4 2 c I 14 1 3 d C*/ （3）操作数栈操作数栈（operand stack），即在方法执行过程中，根据字节码指令，往栈中写入数据或提取数据的一个临时存储空间，主要用于保存一些指令需要用的临时数据。如指令中的push、store、load等指令，都会对操作数栈进行操作。 JVM的执行引擎是基于栈的执行引擎，其中这里说的栈，就是操作数栈。 JVM操作数栈用数组来实现，操作数栈的大小在编译时即确定了，对操作数栈的操作只有入栈和出栈。 操作数栈中的任何一个元素的都可以是任意类型的java数据类型，只是占用的栈单位不一样： 32位及以下的类型占用一个栈单位深度，以int类型存放 64位类型占用两个栈单位深度 指令分析 123456789package net.yury.demo;public class Demo10OperandStack &#123; public void method1() &#123; byte m = 15; int n = 8; int k = m + n; &#125;&#125; 12345678910111213141516171819202122232425public void method1(); descriptor: ()V flags: ACC_PUBLIC Code: stack=2, locals=4, args_size=1 0: bipush 15 2: istore_1 3: bipush 8 5: istore_2 6: iload_1 7: iload_2 8: iadd 9: istore_3 10: return LineNumberTable: line 5: 0 line 6: 3 line 7: 6 line 8: 10 LocalVariableTable: Start Length Slot Name Signature 0 11 0 this Lnet/yury/demo/Demo10OperandStack; 3 8 1 m B 6 5 2 n I 10 1 3 k I 以上指令是如何一步一步进行的，如下图所示： 注意：对于int n = 8;这段代码，虽然指定的是int型，但是数值不超过byte的范围，所以编译后的类型是byte。即并不是我们指定什么类型，它就是什么类型，前端编译器会对我们的代码做优化。 对于有返回值的函数，调用这个函数时，会将函数结果压入栈顶，如下所示。sum方法和上面的method方法的指令，除了ireturn有区别外，其他均无区别。getSum方法的指令中，aload_0是将this自身对应的引用从局部变量表中复制出来，放入操作数栈中；invokevirtual指令则是取出栈顶元素，以该元素为对象，调用对象中的一个方法，返回的结果会被重新压入栈顶；istore_1则是将栈顶元素出栈，即取出刚才的函数返回值，放入局部变量表中的1号slot槽位。 1234567891011public void getSum()&#123; int a = sum(); int b = 10000;&#125;public int sum()&#123; byte m = 15; int n = 8; int k = m + n; return k;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public void getSum(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=3, args_size=1 0: aload_0 1: invokevirtual #2 // Method sum:()I 4: istore_1 5: sipush 10000 8: istore_2 9: return LineNumberTable: line 11: 0 line 12: 5 line 13: 9 LocalVariableTable: Start Length Slot Name Signature 0 10 0 this Lnet/yury/demo/Demo10OperandStack; 5 5 1 a I 9 1 2 b I public int sum(); descriptor: ()I flags: ACC_PUBLIC Code: stack=2, locals=4, args_size=1 0: bipush 15 2: istore_1 3: bipush 8 5: istore_2 6: iload_1 7: iload_2 8: iadd 9: istore_3 10: iload_3 11: ireturn LineNumberTable: line 16: 0 line 17: 3 line 18: 6 line 19: 10 LocalVariableTable: Start Length Slot Name Signature 0 12 0 this Lnet/yury/demo/Demo10OperandStack; 3 9 1 m B 6 6 2 n I 10 2 3 k I i++与++i的区别 先通过字节码分析，可以发现，n++是先将数据从局部变量表中复制到操作数栈中，再对局部变量表中slot为1的变量的值进行++操作，最后将操作数栈中栈顶元素store到新的局部变量中，而++n是先在局部变量表中slot为1的变量的值进行++操作，再load到操作数栈中，最后store到新的局部变量中。即load和++操作的执行顺序不一样，这就导致n++返回的是n，而++n返回的是(n+1)。 12345public void test()&#123; int n = 1000; int m = n++; int k = ++n;&#125; 12345678910111213141516171819202122232425public void test(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=4, args_size=1 0: sipush 1000 3: istore_1 4: iload_1 5: iinc 1, 1 8: istore_2 9: iinc 1, 1 12: iload_1 13: istore_3 14: return LineNumberTable: line 23: 0 line 24: 4 line 25: 9 line 26: 14 LocalVariableTable: Start Length Slot Name Signature 0 15 0 this Lnet/yury/demo/Demo10OperandStack; 4 11 1 n I 9 6 2 m I 14 1 3 k I 栈顶缓存技术（top-of-stack caching） 由于JVM是基于栈设计的，因此在不断的执行指令过程中，经常会有入栈出栈的操作，这就意味着存在多次的内存读写操作，会对整体运行速度有所影响。因此HotSpot JVM对此处做了一个缓存，即将所有栈顶元素缓存到物理CPU寄存器中，依次降低对内存的读写，提高执行效率。 CPU还有一个东西叫高速缓存（一级、二级、三级），这个东西是集成到CPU内和CPU完全独立的一个器件，作为CPU的临时数据缓存区。而CPU寄存器是属于CPU本身的，因此CPU对寄存器的读写速度比对高速缓存的读写速度快得多。这一块是硬件相关的东西，不同架构的硬件设计不一样，可以去学习学习。 （4）动态链接之前说过，每个栈帧都对应一个方法的执行，那么JVM怎么知道这个栈帧是对应的哪个方法呢，这就是通过动态链接来实现的。 每个栈帧中都包含了一个指向运行时常量池中的该栈帧对应的方法类型的引用，这样字节码文件中的指令就可以支持使用动态链接。 动态链接（Dynamic Linking），将符号方法引用转换为直接方法引用，加载符号引用对应的类，将对变量的访问转换为这些变量在对应的存储结构中的对应的偏移量。 java代码被编译成字节码指令后，所有的类、变量和方法都保存在运行时常量池中，并都指定了一个通过#开头的独一无二的符号引用作为这个变量或方法的引用。动态链接就是将这个符号引用转换为直接引用，从而实现方法的调用、类的加载和变量的使用。 比如下面这个类的运行时常量池和test()方法的字节码。运行时常量池中记录了这个类所需要的字节码对象，左边的#加数字即为对应的符号引用，test()方法中的指令需要用到某个对象时，则使用这个引用即可。 JVM的运行时常量池可以把其他字节码文件中的对象在本字节码文件中设置引用，但动态链接只能链接本字节码文件中运行时常量池中包含的对象，无法直接链接到外部字节码文件的对象。 12345678910111213141516171819package net.yury.demo;public class Demo11DynamicLinking &#123; public static void main(String[] args) &#123; new Demo11DynamicLinking().test(); &#125; public void test(int i)&#123; &#125; public void test()&#123; String test = Demo08Slot.test(null); Demo10OperandStack demo = new Demo10OperandStack(); demo.test(); test(10); System.out.println(test); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101Constant pool: #1 = Methodref #14.#36 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = Class #37 // net/yury/demo/Demo11DynamicLinking #3 = Methodref #2.#36 // net/yury/demo/Demo11DynamicLinking.&quot;&lt;init&gt;&quot;:()V #4 = Methodref #2.#38 // net/yury/demo/Demo11DynamicLinking.test:()V #5 = Class #39 // java/lang/String #6 = String #40 // 123456 #7 = Methodref #41.#42 // net/yury/demo/Demo08Slot.test:([Ljava/lang/String;)Ljava/lang/String; #8 = Class #43 // net/yury/demo/Demo10OperandStack #9 = Methodref #8.#36 // net/yury/demo/Demo10OperandStack.&quot;&lt;init&gt;&quot;:()V #10 = Methodref #8.#38 // net/yury/demo/Demo10OperandStack.test:()V #11 = Methodref #2.#44 // net/yury/demo/Demo11DynamicLinking.test:(I)V #12 = Fieldref #45.#46 // java/lang/System.out:Ljava/io/PrintStream; #13 = Methodref #47.#48 // java/io/PrintStream.println:(Ljava/lang/String;)V #14 = Class #49 // java/lang/Object #15 = Utf8 &lt;init&gt; #16 = Utf8 ()V #17 = Utf8 Code #18 = Utf8 LineNumberTable #19 = Utf8 LocalVariableTable #20 = Utf8 this #21 = Utf8 Lnet/yury/demo/Demo11DynamicLinking; #22 = Utf8 main #23 = Utf8 ([Ljava/lang/String;)V #24 = Utf8 args #25 = Utf8 [Ljava/lang/String; #26 = Utf8 test #27 = Utf8 (I)V #28 = Utf8 i #29 = Utf8 I #30 = Utf8 testString #31 = Utf8 Ljava/lang/String; #32 = Utf8 demo #33 = Utf8 Lnet/yury/demo/Demo10OperandStack; #34 = Utf8 SourceFile #35 = Utf8 Demo11DynamicLinking.java #36 = NameAndType #15:#16 // &quot;&lt;init&gt;&quot;:()V #37 = Utf8 net/yury/demo/Demo11DynamicLinking #38 = NameAndType #26:#16 // test:()V #39 = Utf8 java/lang/String #40 = Utf8 123456 #41 = Class #50 // net/yury/demo/Demo08Slot #42 = NameAndType #26:#51 // test:([Ljava/lang/String;)Ljava/lang/String; #43 = Utf8 net/yury/demo/Demo10OperandStack #44 = NameAndType #26:#27 // test:(I)V #45 = Class #52 // java/lang/System #46 = NameAndType #53:#54 // out:Ljava/io/PrintStream; #47 = Class #55 // java/io/PrintStream #48 = NameAndType #56:#57 // println:(Ljava/lang/String;)V #49 = Utf8 java/lang/Object #50 = Utf8 net/yury/demo/Demo08Slot #51 = Utf8 ([Ljava/lang/String;)Ljava/lang/String; #52 = Utf8 java/lang/System #53 = Utf8 out #54 = Utf8 Ljava/io/PrintStream; #55 = Utf8 java/io/PrintStream #56 = Utf8 println #57 = Utf8 (Ljava/lang/String;)V public void test(); descriptor: ()V flags: ACC_PUBLIC Code: stack=4, locals=4, args_size=1 0: iconst_1 1: anewarray #5 // class java/lang/String 4: dup 5: iconst_0 6: ldc #6 // String 123456 8: aastore 9: astore_1 10: aload_1 11: invokestatic #7 // Method net/yury/demo/Demo08Slot.test:([Ljava/lang/String;)Ljava/lang/String; 14: astore_2 15: new #8 // class net/yury/demo/Demo10OperandStack 18: dup 19: invokespecial #9 // Method net/yury/demo/Demo10OperandStack.&quot;&lt;init&gt;&quot;:()V 22: astore_3 23: aload_3 24: invokevirtual #10 // Method net/yury/demo/Demo10OperandStack.test:()V 27: aload_0 28: bipush 10 30: invokevirtual #11 // Method test:(I)V 33: getstatic #12 // Field java/lang/System.out:Ljava/io/PrintStream; 36: aload_2 37: invokevirtual #13 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 40: return LineNumberTable: line 16: 0 line 17: 10 line 18: 15 line 19: 23 line 20: 27 line 21: 33 line 22: 40 LocalVariableTable: Start Length Slot Name Signature 0 41 0 this Lnet/yury/demo/Demo11DynamicLinking; 10 31 1 testString [Ljava/lang/String; 15 26 2 test Ljava/lang/String; 23 18 3 demo Lnet/yury/demo/Demo10OperandStack; 早期绑定和晚期绑定 绑定是一个字段、方法或类在符号引用被转换成直接引用的过程。 早期绑定：如果被调用的目标的类型在编译期间就是确定的，且运行期间保持不变，则这个目标的绑定过程叫早期绑定。 晚期绑定：如果被调用的目标的类型在编译期间无法确定，只能在运行期间确定，则这个目标的绑定过程叫晚期绑定。 对于方法而言，有早期绑定和晚期绑定又叫静态链接和动态链接。 静态链接：如果被调用的方法类型在编译期间就是确定下来的，并且运行期间保持不变，这种情况下将调用的方法的符号引用转换为直接引用的过程叫静态链接。 动态链接：如果被调用的方法类型在编译期间无法确定，只能在运行期间将调用的方法的符号引用转换为直接引用，这个过程叫动态链接。 虚方法和非虚方法 虚方法：编译期间无法确定的方法叫虚方法； 非虚方法：编译期间就可以确定，且运行期间无法修改的方法，叫非需方法。 虚方法的调用使用在字节码层面会使用invokevirtual和invokeinterface两个指令；而非虚方法的调用在字节码层面使用invokestatic和invokespecial指令。final方法外除外，final方法的调用仍然使用invokevirtual指令。 invokestatic：调用静态方法 invokespecial：调用父类方法、私有方法、构造器方法 invokevirtual：调用其他普通方法或final方法 invokeinterface：调用接口方法 invokedynamic：使用函数式接口或lamdba表达式生成一个对象时使用的指令 不同方法的调用使用的字节码指令如下。值得注意的是，下面这段代码中，Function接口通过lamdba表达式和new的方式使用的底层指令不一样，lamdba使用invokedynamic，而new实际上是生成了一个匿名内部类Demo12DynamicLinking2$1，并初始化这个匿名内部类，因此使用了invokespecial。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package net.yury.demo;import java.util.function.Function;public class Demo12DynamicLinking2 extends Parent implements Interface&#123; public static void main(String[] args) &#123; &#125; public static void test1()&#123; &#125; private void test2()&#123; &#125; public void test3()&#123; &#125; protected void test4()&#123; &#125; public final void test5()&#123; &#125; @Override public void test6()&#123; System.out.println(&quot;child&quot;); &#125; public final void test7()&#123; &#125; @Override public void test8()&#123; &#125; public void method(Interface demo)&#123; test1(); test2(); test3(); test4(); test5(); test6(); super.test6(); test7(); test8(); demo.test8(); Function&lt;String, String&gt; function1 = s -&gt; &#123; return s; &#125;; Function&lt;String, String&gt; function2 = new Function&lt;String, String&gt;() &#123; @Override public String apply(String s) &#123; return s; &#125; &#125;; String s1 = function1.apply(&quot;123&quot;); String s2 = function2.apply(&quot;456&quot;); &#125;&#125;interface Interface&#123; public void test8();&#125;class Parent&#123; public void test6()&#123; System.out.println(&quot;parent&quot;); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738stack=3, locals=6, args_size=2 0: invokestatic #5 // Method test1:()V 3: aload_0 4: invokespecial #6 // Method test2:()V 7: aload_0 8: invokevirtual #7 // Method test3:()V 11: aload_0 12: invokevirtual #8 // Method test4:()V 15: aload_0 16: invokevirtual #9 // Method test5:()V 19: aload_0 20: invokevirtual #10 // Method test6:()V 23: aload_0 24: invokespecial #11 // Method net/yury/demo/Parent.test6:()V 27: aload_0 28: invokevirtual #12 // Method test7:()V 31: aload_0 32: invokevirtual #13 // Method test8:()V 35: aload_1 36: invokeinterface #14, 1 // InterfaceMethod net/yury/demo/Interface.test8:()V 41: invokedynamic #15, 0 // InvokeDynamic #0:apply:()Ljava/util/function/Function; 46: astore_2 47: new #16 // class net/yury/demo/Demo12DynamicLinking2$1 50: dup 51: aload_0 52: invokespecial #17 // Method net/yury/demo/Demo12DynamicLinking2$1.&quot;&lt;init&gt;&quot;:(Lnet/yury/demo/Demo12DynamicLinking2;)V 55: astore_3 56: aload_2 57: ldc #18 // String 123 59: invokeinterface #19, 2 // InterfaceMethod java/util/function/Function.apply:(Ljava/lang/Object;)Ljava/lang/Object; 64: checkcast #20 // class java/lang/String 67: astore 4 69: aload_3 70: ldc #21 // String 456 72: invokeinterface #19, 2 // InterfaceMethod java/util/function/Function.apply:(Ljava/lang/Object;)Ljava/lang/Object; 77: checkcast #20 // class java/lang/String 80: astore 5 82: return 继承和方法重写的本质 java的类可能会有继承关系，而调用一个子类的方法时，JVM怎么知道这个方法是子类重写的方法，还是父类的方法呢。JVM会在编译期做以下操作。 将该方法所属的对象引用加载到操作数栈（如果调用静态方法则省略这一步） 找到操作数栈栈顶的第一个元素所执行的对象的实际类型，记作 C 如果在运行时常量池的类型C中找到参数和返回值类型都和调用的方法相同的方法，则再进行权限校验，如果通过则返回这个方法的直接引用，如果权限校验不通过，则返回java.lang.IllegalAccessError异常； 如果没找到类型C中没找到这种方法，则按照类继承关系依次往父类执行第3步查找和权限校验。 如果最终无法正常返回一个方法的直接引用，则抛出异常。 因此如果子类有这个方法则调用的是子类的方法，如果没有，则调用的是第一个有该方法的父类方法。 （5）方法返回地址正常返回：方法返回地址存放调用该方法的pc寄存器的值，即返回下一条将要执行的指令的地址。 异常返回：通过异常表来确定，栈帧中一般不保存这部分信息。 方法返回的本质： 当前栈帧出栈 返回到上层方法的局部变量表、操作数栈，将当前方法的返回值压入操作数栈（如果上层方法要使用的话） 设置pc寄存器的值 return指令根据返回值类型的不同分为以下几种： return：返回void ireturn：返回32位的类型，如boolean、byte、short、char、int dreturn：返回double freturn：返回float lreturn：返回long areturn：返回引用类型，如所有的类 异常处理表： 从from这一行字节码开始，到to这一行字节码，如果出现了type类型，则从target行指令继续执行。如下Exception table所示。 12345678910111213141516171819package net.yury.demo;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.util.Arrays;public class Demo09Method &#123; private String name; public static void main(String[] args) &#123; Demo09Method demo = new Demo09Method(); String s; try&#123; s = demo.test1(100L); System.out.println(s); &#125;catch (FileNotFoundException ex)&#123; System.out.println(Arrays.toString(ex.getStackTrace())); &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=3, locals=4, args_size=1 0: new #2 // class net/yury/demo/Demo09Method 3: dup 4: invokespecial #3 // Method &quot;&lt;init&gt;&quot;:()V 7: astore_1 8: aload_1 9: ldc2_w #4 // long 100l 12: invokevirtual #6 // Method test1:(J)Ljava/lang/String; 15: astore_2 16: getstatic #7 // Field java/lang/System.out:Ljava/io/PrintStream; 19: aload_2 20: invokevirtual #8 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 23: goto 40 26: astore_3 27: getstatic #7 // Field java/lang/System.out:Ljava/io/PrintStream; 30: aload_3 31: invokevirtual #10 // Method java/io/FileNotFoundException.getStackTrace:()[Ljava/lang/StackTraceElement; 34: invokestatic #11 // Method java/util/Arrays.toString:([Ljava/lang/Object;)Ljava/lang/String; 37: invokevirtual #8 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 40: return Exception table: from to target type 8 23 26 Class java/io/FileNotFoundException LineNumberTable: line 10: 0 line 13: 8 line 14: 16 line 17: 23 line 15: 26 line 16: 27 line 18: 40 （6）一些附加信息略。 （7）问题 栈相关的报错有哪些？ stackoverflow，栈溢出，方法调用的次数太多以致于栈耗费的空间超过了我们设置的栈大小，可以通过修改-Xss设置栈大小。 outofmemory，内存不足，栈空间设置足够大，但是方法调用过多，导致整体内存不足。（OOM很少出现在栈空间发生） 调整栈空间大小，可以保证不出现溢出吗？ 不能。若使用固定大小的栈，即使调整了栈空间大小，他也是确定的；而程序方法的调用如在出现递归的情况下，调用次数是不确定的，有可能会出现栈溢出的情况。 分配栈空间内存越大越好吗？ 服务器资源是有效的，栈空间内存分配越大，则其他资源（如堆）分配到的空间就受限。应根据服务器和应用程序的实际情况分配。 垃圾回收会设计到栈空间吗？ 不会，栈没有垃圾回收。 java对象一定都是在对空间上创建的吗？ 不一定，栈也可以创建对象。堆那边再聊。 局部变量是线程安全的吗？ 若该局部变量是方法内部产生，且方法内部销毁的，则这个局部变量是线程安全的。 若该局部变量以参数形式传入方法，或者作为返回值返回出去的，则这个局部变量是线程不安全的。 即只要这个局部变量的生命周期完全在这个方法内，则是线程安全的；否则是线程不安全的。 一个对象的指针或引用被多个方法或线程使用，即称这个对象出现了逃逸（Escape）。全局变量如类属性和私有属性可能会被多个方法或线程使用，这种线程安全问题很常见；而局部变量如果其作用域不仅限于该方法，还被其他方法或线程使用了，即出现了逃逸，则也可能会存在线程安全问题。具体见后面的逃逸分析。 最稳妥的方式是，避免局部变量逃逸到其他方法或线程中，如 方法参数尽量使用线程安全的对象，或者不可变对象（如String），或者使用私有属性的方式代替传入参数的形式来定义方法。 避免将局部变量作为方法返回值，或者以一个线程安全的或不可变的对象的形式返回（如返回StringBuilder.ToString()）。 坚守一个原则：对象的作用域尽量维持在使用它的最小作用域 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package net.yury.demo;public class Demo13ThreadSafe &#123; public static void main(String[] args) throws InterruptedException &#123; method2(); &#125; public static void method1() throws InterruptedException &#123; StringBuilder sb = new StringBuilder(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10000; i++) &#123; sb.append(&#x27;a&#x27;); &#125; &#125;).start(); new Thread(() -&gt; add(sb)).start(); Thread.sleep(1000); System.out.println(sb.toString()); &#125; public static void add(StringBuilder sb)&#123; for (int i = 0; i &lt;10000; i++) &#123; sb.append(&#x27;b&#x27;); &#125; &#125; public static void method2() throws InterruptedException &#123; StringBuilder sb = add2(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10000; i++) &#123; sb.append(&#x27;1&#x27;); &#125; &#125;).start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10000; i++) &#123; sb.append(&#x27;2&#x27;); &#125; &#125;).start(); Thread.sleep(1000); System.out.println(sb.toString()); &#125; public static StringBuilder add2()&#123; StringBuilder sb = new StringBuilder(); sb.append(&quot;abc&quot;); return sb; &#125;&#125; 5、本地方法栈本地方法栈（Nativa Method Stack）用于管理本地方法的调用，本地方法栈也是线程私有的。 本地方法：使用native关键字修饰的方法，由C/C++实现。 本地方法栈的大小和异常种类和普通的JVM栈是相同的，允许被实现成固定大小或者动态扩展的，溢出和OOM都有可能存在。 本地方法栈的具体做法就是执行本地方法时，在本地方法栈中压入本地方法，然后让执行引擎从本地方法库中加载这个本地方法，再让本地方法执行，最后本地方法栈栈顶元素出栈。 本地方法接口就是本地方法运行时和JVM沟通的接口，本地方法库就是java所有本地方法的集合。 当某个线程调用本地方法时，本地方法执行的权限和JVM有着相同的权限。 本地方法可以通过本地方法接口来访问JMV内部的运行时数据区。 本地方法可以直接使用本地处理器中的寄存器 本地方法可以直接从本地内存堆中分配任意数量的内存。 并不是所有的JVM都支持本地方法，因为JVM规范中并没有明确要求本地方法栈的使用语言、具体实现方式、数据结构等，如果JVM产品不打算支持本地方法，那么无需实现本地方法栈也是可以的。 在HotSpot JVM中，直接将JVM栈和本地方法栈合二为一。 6、JVM堆 堆是JVM进程私有的，一个JVM进程只有一个堆内存，堆也是JVM内存管理的核心区域。 堆内存的大小是可以调节的。 JVM规范规定，堆可以处于物理上不连续的内存空间中，但在逻辑上他应该被视为连续的。","categories":[{"name":"java","slug":"java","permalink":"https://yury757.github.io/categories/java/"},{"name":"JVM","slug":"java/JVM","permalink":"https://yury757.github.io/categories/java/JVM/"}],"tags":[]},{"title":"postgresql","slug":"database/postgresql/postgresql","date":"2021-09-10T16:00:00.000Z","updated":"2021-10-31T18:45:23.561Z","comments":true,"path":"database/postgresql/postgresql/","link":"","permalink":"https://yury757.github.io/database/postgresql/postgresql/","excerpt":"","text":"一、postgresql安装部署postgresql版本：11.13 源码下载路径（最好下载源码，自己编译）：PostgreSQL: File Browser 安装过程： 1234567891011121314# 下载wget https://ftp.postgresql.org/pub/source/v11.13/postgresql-11.13.tar.gztar -xzf postgresql-11.13.tar.gzcd postgresql-11.13.tar.gz# 安装./configure# 上面一步可能会出现library not found问题，安装对应的lib即可，经常碰到需要安装以下两个lib：# sudo apt install libreadline-dev# sudo apt install zlib1g-devmake# 上面这一步如果成功后会显示：All of PostgreSQL successfully made. Ready to install.sudo make install# 上面这一步成功后会显示：Postgresql installation complete，默认安装在/usr/local/pgsql目录下 配置： 1234567891011cd /usr/local/pgsqlsudo mkdir datasudo chown yury /usr/local/pgsql/data # 修改data目录的所有者为yury，或者你自己新建的postgresql用户# 创建环境变量，将以下两行写入/etc/profile文件中export PGHOME=/usr/local/pgsqlexport PGDATA=/usr/local/pgsql/dataexport PATH=$&#123;PATH&#125;:$&#123;PG_HOME&#125;/bin# 重新加载环境变量source /etc/profile 初始化数据库： 1234567891011121314151617cd /usr/local/pgsqlbin/initdb# 出现以下输出时，说明初始化成功# Success. You can now start the database server using:# bin/pg_ctl -D /usr/local/pgsql/data -l logfile start# 启动数据库bin/pg_ctl start# 创建一个postgres用户bin/createuser postgres# 以postgres用户登录bin/psql postgres# 修改当前用户（postgres）的密码\\password 配置网络以便其他客户端可以连接： 1234567891011vi bin/pg_hba.conf# 修改下面这一行为第二行的值# host all 127.0.0.1/32 trust# host all 0.0.0.0/0 trustvi bin/postgresql.conf# 修改下面这一行为第二行的值，并且取消注释这个配置# listen_addresses = &#x27;127.0.0.1&#x27;# listen_addresses = &#x27;*&#x27;bin/pg_ctl restart # 重启服务 二、MySQL安装部署MySQL版本：8.0.25 使用apt安装 数据库目录：/var/lib/mysql/ 配置文件：/usr/share/mysql-8.0（命令及配置文件），/etc/mysql（如my.cnf） 相关命令：/usr/bin（mysqladmin、mysqldump等命令）和/usr/sbin 启动脚本：/etc/init.d/mysql（启动脚本文件mysql的目录）","categories":[{"name":"postgresql","slug":"postgresql","permalink":"https://yury757.github.io/categories/postgresql/"}],"tags":[]},{"title":"flink","slug":"bigdata/flink/flink-study","date":"2021-08-31T16:00:00.000Z","updated":"2022-10-07T12:39:56.630Z","comments":true,"path":"bigdata/flink/flink-study/","link":"","permalink":"https://yury757.github.io/bigdata/flink/flink-study/","excerpt":"","text":"java版本：1.8 Flink版本：1.13.6 一、Flink介绍1、Flink是什么Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. Apache Flink是一个框架和分布式处理引擎，用于对无界和有界数据流进行状态计算。 注意：flink应用的计算对象是数据流，即一个一个的源源不断的在连续时间内发过来的数据，并且在接收到数据时做实时计算。 2、哪些行业需要处理流数据 电商和市场营销：数据报表、广告投放、业务流程需要 物联网（IOT）：传感器实时数据采集和显示、实时警报、交通运输业 电信业：基站流量调配 银行和金融业：实时计算和通知推送，实时监测异常行为 3、Flink的特点 事件驱动。Flink的运行是由数据的到来而触发的，并且作出实时处理。 基于流的世界观。在Flink中，一切都是基于流组成的，离线数据可以视为有界的流；实时数据可以视为无界的流。 分层API。最顶层是SQL/table API（daynamic tabls），其次是DataStream API（streams），最底层是Process Function（events、state、time） 4、Flink对比Spark Streaming Spark Streaming的流处理其实是转换成微批，再做处理。而Flink是完全的流处理。因此Flink的延迟更低。 数据模型上，Spark采用的RDD模型，Spark Stream的DStream实际上也是一组组小批数据集RDD的集合。而Flink的基本数据模型是流数据，以及事件（Event）序列。 运行时架构，Spark是批计算，而Flink是标准的流计算，一个时间在一个节点处理完后可以直接发给下一个节点处理。 5、java版本依赖 1234567891011121314151617181920212223242526&lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;flink.version&gt;1.13.6&lt;/flink.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;!-- flink java依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- flink java的另外一个依赖，依赖版本号还是和上面那个一致，而2.12是指scale版本，因为底层有一部分是用scala写的 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.12&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 1.10以上版本的flink必须要有这个依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.12&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 二、简单使用1、文件数据批处理1234567891011121314151617181920212223242526272829303132333435363738package net.yury;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.util.Collector;/** * 批处理word count */public class Demo1WordCount &#123; public static void main(String[] args) throws Exception &#123; // 创建执行环境 ExecutionEnvironment executionEnvironment = ExecutionEnvironment.getExecutionEnvironment(); // 从文件中读取数据 String path = &quot;D:\\\\Adocument\\\\bigdata\\\\Flink\\\\flink-study\\\\src\\\\main\\\\resources\\\\demo1.txt&quot;; DataSet&lt;String&gt; stringDataSource = executionEnvironment.readTextFile(path); // 对数据集进行处理，按空格分词展开 DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; res = stringDataSource.flatMap(new MyFlatMapper()) .groupBy(0) // 按照第0个位置的值group by .sum(1); // group后分组统计第1个位置的值的sum res.print(); &#125; public static class MyFlatMapper implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;&#123; @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) throws Exception &#123; // 按空格分词 String[] words = value.split(&quot; &quot;); for (String word: words) &#123; collector.collect(new Tuple2&lt;&gt;(word, 1)); &#125; &#125; &#125;&#125; 2、文件数据流处理一个数据集，可以看作一个有界的数据流，因此也可以用数据流处理数据集。 1234567891011121314151617181920212223242526package net.yury;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;/** * 流处理word count * 同样，先创建执行环境，再读取数据，再基于数据流进行数据处理，最后打印输出 */public class Demo2WordCount &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(16); // 设置并行的操作的线程数 String path = &quot;D:\\\\Adocument\\\\bigdata\\\\Flink\\\\flink-study\\\\src\\\\main\\\\resources\\\\demo1.txt&quot;; DataStreamSource&lt;String&gt; stringDataSource = env.readTextFile(path); SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; res = stringDataSource.flatMap(new Demo1WordCount.MyFlatMapper()) .keyBy(0) .sum(1); res.print(); env.execute(); &#125;&#125; 3、数据流流处理生产环境中，大部分数据应该来源于网络和kafka推送，而这两种实际上是无界的数据流。 1234567891011121314151617181920212223242526package net.yury;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;/** * 从socket中读取流数据，并对单词进行计数 */public class Demo3SocketStream &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 设置并行的操作的线程数 // 在linux中使用nc -lk 7777命令，nc即netcat，是一个基于网络连接发送纯文本数据的功能。 DataStream&lt;String&gt; inputDataStream = env.socketTextStream(&quot;192.168.141.141&quot;, 7777); SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; res = inputDataStream.flatMap(new Demo1WordCount.MyFlatMapper()) .keyBy(0) .sum(1); res.print(); env.execute(); &#125;&#125; 值得注意的是，当我们的断开我们的nc进程后，flink程序也自动结束了，可能是因为socket连接断开了。 三、安装配置1、下载启动国内镜像下载地址如下：Index of /apache/flink (tsinghua.edu.cn) 1wget https://mirrors.tuna.tsinghua.edu.cn/apache/flink/flink-1.13.6/flink-1.13.6-bin-scala_2.12.tgz 启动standalone模式 1234cd flink-1.13.6/bin/start-cluster.sh # 启动bin/stop-cluster.sh # 停止bin/sql-client.sh embedded # 启动sql客户端 web管理界面：http://192.168.141.141:8081/ 2、配置（重要！） 配置 默认值 解释 jobmanager.execution.failover-strategy region jobmanager.memory.heap.size 1073741824b JVM堆内存 jobmanager.memory.jvm-metaspace.size 268435456b jvm-metaspace内存 jobmanager.memory.jvm-overhead.max 201326592b jvm-overhead最大内存 jobmanager.memory.jvm-overhead.min 201326592b jvm-overhead最小内存 jobmanager.memory.off-heap.size 134217728b JVM堆外内存 jobmanager.memory.process.size 1600m job管理器的总内存 jobmanager.rpc.address localhost rpc通信host jobmanager.rpc.port 6123 rpc通信port parallelism.default 8 每个task默认的并行线程数 taskmanager.memory.process.size 1728m task管理器的总内存 taskmanager.numberOfTaskSlots 8 每个taskmanager中的slot数量 web.tmpdir /tmp/flink-web-xxxx web界面临时数据存储目录 3、命令大全（todo 持续完善）12345678910# 提交一个job，-c参数为main方法，-p为提交jon时设置的parallelism，jar文件为job任务程序包，后面跟着的参数都是该job程序的参数，还是其他很多参数，可以具体使用bin/flink run --helo查看bin/flink run -c net.yury.Demo3SocketStream -p 3 /home/yury/flink-1.13.6/job/flink-study-1.0-SNAPSHOT.jar 192.168.141.141 7777bin/flink run -c net.yury.Demo1WordCount -p 3 /home/yury/flink-1.13.6/job/flink-study-1.0-SNAPSHOT.jar &quot;/disk4/flink-1.13.6/README.txt&quot;# 查看正在运行的jobbin/flink list# 停止一个正在运行的jobbin/flink cancel c433217551957f31979492ff1f8ed5f5 4、提交job（1）web界面提交在web界面中的submit new job界面可以提交命令。 （2）命令行提交具体见提交job的命令。 四、部署根据依赖的外部资源管理平台，可以有以下几种部署模式： standalone：即独立部署，不依赖外部资源管理平台。standalone模式支持session-cluster模式和Application模式部署。 yarn：依赖yarn作为资源管理平台的部署模式。yarn模式支持session-cluster模式、per-job-cluster模式和Application模式部署。 k8s：略 分布式部署，配置修改： 将conf/flink-conf.yaml中的jobmanager.rpc.address修改为jobmanager节点的host 将conf/workers中的host修改成taskmanager节点的host 以下介绍yarn模式下的几种部署模式 1、session-cluster模式先启动一个共享的yarn session集群，然后提交job，接着job会向yarn session申请资源。如果资源不足了，则要等待资源释放。所有资源共享Dispatcher和ResourceManager。即资源共享。适合小规模部署，任务运行时间通常较短。 部署： 12345# 启动hadoop集群，hadoop根目录下sbin/start-dfs.sh# 启动yarn集群，flink根目录下，其中的参数参见hadoop的yarn配置bin/yarn-session.sh -n 2 -s 2 -jm 1024 -tm 1024 -nm test -d 当flink部署了这种模式时，提交job的命令格式不变，但是会使用这种模式运行，否则默认使用standalone模式。 job管理还是在flink的web界面管理，而yarn-session的管理要在hadoop的yarn web界面上管理。 2、per-job-cluster模式每个job都对应一个集群，每提交一个job，都会根据自身情况向yarn申请资源，只要yarn容器有资源，就可以运行。独享Dispatcher和ResourceManager。适合大规模部署，长时间运行。 这种模式的部署只要启动hadoop集群即可，然后直接提交job。 但是提交job的命令要加上-m yarn-cluster参数。 1234bin/flink run -d -t yarn-per-job -c net.yury.Demo3SocketStream -p 3 /home/yury/flink-1.13.6/job/flink-study-1.0-SNAPSHOT.jar 192.168.141.141 7777# 或者bin/flink run -m yarn-cluster -c net.yury.Demo3SocketStream -p 3 /home/yury/flink-1.13.6/job/flink-study-1.0-SNAPSHOT.jar 192.168.141.141 7777 3、Application模式application模式就是一个应用程序对应一个集群。 五、Flink架构1、四大组件JobManager 控制应用程序的主进程，每提交一个job都会分配一个JobManager，这个JobManager只用来管理这个Job JobManager接收要执行的应用程序，包括作业图（JobGraph）、逻辑数据图（logical dataflow graph）和打包了所有类、库和其他资源的jar包 把JobGraph转换为一个物理层面的数据流图，即执行图（ExecutionGraph），即web界面展示的Task执行图 向ResourceManager申请资源（slot），把执行图分发给TaskManager 运行时，JobManager还会负责所有需要中央协调的操作，比如检查点（checkpoints）的协调等 TaskManager Flink工作进程中，通常有多个TaskManager管理Task，TaskManager包含了一定数量的slot，slot的数量限制了TaskManager能够执行的任务数 启动后，TaskManager会向ResourceManager注册他的slot，注册成功后，TaskManager会将一个或多个slot提供给JobManager调用，JobManager就可以向slot分配Task去执行 运行中，TaskManager之间可以交换数据 ResourceManager 主要负责管理TaskManager的slot，slot是Flink定义的处理资源单元 Flink为不同的环境提供了不同的ResourceManager，如yarn、k8s和standalone等 当JobManager申请资源时，ResourceManager会将有空闲slot的TaskManager分配给JobManager。如果ResourceManager没有足够的slot时，它还可以向资源平台发起会话，以提供启动TaskManager进程的容器。 Dispatcher 可以跨作业运行，它为应用提供了rest接口 提交job后，Dispatcher分发器就会启动并将job移交给一个JobManager Dispatcher也会启动一个web ui，用来实时展示和监控执行的信息 Dispatcher在应用中不是必须的，取决于job的提交运行方式 2、job提交流程standalone模式 per-jon-cluster模式 3、任务调度原理 4、parallelismparallelism是指并行执行的子任务个数，即一个任务会被拆分成几个子任务去并行执行。这很像在处理器处理多线程任务一样，当cpu只有一核时，多线程实际上时切换运行的，当cpu有多核时，多线程之间就可以实现并行运行。因此parallelism最大最好设置为服务器cpu的核心数。 一共有四处可以配置parallelism参数： 配置 优先级（从小到大，优先级递减） 代码中每一个task的parallelism值 1 代码中environment设置的全局parallelism值 2 提交任务时给定的parallelism值（web界面和命令行都可以设置该参数） 3 配置文件中的parallelism值 4 5、parallelism和slotFlink中每个TaskManager都是一个JVM进程，它可能会在独立的线程上执行一个或多个子任务。而TaskManager管理的slot的数量则是该进程的parallelism可以设置的最大数量，当parallelism大于slot时，就会有部分子任务没有slot来运行，就会一直等待。 和上面所说的，TaskManager对应进程，parallelism对应该进程启动的线程数，slot对应服务器的cpu核心数。在操作系统中，当线程数大于cpu核心数时，有部分线程会切换运行。而在Flink中，当parallelism大于slot数量时，Flink会一致等待。 由于parallelism的存在，一个TaskManager要分配并行子任务。那么在分配slot时，传统方式是把每一个子任务和一个slot绑定在一起，如下图所示。但是实际上TaskManager存在多个，因此需要的总slot数量为每个TaskManager下的parallelism加总，需要的slot量多。 而Flink允许不同类型的子任务共享slot，这样可以做到让其中某一个slot保存这个作业的整个pipeline，而且还可以减少slot的占用。如下图所示。 然而上上图也并不是不能实现的，可以在代码中设置不同任务的slot共享组，当设置的slot共享组名称不同时，即他们应该分配在不同的slot中运行。当一个任务没设置共享组名称时，它和前一个步骤的任务的slot共享组名称相同，第一个任务的默认的名称为default。如下这段代码需要的slot数量为5。 1234567DataStream&lt;String&gt; inputDataStream = env.socketTextStream(host, port);SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; res = inputDataStream.flatMap(new Demo1WordCount.MyFlatMapper()) // 默认slot共享组名称为default .keyBy(0) .sum(1).setParallelism(3).slotSharingGroup(&quot;red&quot;); // 设置slot共享组名称为redres.print().setParallelism(1).slotSharingGroup(&quot;green&quot;); // 设置slot共享组名称为greenenv.execute(); 一个job需要的slot数量为：先将slot按slot名称分组，取每个组里面的最大parallelism，最后对这些parallelism加总，即是这个job需要的slot的数量。 6、程序与数据流在程序运行时，Flink上运行的程序会被映射成”逻辑数据流“（dataflow），它包含了三部分： 每个dataflow以一个或多个sources开始 中间的转换运算（transformation）和dataflow中的operator是一一对应的关系 最后以一个或多个sink输出结束 因此，整个数据流就是一个有向无环的数据流图。 而这个数据流图转换为具体的执行流图，由以下四步组成： StreamGraph：根据用户写的stream api生成的最初的图 JobGraph：StreamGraph经slot组合等等优化的来的，提交给JobManager ExecutionGraph：这个是JobGraph的并行版本，调度最核心的数据结构 物理执行图：部署在TaskManager上的具体的执行”图“，并不是一个具体的数据结构 7、数据传输形式one-to-onestream维护着分区以及元素的顺序。一般简单的操作可以做到，如map、filter、flatmap等 Reditributingstream的的分区会发生改变，如keyBy操作会根据hashcode进行重分区，而broadcast和rebalance会随机重新分区，这些操作都将打乱stream的顺序。 8、任务链Flink采用了一种称为任务链的优化技术，当两个算子之间满足以下三个条件时，这两个算子会组合成一个task，这样可以在特定条件下减少本地通信的开销。为了满足任务链的要求，必须满足以下三个条件： 都是one-to-one模式 parallelism相同 处于同一个slot组下 六、StreamAPI七、WindowAPIStreamAPI是来一个处理一个，而window是将到来的数据划分到某个window中，然后对window中的这一批数据进行处理。 窗口类型： 时间窗口（TimeWindow）：时间间隔左闭右开 滚动时间窗口（TumblingTimeWindow），参数：窗口大小（window size） 滑动时间窗口（SlidingTimeWindow），参数：窗口大小（window size）、滑动距离（slide size） 会话窗口（SessionWindow），相邻数据之间到来的时间差小于设定的会话间隙参数，则认为这两个数据同属于一个窗口。超过这个绘画间隙参数的时间没有收到新数据时，下一个新数据就会归属于下一个新的窗口。参数：会话间隙（session gap） 计数窗口（CountWindow） 滚动计数窗口（TumblingCountWindow） 滑动计数窗口（SlidingCountWindow） 1、window aggsiner（1）window()window()方法，窗口分配器方法，是将一个stream流转换为一个window流。window()方法必须在keyby()方法后面使用。因为只有该方法是KeyedStream对象才能调用的方法。 参数：WindowAssigner（窗口分配器），即数据流要按照什么样的分配规则来分配窗口。 常用的如下： 12345678910111213141516 // 滚动时间窗口 TumblingProcessingTimeWindows tumblingTimeWindows = TumblingProcessingTimeWindows.of(Time.seconds(5)); // 滑动时间窗口 SlidingProcessingTimeWindows slidingTimeWindows = SlidingProcessingTimeWindows.of(Time.seconds(5), Time.seconds(1)); // session窗口 EventTimeSessionWindows sessionWindows = EventTimeSessionWindows.withGap(Time.seconds(5)); inputDataStream .keyBy(i -&gt; i) .window(tumblingTimeWindows) // 滚动时间窗口// .window(slidingTimeWindows) // 滑动时间窗口// .countWindow(10) // 滚动计数窗口// .countWindow(10, 2) // 滚动滑动窗口// .window(sessionWindows) // session窗口// .timeWindow(Time.seconds(5)) // v1.13.6 时间滚动窗口，该方法已经被废弃 ; （2）windowAll()对一个未经过keyby的数据流进行开窗，将整个数据流视为一个window数据流，是一个非并行操作。 其他操作和window()方法一致。 2、window function窗口函数，即要对窗口里面的数据做何种操作的函数。类型： 增量聚合函数（Incremental Aggregation function）：每条数据进入这个窗口，就做一次计算，得到一个当前状态。如ReduceFunction、AggregateFunction。只能适合简单的计算操作，即历史计算得到的状态对于新数据是有效的。 全窗口函数（Full Window Function）：等该窗口的数据到齐之后再进行处理。如：ProcessWindowFunction、WindowFunction。历史计算状态对于新数据的计算是没有意义的，则应该使用全窗口函数，如计算1/4分位数等。 （1）AggregateFunction12345678910111213141516171819202122232425262728293031AggregateFunction&lt;Tuple2&lt;String, Integer&gt;, Integer, Integer&gt; aggregateFunction = new AggregateFunction&lt;Tuple2&lt;String, Integer&gt;, Integer, Integer&gt;() &#123; // 创建初始状态 @Override public Integer createAccumulator() &#123; return 0; &#125; // 新数据处理逻辑 @Override public Integer add(Tuple2&lt;String, Integer&gt; value, Integer accumulator) &#123; return accumulator + value.f1; &#125; // 获取结果 @Override public Integer getResult(Integer accumulator) &#123; return accumulator; &#125; // 对两个历史状态进行合并的逻辑 @Override public Integer merge(Integer a, Integer b) &#123; return a + b; &#125;&#125;;SingleOutputStreamOperator&lt;Integer&gt; aggregate = inputDataStream .flatMap(new Demo1WordCount.MyFlatMapper()) .keyBy(tuple -&gt; tuple.f0) .window(tumblingTimeWindows) // 滚动时间窗口 .aggregate(aggregateFunction); （2）WindowFunctionProcessWindowFunction是WindowFunction的增强版。 123456789101112131415161718SingleOutputStreamOperator&lt;Tuple3&lt;String, Long, Integer&gt;&gt; apply = inputDataStream .flatMap(new Demo1WordCount.MyFlatMapper()) .keyBy(tuple -&gt; tuple.f0) .window(tumblingTimeWindows) // 滚动时间窗口 .apply(new WindowFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Long, Integer&gt;, String, TimeWindow&gt;() &#123; @Override public void apply(String s, TimeWindow window, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; input, Collector&lt;Tuple3&lt;String, Long, Integer&gt;&gt; out) throws Exception &#123; int count = 0; for (Tuple2&lt;String, Integer&gt; tuple : input) &#123; count++; &#125; Tuple3&lt;String, Long, Integer&gt; res = new Tuple3&lt;&gt;(); res.f0 = s; res.f1 = window.getEnd(); res.f2 = count; out.collect(res); &#125; &#125;); 3、其他API trigger()：可以定义window什么时候计算结果，什么时候关系 evictor()：定义将某些数据从window中移除的逻辑 allowedLateness()：允许迟到的数据 sideOutputLateDate()：将迟到的数据放入侧输出流 getSideOutput()：获取侧输出流 基于这些底层API，可以按照自己的需求实现一个自定义的窗口处理逻辑。","categories":[{"name":"bigdata","slug":"bigdata","permalink":"https://yury757.github.io/categories/bigdata/"}],"tags":[]},{"title":"zookeeper-study","slug":"bigdata/zookeeper/zookeeper-study","date":"2021-08-27T16:00:00.000Z","updated":"2022-10-07T12:41:10.692Z","comments":true,"path":"bigdata/zookeeper/zookeeper-study/","link":"","permalink":"https://yury757.github.io/bigdata/zookeeper/zookeeper-study/","excerpt":"","text":"一、Zookeeper介绍1、简介zookeeper是一个分布式数据一致性解决方案，致力于为分布式应用提供一个高性能、高可能，且具有严格顺序访问控制能力的分布式协调存储服务。提供的功能包括：配置维护、域名服务、分布式同步、组服务等。 zookeeper是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。 二、下载安装地址：Index of /apache/zookeeper (tsinghua.edu.cn) 123456789101112cd /home/yurywget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.6.3/apache-zookeeper-3.6.3-bin.tar.gz# 解压tar xzvf apache-zookeeper-3.6.3-bin.tar.gz# 将文件夹重命名为zookeeper-3.6.3-bin方便使用mkdir zookeeper-3.6.3-bin/mv -f apache-zookeeper-3.6.3-bin/* zookeeper-3.6.3-bin/# 使用这个目录作为根目录cd zookeeper-3.6.3-bin 将以下配置写入一个新建的文件：vi /conf/zoo.cfg 123456# ZooKeeper使用的基本时间单位（毫秒）。 它用于做心跳，并且最小会话超时将是tickTime的两倍。tickTime=2000# 除非另有说明，否则存储内存中数据库快照的位置以及数据库更新的事务日志dataDir=/home/yury/zookeeper-3.6.3-bin/zookeeper-data# 客户端访问的端口clientPort=2181 三、单机模式12345678910111213141516# 启动bin/zkServer.sh start# 查看运行状态bin/zkServer.sh status# ZooKeeper JMX enabled by default# Using config: /home/yury/zookeeper-3.6.3-bin/bin/../conf/zoo.cfg# Client port found: 2181. Client address: localhost. Client SSL: false.# Mode: standalone# 使用命令行连接到服务器bin/zkCli.sh -server localhost:2181# jps命令，需要手动安装jps16722 Jps16365 QuorumPeerMain 四、分布式模式官方文档建议使用奇数个服务器。 如果只有两台服务器，那么您将处于一种情况，如果其中一台服务器发生故障，则没有足够的计算机构成多数仲裁。由于存在两个单点故障，因此两个服务器本来就不如单个服务器稳定。因此我们创建3台服务器。 修改之前创建的那个配置文件，新增以下内容： 12345678910111213# initLimit is timeouts ZooKeeper uses to limit the length of time the ZooKeeper servers in quorum have to connect to a leader.initLimit=5# syncLimit limits how far out of date a server can be from a leader# 这几个时间都是以上面那个tickTime为单位时间syncLimit=2# server.N是指第N台服务器# A=B:C:D，其中B位置是ip，或者ip的别名，在hosts中可以为ip设置别名解析# 服务器使用前一个端口连接到其他服务器。ZooKeeper服务器使用此端口将follower连接到leader。当出现新的leader时，follower使用此端口打开与leader的TCP连接。由于leader选举时默认使用TCP，因此我们当前需要另一个端口来进行leader选举。这是配置中的第二个端口。server.1=192.168.0.201:2888:3888server.2=192.168.0.202:2888:3888server.3=192.168.0.203:2888:3888 123# 将配置文件拷贝到其他服务器scp ./conf/* yury@192.168.141.142:/home/yury/zookeeper-3.6.3-bin/conf/scp ./conf/* yury@192.168.141.143:/home/yury/zookeeper-3.6.3-bin/conf/ 注意：最后还要在上面的dataDir目录下新建一个myid的文件，写入本台服务器的数字id，如第2台服务器，只需要放一个数字2进去即可。 启动之后查看服务器状态，结果如下，其中141和142服务器的mode为follower，而143服务器的mode为leader，这是由分布式系统投票决定的，不是我们定义的。 1234ZooKeeper JMX enabled by defaultUsing config: /home/yury/zookeeper-3.6.3-bin/bin/../conf/zoo.cfgClient port found: 2181. Client address: localhost. Client SSL: false.Mode: follower 搭好了之后，像上面一样用zkCli.sh命令连接随意连接一台服务器，创建节点，修改节点，会发现，三台服务器均会做相应修改。 五、使用zkServer.sh脚本的功能如下 Usage: bin/zkServer.sh [–config &lt;conf-dir&gt;] {start|start-foreground|stop|version|restart|status|print-cmd} 1、zkCli.sh命令 命令 描述 help 帮助 ls /path /path为节点路径，如：ls /zookeeper-w 注册监听子节点路径变化，如果其他客户端修改该节点的值，或删除该节点，则可以收到通知。但是有一个缺陷，这个监听只能监控到一次变化， 如果还需要继续监控，需要继续注册。 create /path [value] 创建节点，如：create /zktest mydata get /path 获取节点的值，如：get /zktest-w 注册监听节点的变化，监听效果同上-s 获取节点信息 set /path value 设置节点的值，如：set /zktest junk delete /path 删除节点（如果该节点下面非空，即有子节点，则无法删除），如：delete /zktest deleteall /path 递归删除节点（会删除所有子节点） quit 退出 2、节点类型持久、短暂、有序号、无序号 待完善。 3、API使用六、分布式理论1、CAP理论CAP 理论指出对于一个分布式计算系统来说，不可能同时满足以下三点： 一致性（Consistency）：在分布式环境中，一致性是指数据在多个副本之间是否能够保持一致的特性，等同于所有节点访问同一份最新的数据副本。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一致的状态。 可用性（Availability）：每次请求都能获取到正确的响应，但是不保证获取的数据为最新数据。 分区容错性（Partition Tolerance）：分布式系统在遇到任何网络分区故障的时候，仍然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障。 一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。 在这三个基本需求中，最多只能同时满足其中的两项，P 是必须的，因此只能在 CP 和 AP 中选择，zookeeper 保证的是 CP，对比 spring cloud 系统中的注册中心 eruka 实现的是 AP。 2、BASE理论BASE是Basically Available（基本可用）、Soft-state（软状态）和Eventually Consistent（最终一致性）三个短语的缩写。 基本可用：在分布式系统出现故障，允许损失部分可用性（服务降级、页面降级）。 软状态：允许分布式系统出现中间状态。而且中间状态不影响系统的可用性。这里的中间状态是指不同的 data replication（数据备份节点）之间的数据更新可以出现延时的最终一致性。 最终一致性：data replications 经过一段时间达到一致性。 BASE 理论是对 CAP 中的一致性和可用性进行一个权衡的结果，理论的核心思想就是：我们无法做到强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性。 五、分布式一致性 1、强一致性：用户更新了value后，服务器先把数据同步到其他服务器，再将“更新成功”的消息返回给用户，即这两个操作是同步的，也可以说服务器的主从复制是同步的，则用户不管读那台服务器都是更新后的值。 2、弱一致性：用户更新了value后，服务器把数据同步到其他服务器和将“更新成功”的消息返回给用户这两个操作是异步的，也可以说服务器的主从复制是异步的，因此用户读取时可能会读到旧的数据。 3、半同步：保证一台从服务器是同步的，其他从服务器则是异步的，如果同步的从服务器出现问题，则让另外一台异步服务器来做同步。即始终保证有两个节点拥有完整数据。 3、最终一致性，最终一致性其实还是弱一致性，只不过用户看到的旧数据只是一个暂时的状态，如果等待一段时间，从服务器最终会和主服务器数据一致，这就是最终一致性。数据同步的速度受很多因素影响，一般都比较快，即这个等待延迟一般很短。 七、选举机制1、半数机制。集群中半数以上机器存活，集群可用。所以zookeeper适合安装奇数台服务器。 2、zookeeper虽然在配置文件中没有指定leader和follower，但是在集群启动后，zookeeper会通过内部选举产生临时的leader和follower。 八、监听器","categories":[{"name":"bigdata","slug":"bigdata","permalink":"https://yury757.github.io/categories/bigdata/"}],"tags":[]},{"title":"SSM-Build","slug":"java/SSM-Build/SSM-Build","date":"2021-08-23T16:00:00.000Z","updated":"2022-10-07T12:57:23.499Z","comments":true,"path":"java/SSM-Build/SSM-Build/","link":"","permalink":"https://yury757.github.io/java/SSM-Build/SSM-Build/","excerpt":"","text":"SSM框架整合1、web.xml中配置DispatcherServlet时的初始化参数要连接所有的spring配置文件。先当与配置文件从web.xml是一个顶点，然后依次往下细分。 2、排错方法： 使用IDEA查看bean和MVC的细节 Junit单元测试 3、即使类上面加了@RestController，springmvc不会自动帮你把对象转字符串再返回给前端，这时若方法返回的是对象或List接口或其他则会报错No converter found for return value of type: class java.util.ArrayList。若导入了jackson-databind包，则会自动帮你把对象或列表转字符串，就可以直接返回一个对象了。 123456789@RequestMapping(&quot;/a2&quot;)public List&lt;User&gt; a2()&#123; List&lt;User&gt; userList = new ArrayList&lt;&gt;(); userList.add(new User(&quot;yury757&quot;, 18, &quot;男&quot;)); userList.add(new User(&quot;name1&quot;, 28, &quot;男&quot;)); userList.add(new User(&quot;name2&quot;, 38, &quot;女&quot;)); userList.add(new User(&quot;name3&quot;, 48, &quot;男&quot;)); return userList;&#125;","categories":[{"name":"java","slug":"java","permalink":"https://yury757.github.io/categories/java/"}],"tags":[]},{"title":"Java_NIO-study","slug":"java/java_NIO/Java_NIO-study","date":"2021-08-23T16:00:00.000Z","updated":"2022-10-07T12:55:22.778Z","comments":true,"path":"java/java_NIO/Java_NIO-study/","link":"","permalink":"https://yury757.github.io/java/java_NIO/Java_NIO-study/","excerpt":"","text":"一、NIO和IO的区别 传统IO面向流，而NIO面向管道（channel）和缓冲区（buffer）。传统IO只能单向传输数据，而NIO可以双向传输数据。传统IO就像单向水管，而NIO中channel像列车轨道，buffer就像火车，可以双向传输数据。 传统IO是阻塞（blocking）的，而NIO就是非阻塞（Non blocking）的。 NIO多了一个选择器（Selector），是针对网络编程用的。 二、缓冲区（Buffer）缓冲区（Buffer）是基于数组来做管理的，负责存取数据。java NIO有七种xxxBuffer类，都继承了Buffer类，即八种基本数据类型中，除了boolean以外都提供了对应的xxxBuffer。 1234567ByteBufferCharBufferShortBufferIntBufferLongBufferFloatBufferDoubleBuffer 每种xxxBuffer类还不是最终实现类，最终的实现类有两种，以下以ByteBuffer为例： 12345// 属于下面说的直接缓冲区，不懂操作系统的人最好不要使用，有风险class DirectByteBufferR extends DirectByteBuffer implements DirectBuffer&#123;&#125;// 常用这个实现类class HeapByteBuffer extends ByteBuffer&#123;&#125; 1、四个核心属性12345678910111213// 容量，缓冲区总的最大容量int capacity;// 最大可读写的容量int limit;// 位置，表示缓冲区中正在操作的数据的下标。当position&lt;limit，可以做写入操作，当position=limit时，写入操作会报错。int position;// 标记位置，默认为-1int mark = -1;// mark &lt; position &lt; limit &lt; capacity 2、主要方法下面以ByteBuffer为例： 123456789101112131415161718192021222324252627282930313233343536// 分配缓冲区public static ByteBuffer allocate(int capacity);// 存入数据到缓冲区中public abstract ByteBuffer put(byte b);// 读取数据public abstract byte get(int index);// 将缓冲区的数据读取到另外一个数组中public ByteBuffer get(byte[] dst, int offset, int length);// 切换到读取数据的模式。将limit的值置为当前position的值，再把position归0public final Buffer flip() &#123; limit = position; position = 0; mark = -1; return this;&#125;// 恢复到初始状态，注意数据还在，只是被遗忘了public final Buffer clear() &#123; position = 0; limit = capacity; mark = -1; return this;&#125;// 将position恢复到上一次mark标记的位置public final Buffer reset() &#123; int m = mark; if (m &lt; 0) throw new InvalidMarkException(); position = m; return this;&#125; 注意：public ByteBufferget(byte[] dst, int offset, int length)方法参数中的offset和length不是针对源buffer，而是针对新数组dst的！！ 3、非直接缓冲区 4、直接缓冲区 优点：效率更高！ 缺点：消耗资源大，数据写入到物理内存中后不受java控制，垃圾回收也有一定的问题。 三、通道（Channel）之前进行IO操作，是通过CPU授权给DMA（Direct Memory Access）总线，然后在DMA总线的管理下进行IO操作。而Channel则是一种独立的专门处理IO操作的特殊的（协）处理器，具有自己的IO指令，进行IO操作时不需要CPU授权。 1、接口和实现类接口： java.nio.Channels.Channel 实现类： FileChannel：本地数据IO SocketChannel：TCP连接用 ServerSocketChannel：TCP连接用 DatagramChannel：UDP连接用 2、获取通道的三种方式（1）各个支持通道的IO类提供了相应的getChannel方法1234567891011121314151617181920212223242526272829303132333435@Testpublic void test02() &#123; try(// 获取流 FileInputStream fis = new FileInputStream(filename1); FileOutputStream fos = new FileOutputStream(filename2); // 获取对应的通道 FileChannel fisChannel = fis.getChannel(); FileChannel fosChannel = fos.getChannel();) &#123; // 通过非直接缓冲区方式 ByteBuffer buffer = ByteBuffer.allocate(1024); // 将源数据通道的数据写入缓冲区 while (fisChannel.read(buffer) != -1)&#123; // 切换成读取模式 buffer.flip(); // 读取缓冲区中的数据，写入目标数据通道 fosChannel.write(buffer); // 清空缓冲区 buffer.clear(); &#125; &#125;catch (Exception ex)&#123; ex.printStackTrace(); &#125; // 使用try()&#123;&#125;的方式就可以不用关闭，否则就要关闭所有的流和通道 // fosChannel.close(); // fisChannel.close(); // fis.close(); // fos.close();&#125; （2）使用open函数123456789101112131415161718192021222324252627282930313233343536373839404142@Testpublic void test03() &#123; Date startTime = new Date(); // 通过open的方式获取通道 // CREATE_NEW：当文件存在时会报错 // CREATE：当文件存在时，会在原文件上从头开始覆盖写入。 // 但是很奇怪，当使用CREATE和直接缓冲区结合的方式时，当source文件的字节数小于destination文件的字节数时，并不会发生写入操作。 // 即一般使用CREATE_NEW就行 try(FileChannel fisChannel = FileChannel.open(Paths.get(filename1), StandardOpenOption.READ); FileChannel fosChannel = FileChannel.open(Paths.get(filename2), StandardOpenOption.WRITE, StandardOpenOption.READ, StandardOpenOption.CREATE_NEW)) &#123; // 使用非直接缓冲区 ByteBuffer buffer = ByteBuffer.allocate(8 * 1024 * 1024); while (fisChannel.read(buffer) != -1)&#123; buffer.flip(); fosChannel.write(buffer); buffer.clear(); &#125; /* // 通过直接缓冲区方式 MappedByteBuffer inMappedBuffer = fisChannel.map(FileChannel.MapMode.READ_ONLY, 0, fisChannel.size()); MappedByteBuffer outMappedBuffer = fosChannel.map(FileChannel.MapMode.READ_WRITE, 0, fisChannel.size()); // 对直接缓冲区中的数据进行读写，因此省略了从OS地址空间到JVM地址空间的copy操作 // 使用内存映射文件时，就是将一个硬盘上的文件通过通道映射到物理内存的缓冲区中，当缓冲区有put操作，则会直接将对应的数据写入硬盘 byte[] dst = new byte[inMappedBuffer.limit()]; inMappedBuffer.get(dst); outMappedBuffer.put(dst); */ // 关闭通道，同样如果使用了try()&#123;&#125;则不用关闭 // fisChannel.close(); // fosChannel.close(); &#125;catch (Exception ex)&#123; ex.printStackTrace(); &#125; Date endTime = new Date(); System.out.println(&quot;time: &quot; + (endTime.getTime() - startTime.getTime()) / 1000 + &quot;秒&quot;);&#125; 注意！！使用Channel.transferTo方法时，一次传输最大传输支持2G，如果文件超过2G，则要断点传输！如下： 1234567891011121314151617181920212223@Testpublic void test04()&#123; Date startTime = new Date(); try(FileChannel fisChannel = FileChannel.open(Paths.get(filename1), StandardOpenOption.READ); FileChannel fosChannel = FileChannel.open(Paths.get(filename2), StandardOpenOption.WRITE, StandardOpenOption.READ, StandardOpenOption.CREATE_NEW)) &#123; // 通过transferTo，也是通过直接缓冲区的方式 // transferTo一次传输最大2G，因此无论文件大小的化，最好使用以下方式 long position = 0; long len = fisChannel.size(); while (0 &lt; len)&#123; long l = fisChannel.transferTo(position, len, fosChannel); if (l &gt; 0)&#123; position = l; len -= l; &#125; &#125; &#125;catch (Exception ex)&#123; ex.printStackTrace(); &#125; Date endTime = new Date(); System.out.println(&quot;time: &quot; + (endTime.getTime() - startTime.getTime()) / 1000 + &quot;秒&quot;);&#125; （3）通过Files创建1Files.newByteChannel(); 3、聚集和分散分散读取（Scattering Reads）：读取文件时按顺序填入多个缓冲区中，前面的缓冲区填满了，再填后面的缓冲区 聚集写入（Gathering Writes）：将多个缓冲区中的数据按顺序写入到通道中 即就是将channel写入一个bytebuffer数组中，或者从一个bytebuffer数组中读取数据到channel。 1234567891011121314151617181920212223242526272829@Testpublic void test05()&#123; try(RandomAccessFile raf = new RandomAccessFile(filename1, &quot;r&quot;); FileChannel channel = raf.getChannel(); RandomAccessFile raf2 = new RandomAccessFile(filename2, &quot;rw&quot;); FileChannel channel2 = raf2.getChannel(); ) &#123; System.out.println(&quot;=======分散读取======&quot;); ByteBuffer buffer1 = ByteBuffer.allocate(10); ByteBuffer buffer2 = ByteBuffer.allocate(1024); // 写入缓冲区 ByteBuffer[] buffers = &#123;buffer1, buffer2&#125;; channel.read(buffers); // 将缓冲区切换成读取模式 for (ByteBuffer buffer : buffers) &#123; buffer.flip(); &#125; // 将缓冲区中的内容打印出来 System.out.println(new String(buffers[0].array(), 0, buffers[0].limit())); System.out.println(&quot;=============&quot;); System.out.println(new String(buffers[1].array(), 0, buffers[1].limit())); System.out.println(&quot;=======聚集写入======&quot;); channel2.write(buffers); &#125;catch (Exception ex)&#123; ex.printStackTrace(); &#125;&#125; 四、字符集（Charset）编码：字符串 =&gt; 字节数组 解码：字节数组 =&gt; 字符串 123456789@Testpublic void test01()&#123; final SortedMap&lt;String, Charset&gt; stringCharsetSortedMap = Charset.availableCharsets(); stringCharsetSortedMap.forEach((key, value) -&gt; &#123; System.out.println(key.getClass().getName()); System.out.println(value.getClass().getName()); System.out.println(key + &quot;: &quot; + value); &#125;);&#125; 获取一个字符集的类，并得到他们的编码器和解码器： 123Charset gbk = Charset.forName(&quot;GBK&quot;);CharsetEncoder charsetEncoder = gbk.newEncoder();CharsetDecoder charsetDecoder = gbk.newDecoder(); encode方法得到一个ByteBuffer，而decode方法得到一个CharBuffer： 123456789101112131415161718192021222324252627@Testpublic void test02() throws Exception&#123; final Charset gbk = Charset.forName(&quot;GBK&quot;); final CharsetEncoder charsetEncoder = gbk.newEncoder(); final CharsetDecoder charsetDecoder = gbk.newDecoder(); CharBuffer charBuffer = CharBuffer.allocate(1024); charBuffer.put(&quot;你好，世界！hello, world!!&quot;); charBuffer.flip(); ByteBuffer byteBuffer = charsetEncoder.encode(charBuffer); for (int i = 0; i &lt; byteBuffer.limit(); i++) &#123; System.out.print(byteBuffer.get() + &quot; &quot;); &#125; System.out.println(); byteBuffer.flip(); CharBuffer charBuffer2 = charsetDecoder.decode(byteBuffer); for (int i = 0; i &lt; charBuffer2.limit(); i++) &#123; System.out.print(charBuffer2.get() + &quot; &quot;); &#125; System.out.println(); charBuffer2.flip(); System.out.println(charBuffer2.toString());&#125; 五、阻塞和非阻塞（重点！！）这里所说的阻塞和非阻塞主要针对网络编程。在客户端连接服务器时，客户端要向服务器发送数据包请求，双方都会打开一个通道，但是当服务器读取一段数据后，不清楚客户端通道中的数据是否读取完，则这个线程会被阻塞。 而非阻塞网络IO就是在客户端和服务器之间加一个选择器（Selector），所有客户端发送过来的数据包都先通过选择器，由选择器来判断该数据包是否准备完毕，将准备完毕的数据包发送给服务器去处理，而没准备完毕的数据包留在这里。 因此，使用NIO完成网络通信的三个核心： Channel，负责连接 Buffer，负责传输数据 Selector，是SelectableChannel的多路复用器，用于监控SelectableChannel的IO状况。 这部分笔者还不是很清楚。 六、NIO实战项目用NIO写一个非阻塞式http服务器：yury757/httpserver (github.com)","categories":[{"name":"java","slug":"java","permalink":"https://yury757.github.io/categories/java/"}],"tags":[]},{"title":"hbase-study","slug":"bigdata/hbase/HBase-study","date":"2021-08-23T16:00:00.000Z","updated":"2022-10-07T12:41:24.021Z","comments":true,"path":"bigdata/hbase/HBase-study/","link":"","permalink":"https://yury757.github.io/bigdata/hbase/HBase-study/","excerpt":"","text":"Linux：ubuntu18.04.5 hbase：2.3.5 官方文档：Apache HBase ™ Reference Guide 一、HBase介绍1、Hadoop的局限性 hadoop主要是实现批量数据的处理，并通过顺序方式访问数据。比如批量处理一天的数据。 要查找数据必须搜索整个数据集，即不具备随即读取数据的能力。 2、HBase简介 HBase是一个分布式的、面向列的开源数据库，该技术来源于Fay Chang所撰写的Google论文《Bigtable：一个结构化数据的分布式存储系统》。 HBase一开始是Hadoop下的一个子项目，因为也是基于HDFS文件系统的，后成为Apache的顶级项目。 HBase是Google Bigtable的开源实现，类似Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google运行MapReduce来处理Bigtable中的海量数据，HBase同样利用Hadoop MapReduce来处理HBase中的海量数据；Google Bigtable利用 Chubby作为协同服务，HBase利用Zookeeper作为对应。 HBase是一种NoSQL数据库，仅能通过主键（row key）和主键的range来检索数据，对事务的支持较弱。 HBase只支持一种数据类型：byte[] HBase是稀疏存储的，即为空的字段不占用空间，而比如MySQL的null实际上也会占空间的。 应用场景：需要存储海量数据，又要快速的写入和查询数据的场景。 3、关系型数据库（RDBMS）和HBase的比较 关系型数据库（以MySQL为例） HBase 是否以表的形式存在 是 是 支持的文件系统 FAT（windows旧）、NTFS（windows新）、EXT（Linux） HDFS文件系统 物理上的存储方式 以行的形式存储，每个字段之间用分隔符隔开 以每个单元格为一行的形式存储，即每一个单元格数据都会存储其row key、列簇名、列名和时间戳等。HBase会对行进行分割，一片行和一个列簇即形成一个region。具体见后面详解。 索引 支持主键（primary key）和二级索引 仅支持主键（row key） 事务 最常用的InnoDB引擎的事务处理满足ACID原则 对事务的支持较弱，不支持ACID 是否能使用sql查询 能 不能，NoSQL数据库 是否支持join 支持 不支持 适合存储的数据 适合存储少量的、结构化的数据 适合存储大量数据，结构化和非结构话都适合，但是如果使用HBase来存储少量数据，效率和内存消耗上都不如关系型数据库 4、HBase的逻辑结构 5、HBase的物理结构HBase的物理结构，实际上是每一个单元都对应了一行或多行数据，每行存储了其元数据信息和值。 而实际上删除操作的第一时间并不会真正删除数据，而是插入了一条type为delete的数据，timestamp则是版本控制（这就是为什么需要校正服务器时间），查数据get命令其实就是获取timestamp最大的那条数据，如果这条数据的type是delete，则不反悔数据，否则返回那条数据。而scan命令可以通过设置VERSION参数来查看之前版本的数据。 6、Hive和HBase的区别 Hive HBase 定位 Hive是一个数据仓库工具，本质相当于把HDFS中已存在的数据文件在MySQL中做一个映射关系，以方便用HQL去管理查询。 定位是一个NoSQL数据库 功能 用于数据分析和清洗 高效地存储和查询数据 使用场景 离线数据分析和清晰，因为需要时间较长，延迟较高 实时查询和存储海量数据 底层 基于HDFS，编写的HQL最终是转换为MapReduce代码执行 基于HDFS，但是在HDFS上做了进一步的处理和优化 如下面是一种数据仓库架构。 二、下载、安装和配置下载链接：HBase的清华大学镜像包 选择一个稳定版本，点进去后下载其中的二进制的压缩包，不用下载源码的压缩包，源码的压缩包可以用来看源码。 12345678# 下载wget https://mirrors.tuna.tsinghua.edu.cn/apache/hbase/stable/hbase-2.3.5-bin.tar.gz# 解压tar xzvf hbase-2.3.5-bin.tar.gz# 以这个目录为工作目录cd hbase-2.3.5 配置环境变量 1234567vi /etc/profile# 在最下面添加export HBASE_HOME=/home/yury/hbase-2.3.5export PATH=$&#123;PATH&#125;:$&#123;HBASE_HOME&#125;/bin:$&#123;HBASE_HOME&#125;/sbin# 加载环境变量source /etc/profile 修改配置文件./conf/hbase-env.sh 1234567# 修改压缩包根目录下的这个文件vi ./conf/hbase-env.sh# 添加JAVA_HOME配置export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/# 为false时使用自己的ZOOKEEPER，即需要自己启动zookeeper服务。为true时使用hbase内置的zookeeper，如果是单机版建议为true，或者不配置export HBASE_MANAGES_ZK=false 将一个包复制到lib目录下 1cp lib/client-facing-thirdparty/htrace-core4-4.2.0-incubating.jar ./lib/ 移除一个日志jar包，这个版本和hadoop的3.3.0版本的日志包会有冲突 1rm lib/client-facing-thirdparty/slf4j-log4j12-1.7.30.jar 三、单机版1234567891011# 配置完了以上操作后，直接启动即可./bin/start-hbase.sh# 返回结果如下# running master, logging to /home/yury/hbase-2.3.5/bin/../logs/hbase-yury-master-myubuntu1.out# 可以在http://192.168.141.141:16010这个页面中HBase的web管理页面# jps命令可以看到有一个HMaster进程jps# 2032 Jps# 1539 HMaster 四、初步使用HBase的shellhbase的数据库结构主要以下层次： namespace（命名空间，相当于MySQL的schema） table column family（列簇） column qualifier（列名） hbase的namespace默认有两个：default和namespace，默认使用default，即在创建表时如果没有加命名空间前缀，则默认往default命名空间里面建表。 而namespace这个命名空间存储着数据库的元数据信息，这个命名空间向相当于MySQL的information这个schema。 hbase shell里面没有分号，如果敲了分号并回车了，可以通过敲一个单引号，再回车，再敲一个单引号来取消之前的命令。 对于哪个命令不熟悉，可以help &#39;create&#39;，就有这个命令的使用方法，下面只是简单介绍。 12# 启动hbase的shell命令，需要配置hbase环境变量，以下都是hbase的shell命令hbase shell 1、命名空间操作12345list_namespace # 展示所有命名空间create_namespace &#x27;myns&#x27; # 创建命名空间，相当于MySQL的创建一个schemadrop_namespace &#x27;myns&#x27; # 删除命名空间describe_namespace &#x27;myns&#x27; # 查看指定命名空间的详细信息list_namespace_tables &#x27;myns&#x27; # 查看指定命名空间下的所有表 2、表操作1234567891011121314# 对于表的操作默认是指default命名空间，要想对其他命名空间操作表，需加命名空间的前缀，如下# 在myns命名空间下创建一个表create &#x27;myns:myns_test&#x27;, &#x27;myns_cf&#x27;# 以下不加前缀则都是在default命名空间下操作create &#x27;test&#x27;, &#x27;cf&#x27; # 一个列簇cfcreate &#x27;test2&#x27;, &#x27;cf1&#x27;, &#x27;cf2&#x27; # 两个列簇cf1和cf2list &#x27;test&#x27; # 确认表是否存在describe &#x27;test&#x27; # 查看表结构disable &#x27;test&#x27; # 使表失效enable &#x27;test&#x27; # 使表生效alter &#x27;test&#x27;, &#123;NAME=&gt;&#x27;cf1&#x27;, VERSION=&gt;3&#125; # 修改表的元数据信息drop &#x27;test&#x27; # 删除表，删除之前要disable这个表scan &#x27;test&#x27;, &#123;STARTROW=&gt;&#x27;1001&#x27;, STOPROW=&gt;&#x27;1003&#x27;&#125; # 扫描查看&#x27;test&#x27;表的所有数据 3、数据操作12345678910111213141516171819202122232425# 插入数据# 参数1：命名空间+表名# 参数2：行号，row key# 参数3：列名全限定名，即列簇名+列名# 参数4：值put &#x27;&#123;namespace&#125;:&#123;tablename&#125;&#x27;, &#x27;&#123;row key&#125;&#x27;, &#x27;&#123;column family&#125;:&#123;column qualifier&#125;&#x27;, &#x27;&#123;value&#125;&#x27;, &#x27;&#123;timestamp&#125;&#x27;, &#x27;&#123;其他属性&#125;&#x27;put &#x27;test&#x27;, &#x27;row1&#x27;, &#x27;cf:a&#x27;, &#x27;value1&#x27;put &#x27;test&#x27;, &#x27;row2&#x27;, &#x27;cf:b&#x27;, &#x27;value2&#x27;put &#x27;test&#x27;, &#x27;row3&#x27;, &#x27;cf:c&#x27;, &#x27;value3&#x27;# 根据表名和row key获取值get &#x27;&#123;namespace&#125;:&#123;tablename&#125;&#x27;, &#x27;&#123;row key&#125;&#x27;, &#x27;&#123;column family&#125;:&#123;column qualifier&#125;&#x27;get &#x27;test&#x27;, &#x27;row1&#x27;# 结果如下# COLUMN CELL# cf:a timestamp=2021-05-01T17:46:15.064, value=value1# 根据表名、row key和列删除数据，一个单元格delete &#x27;&#123;namespace&#125;:&#123;tablename&#125;&#x27;, &#x27;&#123;row key&#125;&#x27;, &#x27;&#123;column family&#125;:&#123;column qualifier&#125;&#x27;# 根据表名、row key删除数据，row key对应的一整行deleteall &#x27;&#123;namespace&#125;:&#123;tablename&#125;&#x27;, &#x27;&#123;row key&#125;&#x27;# 清空表truncate &#x27;&#123;namespace&#125;:&#123;tablename&#125;&#x27; 注意： row key为字符串类型，其排序是按照字符串的大小排序，如’10010’ &gt; ‘1001’。 get命令的列参数哪里如果没有冒号，则这个参数是指column family而不是column qualifier。 scan命令如果有STARTROW和STOPROW参数，则筛选范围为左闭右开！ 数据的改操作没有update命令，其实直接put进去一个值，就会自动完成改操作，有点类似于hashmap直接put后会覆盖原来的值这种。 创建表和列簇时的VERSION属性表示这个这个列簇最终将会存几个版本的数据，如VERSION=2，则取数据时设置VERSION=3也只能拿到两条数据，并且在hbase空间时，除最新的两个版本的数据会被保留外，其余版本数据都会从磁盘中删除。 五、分布式部署1、伪分布式伪分布式：在同一个服务器中部署，但是HMaster，HRegionServer和ZooKeeper服务在不同的JVM进程中。 前提：先启动zookeeper服务（可以单机模式），再启动hadoop服务（至少伪分布式），最后才能启动hbase 配置/conf/hbase-site.xml 1vi /conf/hbase-site.xml 123456789101112&lt;!-- 为true则是分布式的，为false则是单机版 --&gt;&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 这个是指数据的根目录在哪里，可以指定hdfs文件系统，即在hadoop的etc/hadoop/core-site.xml中配置的fs.defaultFS --&gt;&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;&lt;/property&gt;&lt;!-- 官网文档说还要删除hbase.tmp.dir配置和hbase.unsafe.stream.capability.enforce配置 --&gt; 启动 123456789101112131415# 启动hbasebin/start-hbase.sh# 在hadoop中校验是否在hdfs文件系统中创建了一个hbase的文件夹bin/hadoop fs -ls /hbase# zookeeper、hadoop和hbase全部启动成功后，运行jps命令结果应该是这样的。如果没有jps命令，linux会提示你安装一个jdk的东西jps6976 DataNode # hadoop6787 NameNode # hadoop6531 QuorumPeerMain # zookeeper7237 SecondaryNameNode # hadoop7941 Jps # jps7592 HMaster # hbase7786 HRegionServer # hbase 2、完全分布式hbase-site.xml配置文件 123456789101112131415161718192021222324&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;./tmp&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://192.168.0.201:9000/hbase&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;192.168.0.201,192.168.0.202,192.168.0.203&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/home/yury/zookeeper-3.6.3/zookeeper-data/hbase&lt;/value&gt;&lt;/property&gt; 3、注意点1、各个服务器之间的时间必须要同步，不然会出现不可预知的错误 六、HBase进阶1、HBase架构HMaster负责DDL操作，而HRegionServer负责DML操作，而实际操作中都会有zookeeper，zookeeper则是负责调度HRegionServer，因此当HMaster挂了，并不影响DML操作，只是不能进行DDL操作。 HLog类似于MySQL的Binlog，防止数据库奔溃时数据丢失。 2、写数据流程 收到put请求：put table/rowkey/cf/column value 前往meta-region-server这个服务器，请求查询该表的meta表所在RegioinServer 前往meta表所在服务器，请求查询该表和列簇所在的RegioinServer 将该put请求发送给对应的服务器 3、MemStore Flush即把内存中的数据刷新到HDFS中。 配置 解释 hbase.regionserver.global.memstore.size 全局配置，一个regionserver中所有memstore之和的最大值，默认为堆内存的40%，当memstore超过这个值时，就会阻塞写数据操作 hbase.regionserver.global.memstore.size.lower.limit 全局配置，一个regionserver中所有memstore之和的最大值，默认为堆内存的40%*0.95=38%，当memstore超过这个值时，就会开始flush操作，此时还不会阻塞写数据操作 hbase.regionserver.optionalcacheflushinterval 内存中的最后一次编辑的数据文件在自动刷新前能够存活的最长时间，默认1小时，当某些数据超过这个时间时，即使memstore内存没达到flush阈值，也会进行flush hbase.hregion.memstore.flush.size 单个region中memstore的缓存最大值，超过这个值时这个region就会进行flush，默认值为128M 4、读数据流程 收到get请求 前往meta-region-server查询meta表所在的RegionServer 前往meta表所在服务器，请求查询该表和列簇所在的RegioinServer 同时读memstore（内存）和storefile（磁盘），将两份数据读进block cache，取时间戳最大的那条数据。 5、StoreFile Compactionhdfs中的hfile文件的合并，compaction操作是先全部读出来，再重新合并在一起。有以下两种： Minor compaction：只选取一些小的文件进行合并，不会删除delete类型或时间戳更小的数据 Major compaction：将一个store下的所有hfile合并成一个大文件，对于相同rowkey且时间戳更小的数据会执行物理删除操作 配置： 配置 解释 hbase.hregion.majorcompaction 一个region进行自动major compaction的周期，默认为7天，即7天自动进行一次大合并。生产环境不建议开启（设置为0），因为很耗资源，而是手动进行major compaction hbase.hregion.majorcompaction.jitter 抖动比例，不管，反正都会被关掉 hbase.hstore.compactionThreshold 一个store中允许存的hfile的最大值，超过或等于这个值，就会被合并到一个新的hfile中，默认值为3 6、真正删数据发生在什么时候 进行flush时，内存中版本更老的数据会被删除，即老版本数据不会被写入hfile中 进行major compaction时会将老版本数据删除 7、Region Split 配置 解释 hbase.hregion.max.filesize 一个region的最大大小。默认值为10G。 当一个region中的某个store下的所有storefile总大小超过Min(&quot;count of region&quot;^2*&quot;hbase.hregion.memstore.flush.size&quot;, &quot;hbase.hregion.max.filesize&quot;)时，该region就会进行拆分。 数据热点问题： 第一个region的拆分的阈值为128M，拆分为两个，分别为64M rowkey是自增的，在第二个region后面新增数据 第二个region的拆分的阈值为512M（2^2*128），拆分为两个，分别为256M rowkey继续自增，在第三个region后面新增数据 第三个region的拆分阈值为1152M（3^2*128），拆分为两个，分别为576M rowkey继续自增…… 因此，这样的话第n个region的大小为：Min(n^2*64M, 5G)，即region在到达5G之前，各个regino的大小差异会比较大，即数据会集中在某几个region中，导致这几个region服务器压力很大。 官方建议，使用更少的列簇，将更多的列放进同一个列簇中，而不是创建更多的列簇，因为多个列簇flush后容易形成多个小文件 七、优化1、高可用在创建conf/backup-master这个文件，在里面写入备份的master结点的服务器，当主节点挂了之后，会选举一个备份主节点来顶替主节点的位置 12192.168.141.142192.168.141.143 2、预分区（1）手动设置预分区（更常用） 1create &quot;staff&quot;, &quot;info&quot;, &quot;partition&quot;, SPLITS =&gt; [&#x27;1000&#x27;, &#x27;2000&#x27;, &#x27;3000&#x27;, &#x27;4000&#x27;] （2）手动生成16进制预分区 1create &quot;staff2&quot;, &quot;info&quot;, &quot;partition2&quot;, &#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; &#x27;HexStringSplit&#x27;&#125; （3）按照文件中设置的分区规则预分区 123456# 在hbase根目录下touch splits.txtaaaabbbbccccdddd 12create &quot;staff3&quot;, &quot;info&quot;, &quot;partition3&quot;, SPLITS_FILE =&gt; &#x27;splits.txt&#x27;# 系统会给splits.txt这个文件进行排序 （4）使用javaAPI创建分区 略 3、rowkey的设计rowkey要保持散列性（随机性，使其可以随机落在不同的region中）、唯一性、长度足够长等原则，最好是70-100位字母或数字。 如生成随机数、hash、散列值、字符串拼接。 （1）案例1存储通话记录以及通话详情的rowkey设计。需存储的数据如下： 12phone_from phone_to time_start duration13112345678 13187654321 2021-01-01 12:12:12 45 首先根据业务对未来十年的数据的预期，需要设置300个分区，分区键分别是： 12345678000|001|002|...156|157|...298| 我们将rowkey前三位作为分区号，那么为了保证随机性，我们如何将数据散列分布在这300个分区内呢？即我们如何设计rowkey以保证数据会随机分布在300个分区中？ 此外根据业务需求，我们最好将同一拨出号码的同一个月份的通话记录放在一个分区内，以便以后做计算更快。 我们设计这样一个算法： 因此有300个分区，因此我们将rowkey的前三位作为分区号，从第4位开始，我们将拨出号码作为字符串拼接进去，再将通话开始时间拼接进去，以下划线分割，如下： 1xxx_13112345678_2021-01-01 12:12:12 那么如何将这个rowkey随机分布到300个分区中且同一拨出号码同一月份的通话记录在同一分区呢？即rowkey前面的xxx要根据后面的13112345678_2021-01-01 12:12:12来区分。 首先，不同的手机号可以随机区分开，因此可以用手机号进行hash，这样不同手机号的hashcode是随机分布的 其次，同一拨出号码的同一月份要放一起，因此可以将手机号+年月进行hash，这样同一手机号同一月份的hashcode是相同的 最后，我们只需要三位数字来存储分区号，因此我们可以通过获取hashcode除以299的余来获取分区号 1分区号=(hash(13112345678_202101))%299 技巧：我们对分区键的设计可以用一个常用符号中ascii序号最大的符号为结尾，如|，而rowkey中以一个ascii序号小点的符号为分隔符，如_。这样000|就会大于任意以000_开始的值。且我们在扫描表时，是左闭右开的原则，这样做对于rowkey的比较起来更方便，如下。 123456# 扫描001分区的数据STARTKEY =&gt; &#x27;001&#x27;, STOPKEY =&gt; &#x27;001|&#x27;# 扫描某拨出号码在4月份的数据STARTKEY =&gt; &#x27;XXX_13112345678_202103&#x27;, STOPKEY =&gt; &#x27;XXX_13112345678_202103|&#x27;# 其中xxx=(hash(13112345678_202103))%299 4、基础优化（1）允许在HDFS的文件中追加内容 hdfs-site.xml、hbase-site.xml 属性：dfs.support.append 解释：开启HDFS追加同步，可以优秀地配合HBase的数据同步和持久化。默认值为true。 （2）优化DataNode允许的最大文件打开数 hdfs-site.xml 属性：dfs.datanode.max.transfer.threads 解释：","categories":[{"name":"bigdata","slug":"bigdata","permalink":"https://yury757.github.io/categories/bigdata/"}],"tags":[]},{"title":"mybatis-study","slug":"java/mybatis/mybatis-study","date":"2021-08-23T16:00:00.000Z","updated":"2022-10-07T12:55:59.831Z","comments":true,"path":"java/mybatis/mybatis-study/","link":"","permalink":"https://yury757.github.io/java/mybatis/mybatis-study/","excerpt":"","text":"练习用代码：yury757/Mybatis-Study (github.com) 一、Mybatis问题Mybatis遇到的问题大部分有以下五类： 1、配置文件没有注册 2、绑定接口错误 3、方法名不对 4、返回类型不对 5、Maven导出资源问题 二、Mybatis实现方式 写一个实体类和对应的查询接口 本来我们应该手写实现这个查询接口的类，并在对应的方法里面写sql语句、使用SqlSession执行SQL语句，再把结果集强转成我们自己的实体类。 Mybatis则不需要我们手写这个实现类，而是弄了一个mapper的xml文件，里面定义了某个接口的某个方法的实现，我们只需要在xml中定义这个方法的SQL语句、参数类型、参数集、结果类型、结果集等标签。 再将对应的mapper注册到Mybatis的配置文件中。 然后项目启动时，Mybatis框架去配置文件的注册中心中把注册过的类提前实现好，生成.class字节码文件（猜测），我们只需要通过getMapper(UserDao.class)方法（这个方法里面肯定封装了newInstance或类似的方法）就可以拿到对应类的实例，然后直接调用相应的方法就行。而且会自动帮我们把结果集封装到mapper定义的结果类型中。 三、Mybatis中的三个核心类（1）SqlSessionFactoryBuilder这个类是用于创建SqlSessionFactory对象的，SqlSessionFactory对象一旦创建就不再需要SqlSessionFactoryBuilder了。 1234567891011// 使用mybatis第一步，获取SqlSessionFactory对象static&#123; String resource = &quot;mybatis-config.xml&quot;; InputStream inputStream = null; try &#123; inputStream = Resources.getResourceAsStream(resource); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream);&#125; （2）SqlSessionFactorySqlSessionFactory一旦被创建，应该在程序运行期间一直存在，因为它是创建SqlSession对象的工厂。默认为单例模式。 （3）SqlSessionSqlSession是用于访问数据库的一个会话。 SqlSession实例不是线程安全的，因此避免被共享，最佳的使用域是请求或非静态方法作用域。 使用完一个SqlSession后一定一定一定要关闭它，为避免关闭资源时异常，最好使用以下方式使用SqlSession 123456789// 获取SqlSession对象的方法public static SqlSession getSqlSession()&#123; return sqlSessionFactory.openSession();&#125;// 重载方法，选择是否自动提交public static SqlSession getSqlSession(boolean autoCommit)&#123; return sqlSessionFactory.openSession(autoCommit);&#125; 1234567try(SqlSession sqlSession = MybatisUtils.getSqlSession())&#123; UserDao userDao = sqlSession.getMapper(UserDao.class); List&lt;User&gt; userList = userDao.getUserList(); for (User user : userList) &#123; System.out.println(user.toString()); &#125;&#125; 四、Mapper标签属性注意事项 id：对应接口的方法名 resultType：结果集类型，要写全限定类名，或别名 parameterType：参数类型 当接口方法只有一个参数时，#&#123;&#125;中有以下几种填法 若传入参数类型是一个实体类或其他类，#&#123;&#125;可直接填入相应属性名 若传入参数类型是Map接口类（可以用别名map代表Map），#&#123;&#125;可直接填入相应的键值 若传入参数是String、int等其他类型，#&#123;&#125;填任意值数字或字母的组合都行，建议使用param1 当接口方法只有多个参数时，parameterType可不填，#&#123;&#125;按接口方法的参数顺序填入#&#123;param1&#125;、#&#123;param2&#125;。或者在接口处使用@param注解，给参数起一个别名。 12345678910111213141516171819&lt;!-- 有两个类型相同的参数的查询 --&gt;&lt;select id=&quot;getTwoUserById&quot; parameterType=&quot;int&quot; resultType=&quot;org.xxxx.pojo.User&quot;&gt; select * from user where id = #&#123;param1&#125; or id = #&#123;param2&#125;&lt;/select&gt;&lt;!-- 有两个类型不同的参数的查询2 --&gt;&lt;select id=&quot;getTwoUserById2&quot; parameterType=&quot;Object&quot; resultType=&quot;org.xxxx.pojo.User&quot;&gt; select * from user where id = #&#123;param1&#125; or name = #&#123;param2&#125;&lt;/select&gt;&lt;!-- 有两个类型不同的参数的查询3 --&gt;&lt;select id=&quot;getTwoUserById3&quot; resultType=&quot;org.xxxx.pojo.User&quot;&gt; select * from user where id = #&#123;param1&#125; or id = #&#123;param2.id&#125;&lt;/select&gt;&lt;!-- 有两个类型不同的参数的查询4 --&gt;&lt;select id=&quot;getTwoUserById4&quot; resultType=&quot;org.xxxx.pojo.User&quot;&gt; select * from user where id = #&#123;id&#125; or id = #&#123;user.id&#125;&lt;/select&gt; 1234/** * 有两个类型不同的参数的查询4，使用@Param注解 */public List&lt;User&gt; getTwoUserById4(@Param(&quot;id&quot;) int id,@Param(&quot;user&quot;) User user); 模糊查询有两种方式 在mapper中这样用来拼接%：like &quot;%&quot;#&#123;param1&#125;&quot;%&quot; mapper中仍然使用like #&#123;param1&#125;，而在调用方式时手动在传入参数两边加上% 推荐使用第一种，因为在参数里面加%可能面临被转义的风险。 1234567&lt;select id=&quot;getUserLike1&quot; parameterType=&quot;string&quot; resultType=&quot;org.xxxx.pojo.User&quot;&gt; select * from user where name like &quot;%&quot;#&#123;param1&#125;&quot;%&quot;&lt;/select&gt;&lt;select id=&quot;getUserLike2&quot; parameterType=&quot;string&quot; resultType=&quot;org.xxxx.pojo.User&quot;&gt; select * from user where name like #&#123;param1&#125;&lt;/select&gt; resultMap：结果集映射，将从数据库中取出来的字段和类中的属性做一个映射关系，为解决数据库字段名和类属性名不一致的问题。column为数据库字段名，property为类的属性名。 12345&lt;resultMap id=&quot;UserMap&quot; type=&quot;user&quot;&gt; &lt;result column=&quot;id&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;name&quot; property=&quot;name&quot;/&gt; &lt;result column=&quot;password&quot; property=&quot;pwd&quot;/&gt;&lt;/resultMap&gt; 五、mybatis-config.xml配置解析（1）properties标签可以引入其他某个.properties文件，作为参数值在本配置文件中使用。 1&lt;properties resource=&quot;db.properties&quot;/&gt; 也可以可以加入property标签加入自定义参数。 对于有重复的参数，参数调用顺序是，先生成property标签中的参数，再读取引入的配置文件中的参数，对于有重复的参数会被覆盖掉，理解成一个HashMap即可。 （2）settings标签有以下属性：https://mybatis.org/mybatis-3/zh/configuration.html#settings 主要用的有： cacheEnabled：缓存 useGeneratedKeys：自动生成主键 mapUnderscoreToCamelCase：数据库字段名转java属性名时自动重命名 logImpl：日志实现类 官网也给了一个建议的设置如下： 1234567891011121314151617&lt;settings&gt; &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt; &lt;setting name=&quot;lazyLoadingEnabled&quot; value=&quot;true&quot;/&gt; &lt;setting name=&quot;multipleResultSetsEnabled&quot; value=&quot;true&quot;/&gt; &lt;setting name=&quot;useColumnLabel&quot; value=&quot;true&quot;/&gt; &lt;setting name=&quot;useGeneratedKeys&quot; value=&quot;false&quot;/&gt; &lt;setting name=&quot;autoMappingBehavior&quot; value=&quot;PARTIAL&quot;/&gt; &lt;setting name=&quot;autoMappingUnknownColumnBehavior&quot; value=&quot;WARNING&quot;/&gt; &lt;setting name=&quot;defaultExecutorType&quot; value=&quot;SIMPLE&quot;/&gt; &lt;setting name=&quot;defaultStatementTimeout&quot; value=&quot;25&quot;/&gt; &lt;setting name=&quot;defaultFetchSize&quot; value=&quot;100&quot;/&gt; &lt;setting name=&quot;safeRowBoundsEnabled&quot; value=&quot;false&quot;/&gt; &lt;setting name=&quot;mapUnderscoreToCamelCase&quot; value=&quot;false&quot;/&gt; &lt;setting name=&quot;localCacheScope&quot; value=&quot;SESSION&quot;/&gt; &lt;setting name=&quot;jdbcTypeForNull&quot; value=&quot;OTHER&quot;/&gt; &lt;setting name=&quot;lazyLoadTriggerMethods&quot; value=&quot;equals,clone,hashCode,toString&quot;/&gt;&lt;/settings&gt; （3）typeAliases标签为类型设置别名，这样避免了写全限定类型或全限定接口名。 当为一整个包的类设置别名时，若类型带有@Alias注解时，别名为注解值；否则别名为对应类的类型，首字母小写。 1234&lt;typeAliases&gt; &lt;!-- &lt;typeAlias type=&quot;org.yuyr757.pojo.User&quot; alias=&quot;UserAlias&quot;/&gt; --&gt; &lt;package name=&quot;org.xxxxx.pojo&quot;/&gt;&lt;/typeAliases&gt; Mybatis有一些默认别名，如下：https://mybatis.org/mybatis-3/zh/configuration.html#typeAliases （4）mappers映射器官网有四种写法，使用完全限定资源定位符（URL）不推荐使用。 12345&lt;mappers&gt; &lt;!-- &lt;mapper resource=&quot;org/xxxx/Dao/UserMapper.xml&quot;/&gt;--&gt; &lt;!-- &lt;mapper class=&quot;org.xxxx.Dao.UserMapper&quot;/&gt;--&gt; &lt;package name=&quot;org.xxxx.Dao&quot;/&gt;&lt;/mappers&gt; 最推荐使用第四种，将包内的映射器接口实现全部注册为映射器。使用条件： 接口和mapper必须放在同一个包下，建议包名为Dao，同一个包下是指编译后同一个包下，可以在resources目录下也新建一个org.xxxx.Dao目录，这样接口和mapper配置就会编译到同一个包下了。 接口和mapper两个文件名必须相同（文件类型后缀不管） 使用这种方式必须在pom.xml中把src/java/main下的xml文件作为配置文件添加到build.resources.resource中 123456java.org.xxxx.Dao UserMapper.java（接口） Department.javaresources.org.xxxx.Dao UserMapper.xml（mapper） Department.xml 六、分页1、在mapper的sql语句中把startIndex和endIndex作为参数传入进去 2、分页插件PageHelper：https://pagehelper.github.io/ 七、使用注解开发实现方式：反射、动态代理 123456789101112131415/** * 对于这种很简单的sql，可以不用写mapper，直接写一个Select注解，里面传入sql值即可 * 注意点： * 1、数据库字段名和类属性名要相同 * 2、returnType为接口的返回类型 * 3、parameterType为接口的参数类型 */@Select(&quot;select * from user where id = #&#123;param1&#125;&quot;)public List&lt;User&gt; getUserByIdUsingAnnotation(int id);@Select(&quot;select * from user where id = #&#123;param1&#125; or name = #&#123;param2&#125;&quot;)public List&lt;User&gt; getUserByIdUsingAnnotation2(int id, String name);@Insert(&quot;insert into user(id, name, pwd) values (#&#123;id&#125;, #&#123;name&#125;, #&#123;pwd&#125;)&quot;)public void addUserUsingAnnotation(User user); 八、连表查询1在数据库设计时，为降低数据的冗余，一般都会做到三范式。比如学生老师信息表可能会做成以下这种方式： 比如一个学生表如下： ID NAME TEACHER_ID 1 小明 1 2 小五 1 3 小华 3 4 小石 2 5 李笑 3 6 孙武 2 7 黄铭 2 一个老师表如下： ID NAME 1 李老师 2 黄老师 3 钱老师 因此我们的java对象应该是这样的： 12345678910public class Student &#123; private int id; private String name; private Teacher teacher; // 引用了一个老师&#125;public class Teacher &#123; private int id; private String name;&#125; 这样我们在配置mapper时有两种方法： 1、通过子查询方式1234567891011121314&lt;select id=&quot;getStudent&quot; resultMap=&quot;studentTeacher&quot;&gt; select * from student&lt;/select&gt;&lt;resultMap id=&quot;studentTeacher&quot; type=&quot;student&quot;&gt; &lt;result property=&quot;id&quot; column=&quot;id&quot;/&gt; &lt;result property=&quot;name&quot; column=&quot;name&quot;/&gt; &lt;!-- 对象使用association，集合使用collection --&gt; &lt;association property=&quot;teacher&quot; column=&quot;teacher_id&quot; javaType=&quot;teacher&quot; select=&quot;getTeacher&quot;/&gt;&lt;/resultMap&gt;&lt;select id=&quot;getTeacher&quot; resultType=&quot;teacher&quot;&gt; select * from teacher where id = #&#123;id&#125;&lt;/select&gt; 其中association标签的属性解释： property：属性名 column：该属性要用数据库中的某个字段名去关联查询 javaType：该属性的类型 select：从数据库拿到这个类的数据的select语句 这种方式实际上就是把查询出来的关联字段去重，去重后再去数据库里面查相应的数据，再封装到对象中。 如打开日志后可以发现这种方式实际上查了四次数据库。 12345678910111213141516171819202122232021-02-22 17:40:11[ DEBUG ]Opening JDBC Connection2021-02-22 17:40:12[ DEBUG ]Created connection 202125197.2021-02-22 17:40:12[ DEBUG ]==&gt; Preparing: select * from student 2021-02-22 17:40:12[ DEBUG ]==&gt; Parameters: 2021-02-22 17:40:12[ DEBUG ]====&gt; Preparing: select * from teacher where id = ? 2021-02-22 17:40:12[ DEBUG ]====&gt; Parameters: 1(Integer)2021-02-22 17:40:12[ DEBUG ]&lt;==== Total: 12021-02-22 17:40:12[ DEBUG ]====&gt; Preparing: select * from teacher where id = ? 2021-02-22 17:40:12[ DEBUG ]====&gt; Parameters: 3(Integer)2021-02-22 17:40:12[ DEBUG ]&lt;==== Total: 12021-02-22 17:40:12[ DEBUG ]====&gt; Preparing: select * from teacher where id = ? 2021-02-22 17:40:12[ DEBUG ]====&gt; Parameters: 2(Integer)2021-02-22 17:40:12[ DEBUG ]&lt;==== Total: 12021-02-22 17:40:12[ DEBUG ]&lt;== Total: 7Student&#123;id=1, name=&#x27;小明&#x27;, teacher=Teacher&#123;id=1, name=&#x27;李老师&#x27;&#125;&#125;Student&#123;id=2, name=&#x27;小五&#x27;, teacher=Teacher&#123;id=1, name=&#x27;李老师&#x27;&#125;&#125;Student&#123;id=3, name=&#x27;小华&#x27;, teacher=Teacher&#123;id=3, name=&#x27;钱老师&#x27;&#125;&#125;Student&#123;id=4, name=&#x27;小石&#x27;, teacher=Teacher&#123;id=2, name=&#x27;黄老师&#x27;&#125;&#125;Student&#123;id=5, name=&#x27;李笑&#x27;, teacher=Teacher&#123;id=3, name=&#x27;钱老师&#x27;&#125;&#125;Student&#123;id=6, name=&#x27;孙武&#x27;, teacher=Teacher&#123;id=2, name=&#x27;黄老师&#x27;&#125;&#125;Student&#123;id=7, name=&#x27;黄铭&#x27;, teacher=Teacher&#123;id=2, name=&#x27;黄老师&#x27;&#125;&#125;2021-02-22 17:40:12[ DEBUG ]Closing JDBC Connection [com.mysql.cj.jdbc.ConnectionImpl@c0c2f8d]2021-02-22 17:40:12[ DEBUG ]Returned connection 202125197 to pool. 2、通过连表查询方式12345678910111213&lt;select id=&quot;getStudent2&quot; resultMap=&quot;studentTeacher2&quot;&gt; select s.id, s.name, s.teacher_id, t.name as teacher_name from student s left join teacher t on s.teacher_id = t.id&lt;/select&gt;&lt;resultMap id=&quot;studentTeacher2&quot; type=&quot;student&quot;&gt; &lt;result property=&quot;id&quot; column=&quot;id&quot;/&gt; &lt;result property=&quot;name&quot; column=&quot;name&quot;/&gt; &lt;association property=&quot;teacher&quot; javaType=&quot;teacher&quot;&gt; &lt;result property=&quot;id&quot; column=&quot;teacher_id&quot;/&gt; &lt;result property=&quot;name&quot; column=&quot;teacher_name&quot;/&gt; &lt;/association&gt;&lt;/resultMap&gt; 同样有一个association标签，而下面还有封装这个teacher类的子标签，子标签定义了初始化这个类所需要的字段映射。 property：属性名 javaType：该属性的类型 这种方式只需要查一次数据库： 12345678910111213142021-02-22 17:41:58[ DEBUG ]Opening JDBC Connection2021-02-22 17:41:59[ DEBUG ]Created connection 202125197.2021-02-22 17:41:59[ DEBUG ]==&gt; Preparing: select s.id, s.name, s.teacher_id, t.name as teacher_name from student s left join teacher t on s.teacher_id = t.id 2021-02-22 17:41:59[ DEBUG ]==&gt; Parameters: 2021-02-22 17:41:59[ DEBUG ]&lt;== Total: 7Student&#123;id=1, name=&#x27;小明&#x27;, teacher=Teacher&#123;id=1, name=&#x27;李老师&#x27;&#125;&#125;Student&#123;id=2, name=&#x27;小五&#x27;, teacher=Teacher&#123;id=1, name=&#x27;李老师&#x27;&#125;&#125;Student&#123;id=4, name=&#x27;小石&#x27;, teacher=Teacher&#123;id=2, name=&#x27;黄老师&#x27;&#125;&#125;Student&#123;id=6, name=&#x27;孙武&#x27;, teacher=Teacher&#123;id=2, name=&#x27;黄老师&#x27;&#125;&#125;Student&#123;id=7, name=&#x27;黄铭&#x27;, teacher=Teacher&#123;id=2, name=&#x27;黄老师&#x27;&#125;&#125;Student&#123;id=3, name=&#x27;小华&#x27;, teacher=Teacher&#123;id=3, name=&#x27;钱老师&#x27;&#125;&#125;Student&#123;id=5, name=&#x27;李笑&#x27;, teacher=Teacher&#123;id=3, name=&#x27;钱老师&#x27;&#125;&#125;2021-02-22 17:41:59[ DEBUG ]Closing JDBC Connection [com.mysql.cj.jdbc.ConnectionImpl@c0c2f8d]2021-02-22 17:41:59[ DEBUG ]Returned connection 202125197 to pool. 具体使用哪种方式视情况而定，简单的连表可以使用第二种。当连表查询的sql特别复杂，以致于难以在sql层面去优化时，可以使用第一种，主查询把其他需要连的表的主键查询来，子查询再用主键去查，可能会提高效率。 九、连表查询2对于以上的学生老师表，我们的java类还可能是这样的： 1234567891011public class Student2 &#123; private int id; private String name; private int teacherId;&#125;public class Teacher2 &#123; private int id; private String name; private List&lt;Student&gt; students; // 老师这里有多个学生对象的引用&#125; 同样有子查询和连表查询两种方式： 1、通过子查询方式123456789101112131415161718&lt;!-- 子查询 --&gt;&lt;select id=&quot;getTeacherById2&quot; resultMap=&quot;teacher2Student2&quot;&gt; select t.id, t.name from teacher t where t.id = #&#123;id&#125;&lt;/select&gt;&lt;resultMap id=&quot;teacher2Student2&quot; type=&quot;teacher2&quot;&gt; &lt;result property=&quot;id&quot; column=&quot;id&quot;/&gt; &lt;result property=&quot;name&quot; column=&quot;name&quot;/&gt; &lt;collection property=&quot;students&quot; column=&quot;id&quot; javaType=&quot;ArrayList&quot; ofType=&quot;student2&quot; select=&quot;getStudent&quot;/&gt;&lt;/resultMap&gt;&lt;select id=&quot;getStudent&quot; resultMap=&quot;student2Map&quot;&gt; select * from student where teacher_id = #&#123;id&#125;&lt;/select&gt;&lt;resultMap id=&quot;student2Map&quot; type=&quot;student2&quot;&gt; &lt;result property=&quot;teacherId&quot; column=&quot;teacher_id&quot;/&gt;&lt;/resultMap&gt; 2、通过连表查询方式1234567891011121314151617&lt;!-- 连表查询 --&gt;&lt;select id=&quot;getTeacherById&quot; resultMap=&quot;teacher2Student&quot;&gt; select t.id, t.name, s.id as student_id, s.name as student_name from teacher t left join student s on t.id = s.teacher_id where t.id = #&#123;id&#125;&lt;/select&gt;&lt;resultMap id=&quot;teacher2Student&quot; type=&quot;teacher2&quot;&gt; &lt;result property=&quot;id&quot; column=&quot;id&quot;/&gt; &lt;result property=&quot;name&quot; column=&quot;name&quot;/&gt; &lt;!-- 这里要用ofType，即集合的元素类型 --&gt; &lt;collection property=&quot;students&quot; ofType=&quot;student2&quot;&gt; &lt;result property=&quot;id&quot; column=&quot;student_id&quot;/&gt; &lt;result property=&quot;name&quot; column=&quot;student_name&quot;/&gt; &lt;result property=&quot;teacherId&quot; column=&quot;id&quot;/&gt; &lt;/collection&gt;&lt;/resultMap&gt; 十、缓存1、本地缓存。作用域为SqlSession，默认开启。 在一个session中查两次相同的sql，只会执行一次sql，第二次拿到的对象，和第一次拿到的对象的地址都是一样的。本地缓存将会在做出修改、事务提交或回滚，以及关闭session时清空。默认情况下，本地缓存数据的生命周期等同于整个session的周期。 2、二级缓存。作用域为mapper的namespace，当sqlsession作出修改、事务提交、回滚或关闭时，会把本地缓存扔到二级缓存中。即一级缓存失效时，会把其缓存的数据扔到二级缓存中。 需要在mapper中加入&lt;cache/&gt;就可以为这个mapper开启二级缓存。 12345&lt;cache eviction=&quot;FIFO&quot; flushInterval=&quot;60000&quot; size=&quot;512&quot; readOnly=&quot;true&quot;/&gt; 在mybatis-config.xml配置中，设置cacheEnabled为true可以为所有mapper开启二级缓存。 缓存清除策略： LRU – 最近最少使用：移除最长时间不被使用的对象。 FIFO – 先进先出：按对象进入缓存的顺序来移除它们。 SOFT – 软引用：基于垃圾回收器状态和软引用规则移除对象。 WEAK – 弱引用：更积极地基于垃圾收集器状态和弱引用规则移除对象。 3、缓存顺序二级缓存 =&gt; 本地缓存 =&gt; 数据库 N、注意事项 insert、update、delete要手动提交事务： 1sqlSession.commit();","categories":[{"name":"java","slug":"java","permalink":"https://yury757.github.io/categories/java/"}],"tags":[]},{"title":"springMVC-study","slug":"java/springMVC/SpringMVC-Study","date":"2021-08-23T16:00:00.000Z","updated":"2022-10-07T12:56:58.799Z","comments":true,"path":"java/springMVC/SpringMVC-Study/","link":"","permalink":"https://yury757.github.io/java/springMVC/SpringMVC-Study/","excerpt":"","text":"一、回顾Servlet12345// 转发，forwardrequest.getRequestDispatcher(&quot;/WEB-INF/jsp/hello.jsp&quot;).forward(request, response);// 重定向，redirectresponse.sendRedirect(&quot;/index.jsp&quot;); 1234&lt;!-- session失效时间，单位分钟 --&gt;&lt;session-config&gt; &lt;session-timeout&gt;1&lt;/session-timeout&gt;&lt;/session-config&gt; 二、SpringMVC开始1、前言约定大于配置。 最重要的一个类：DispatcherServlet 1234567891011public class DispatcherServlet extends FrameworkServlet&#123; public DispatcherServlet(WebApplicationContext webApplicationContext) &#123; super(webApplicationContext); setDispatchOptionsRequest(true); &#125; protected void doService(HttpServletRequest request, HttpServletResponse response) throws Exception &#123;&#125; protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123;&#125; &#125; 这个类的作用就是就是把不同的请求分发到不同的类。 2、配置springmvc（重要！）1、web.xml123456789101112131415161718192021222324252627282930&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd&quot; version=&quot;4.0&quot;&gt; &lt;!-- 开始配置SpringMVC --&gt; &lt;!-- 1、注册DispatcherServlet --&gt; &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 关联一个springmvc配置文件，本质是一个spring配置文件 --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc-config.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;!-- 启动级别 --&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;!-- / 和 /* 是有区别的，用/，不能用/* / ：匹配所有请求，不会匹配jsp /* ：匹配所有请求，包括jsp，即把返回一个.jsp页面也当作了一个请求 --&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; 2、springmvc-config.xml本质是一个spring配置文件 12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!-- 处理器映射器，有多种映射器 --&gt; &lt;!-- 这种映射器是通过bean的名字查找 --&gt; &lt;bean class=&quot;org.springframework.web.servlet.handler.BeanNameUrlHandlerMapping&quot; /&gt; &lt;!-- 处理器适配器 --&gt; &lt;bean class=&quot;org.springframework.web.servlet.mvc.SimpleControllerHandlerAdapter&quot; /&gt; &lt;!-- 视图解析器，配置了前后缀，以后重定向到某个jsp时就可以不用写前后缀了 --&gt; &lt;bean id=&quot;internalResourceViewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;!-- 前缀 --&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/jsp/&quot;/&gt; &lt;!-- 后缀 --&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt; &lt;!-- 配置handler --&gt; &lt;!-- 上面那个映射器会去查找和url名字相同的bean id --&gt; &lt;!-- 找到之后上面那个适配器就会把请求交给对应bean的class去处理 --&gt; &lt;bean id=&quot;/hello2&quot; class=&quot;org.yuyr757.controller.Hello2Controller&quot;/&gt;&lt;/beans&gt; 如下图 配置处理器映射器就是为了做2、3、4三步，去找到对应的handler 配置处理器适配器就是为了做5、6、7、8四步，把对应的handler交给controller处理 配置视图解析器就是为了做9、10、11、12四步，把controller处理好的带model和view名字的MV对象交给视图解析器，先去处理对应的jsp，然后把生成好的页面返回给浏览器。 3、配置springmvc时404的问题原因之一可能是：IDEA的项目结构中的Artifacts的utput Layout要确保WEB-INF目录下有classes和lib两个目录，若没有lib目录，则新建一个，然后把所有我们的依赖包放到lib目录中。 4、使用springmvc这里是采用实现Controller接口的方式，不建议使用，建议使用下面注解开发。 12345678910111213141516171819202122232425package org.yuyr757.controller;import org.springframework.web.servlet.ModelAndView;import org.springframework.web.servlet.mvc.Controller;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;// 注意！！这个Controller是org.springframework.web.servlet.mvc.Controller，是一个接口// 而不是org.springframework.stereotype.Controller，这个Controller是注解用的public class Hello2Controller implements Controller &#123; @Override public ModelAndView handleRequest(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; // 模型和视图 ModelAndView mv = new ModelAndView(); // 封装对象 mv.addObject(&quot;msg&quot;, &quot;HelloSpringMVC&quot;); // 封装要跳转的对象 mv.setViewName(&quot;hello2&quot;); System.out.println(&quot;---&quot;); return mv; &#125;&#125; 三、使用注解开发SpringMVCweb.xml中的配置不变，springmvc-config.xml中的配置如下： 12345678910111213141516171819202122232425262728&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc https://www.springframework.org/schema/mvc/spring-mvc.xsd&quot;&gt; &lt;!-- 自动扫描包，让指定包下的注解生效，由IOC容器统一管理 --&gt; &lt;context:component-scan base-package=&quot;org.yuye757.controller&quot;/&gt; &lt;!-- 配置对url的检查，将一些静态资源交给默认的Servlet处理，非静态资源才让DispatcherServlet处理 --&gt; &lt;mvc:default-servlet-handler/&gt; &lt;!-- 开启注解 --&gt; &lt;mvc:annotation-driven/&gt; &lt;!-- 视图解析器，配置了前后缀，以后重定向到某个jsp时就可以不用写前后缀了 --&gt; &lt;bean id=&quot;internalResourceViewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;!-- 前缀 --&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/jsp/&quot;/&gt; &lt;!-- 后缀 --&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 123456789101112131415161718192021222324package org.yuye757.controller;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.RequestMapping;@Controller@RequestMapping(&quot;/hello&quot;) // 不写的话，就直接走方法的mapping uripublic class HelloController &#123; @RequestMapping(&quot;/h1&quot;) // 如果没有配置restful，这里一定要写，不写则找不到这个方法的uri // uil为：localhost:8080/warName/hello/h1 public String index(Model model)&#123; // 封装数据 model.addAttribute(&quot;msg&quot;, &quot;Hello, SpringMVC Annotation!&quot;); String viewName = &quot;hello&quot;; // 加了@Controller注解的类下的所有加了@RequestMapping的方法 // 若返回的类型是字符串，且能够找到对应的jsp，就会被视图解析器处理 // /WEB-INF/jsp/$&#123;viewName&#125;.jsp return viewName; &#125;&#125; 四、restful风格的urijsp不支持DELETE、PUT类型的方法，以下那两个方法看看即可。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package org.yuye757.controller;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.*;@Controller@RequestMapping(&quot;/restful&quot;)public class RestfulController &#123; // 原来的方式：localhost:8080/warName/test?a=1&amp;b=2 @RequestMapping(&quot;/test&quot;) public String test(Model model, int a, int b)&#123; int res = a + b; model.addAttribute(&quot;msg&quot;, res); return &quot;test&quot;; &#125; // restful方式：localhost:8080/warName/test/1/2 // @RequestMapping(value = &quot;/test2/&#123;a&#125;/&#123;b&#125;&quot;, method = RequestMethod.GET) @GetMapping(&quot;/test2/&#123;a&#125;/&#123;b&#125;&quot;) public String test2(@PathVariable int a, @PathVariable int b, Model model)&#123; int res = a + b; model.addAttribute(&quot;msg&quot;, &quot;GET方法：&quot; + res); return &quot;test&quot;; &#125; // restful方式：localhost:8080/warName/test/1/2 // @RequestMapping(value = &quot;/test2/&#123;a&#125;/&#123;b&#125;&quot;, method = RequestMethod.POST) @PostMapping(&quot;/test2/&#123;a&#125;/&#123;b&#125;&quot;) public String test3(@PathVariable int a, @PathVariable int b, Model model)&#123; int res = a - b; model.addAttribute(&quot;msg&quot;, &quot;POST方法：&quot; + res); return &quot;test&quot;; &#125; // restful方式：localhost:8080/warName/test/1/2 // @RequestMapping(value = &quot;/test2/&#123;a&#125;/&#123;b&#125;&quot;, method = RequestMethod.PUT) @PutMapping(&quot;/test2/&#123;a&#125;/&#123;b&#125;&quot;) public String test4(@PathVariable int a, @PathVariable int b, Model model)&#123; int res = a * b; model.addAttribute(&quot;msg&quot;, &quot;PUT方法：&quot; + res); return &quot;test&quot;; &#125; // restful方式：localhost:8080/warName/test/1/2 // @RequestMapping(value = &quot;/test2/&#123;a&#125;/&#123;b&#125;&quot;, method = RequestMethod.DELETE) @DeleteMapping(&quot;/test2/&#123;a&#125;/&#123;b&#125;&quot;) public String test5(@PathVariable int a, @PathVariable int b, Model model)&#123; int res = a / b; model.addAttribute(&quot;msg&quot;, &quot;DELETE方法：&quot; + res); return &quot;test&quot;; &#125;&#125; 五、springmvc使用细节1、转发和重定向可以在方法中加入request、response参数，使用servlet原生的转发或重定向方式。 在springmvc中可以这样： 1234567891011121314151617181920212223package org.yuye757.controller;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PostMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.servlet.ModelAndView;@Controller@RequestMapping(&quot;/modelTest&quot;)public class ModelTest1 &#123; @PostMapping(value = &quot;/test1&quot;) public ModelAndView test()&#123; ModelAndView mv = new ModelAndView(); // 重定向：redirect mv.setViewName(&quot;redirect:/restful/test2/10/5&quot;); // 转发：forward // mv.setViewName(&quot;forward:/restful/test2/10/5&quot;); return mv; &#125;&#125; 注意：通过这种方式的转发会带上方法的类型，如POST、PUT。但是重定向不会，默认是GET方法。因为重定向实际上是重新发起了一次请求，因此默认是GET。 2、参数12345678910111213141516171819202122232425262728293031323334353637383940package org.yuye757.controller;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import org.yuye757.pojo.User;@Controller@RequestMapping(&quot;/user&quot;)public class UserController &#123; @GetMapping(&quot;/t1&quot;) // 若有@RequestParam，则前端传入的参数以注解里面的名字为准，此时变量名失效。没有的话就只能用变量名。 // 建议都加上@RequestParam，这样可以很明显的告诉别人这是要从前端接收的参数 // http://localhost:8080/user/t1?username=123456 public String test1(Model model, @RequestParam(&quot;username&quot;) String name)&#123; System.out.println(&quot;前端接收到的参数：&quot; + name); model.addAttribute(&quot;msg&quot;, name); return &quot;test&quot;; &#125; /* 1、若参数为普通类型，则通过方法的参数名字和url的参数名字匹配 2、若参数为对象，则会调用无参构造方法，再按照对象属性名和url的参数名去匹配，匹配到的就会调用其setter方法 和url参数名没匹配上的属性或没有setter方法的属性则没有值 若没有无参构造方法，则调用有参构造方法。总之把pojo类的构造方法写全是最好的。 */ // http://localhost:8080/user/t2?id=1&amp;name=我是一个名字&amp;age=12 @GetMapping(&quot;/t2&quot;) public String test2(User user, Model model)&#123; String s = user.toString(); System.out.println(s); model.addAttribute(&quot;msg&quot;, s); return &quot;test&quot;; &#125;&#125; 3、乱码配置web.xml 123456789101112131415&lt;!-- 之前我们自己写filter来解决乱码问题 --&gt;&lt;!-- 在springmvc中，他给我们写了一个过滤器来解决乱码 --&gt;&lt;filter&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;utf-8&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;!-- 注意这里要用/*，之前上面说了/*可以把jsp资源也包括在处理范围类 --&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 4、返回值（1）让方法返回一个纯字符串给前端，而不是走视图解析器 在类上面加@RestController，这个注解可以使类中的所有方法都返回字符串，而不是走视图解析器 在方法上面加@ResponseBody注解 （2）返回json字符串 使用jackson包 1234567891011121314151617181920212223package org.yuyr757.controller;import com.fasterxml.jackson.core.JsonProcessingException;import com.fasterxml.jackson.databind.ObjectMapper;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;import org.springframework.web.bind.annotation.RestController;import org.yuyr757.User.User;// @RestController这个注解可以使类中的所有方法都返回字符串，而不是走视图解析器@RestControllerpublic class UserController &#123; @ResponseBody // 使用这个注解，则不会走视图解析器，而是直接返回一个字符串 // produces = &quot;application/json;charset=utf-8&quot;，加上这个指明返回的页面格式和编码 @RequestMapping(value = &quot;/user/j1&quot;) public String json2() throws JsonProcessingException &#123; User user = new User(1, &quot;你好&quot;, 2); ObjectMapper objectMapper = new ObjectMapper(); // 使用jackson包 String s = objectMapper.writeValueAsString(user); return s; &#125;&#125; 使用fastjson包 123456789101112131415161718192021222324@RequestMapping(&quot;/user/j4&quot;)public String json4() throws JsonProcessingException &#123; List&lt;Object&gt; list = new ArrayList&lt;&gt;(); User user = new User(1, &quot;名字&quot;, 2); list.add(user); // 原生日期格式 Date date = new Date(); list.add(date); // 通过java.text.DateFormat的格式化 ObjectMapper objectMapper = new ObjectMapper(); SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); list.add(simpleDateFormat.format(date)); // 通过jackson格式化 ObjectMapper objectMapper2 = new ObjectMapper(); objectMapper2.setDateFormat(simpleDateFormat); String s2 = objectMapper2.writeValueAsString(date); list.add(s2); return JSON.toJSONString(list, &quot;yyyy-MM-dd HH:mm:ss&quot;); // 使用fastjson&#125; （3）使用jackson包返回json字符串到前端后乱码问题 在@RequestMapping注解里面加入produces参数 1@RequestMapping(value = &quot;/user/j1&quot;, produces = &quot;application/json;charset=utf-8&quot;) 在springmvc-config.xml中配置jackson独有的配置（建议使用） 123456789101112131415&lt;!--Jackson乱码解决--&gt;&lt;mvc:annotation-driven&gt; &lt;mvc:message-converters&gt; &lt;bean class=&quot;org.springframework.http.converter.StringHttpMessageConverter&quot;&gt; &lt;constructor-arg value=&quot;UTF-8&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;org.springframework.http.converter.json.MappingJackson2HttpMessageConverter&quot;&gt; &lt;property name=&quot;objectMapper&quot;&gt; &lt;bean class=&quot;org.springframework.http.converter.json.Jackson2ObjectMapperFactoryBean&quot;&gt; &lt;property name=&quot;failOnEmptyBeans&quot; value=&quot;false&quot;/&gt; &lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/mvc:message-converters&gt;&lt;/mvc:annotation-driven&gt; （4）在前端使用json 12345678910111213141516171819202122232425262728&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;script type=&quot;text/javascript&quot;&gt; var user = &#123; name: &quot;yuyr757&quot;, age: 2, sex: &quot;男&quot; &#125;; console.log(user); console.log(&quot;---------将对象解析为json----------&quot;); var value = JSON.stringify(user); console.log(value); console.log(&quot;---------将json解析为对象----------&quot;); var object = JSON.parse(value); console.log(object);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 六、拦截器和过滤器拦截器：拦截器只会拦截访问的控制器方法，如果访问的是jsp、html、css、image、js是不会被拦截的。实现了HandlerInterceptor接口的类就是拦截器。拦截器是AOP思想的一个具体应用。 过滤器：在web.xml中配置的Filter就是过滤器，url_pattern配置了/*会对所有资源进行过滤。 登录拦截示例： 1234567891011121314151617181920212223242526272829303132package org.yuye757.controller;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import javax.servlet.http.HttpSession;@Controllerpublic class LoginController &#123; @RequestMapping(&quot;/login&quot;) public String login(String username, String password, HttpSession session)&#123; session.setAttribute(&quot;username&quot;, username); return &quot;main&quot;; &#125; @RequestMapping(&quot;/goLogin&quot;) public String goLogin()&#123; return &quot;login&quot;; &#125; @RequestMapping(&quot;/main&quot;) public String main()&#123; return &quot;main&quot;; &#125; @RequestMapping(&quot;/logout&quot;) public String logout(HttpSession session)&#123; session.removeAttribute(&quot;username&quot;); return &quot;redirect:/main&quot;; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637package org.yuye757.interceptor;import org.springframework.web.servlet.HandlerInterceptor;import org.springframework.web.servlet.ModelAndView;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import javax.servlet.http.HttpSession;public class LoginInterceptor implements HandlerInterceptor &#123; // return true即放行，return false则阻断 @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; HttpSession session = request.getSession(); System.out.println(request.getRequestURI()); if (request.getRequestURI().contains(&quot;Login&quot;) || request.getRequestURI().contains(&quot;login&quot;))&#123; return true; &#125; if (session.getAttribute(&quot;username&quot;) != null)&#123; return true; &#125; request.getRequestDispatcher(&quot;/WEB-INF/jsp/login.jsp&quot;).forward(request, response); return false; &#125; // 下面两个不会返回值，一般用于其他处理，如日志，或者直接删掉也可以 @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; &#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; &#125;&#125; 123456&lt;mvc:interceptors&gt; &lt;mvc:interceptor&gt; &lt;mvc:mapping path=&quot;/**&quot;/&gt; &lt;bean class=&quot;org.yuye757.interceptor.LoginInterceptor&quot;/&gt; &lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt; 七、文件上传和下载123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107package org.yuye757.controller;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.multipart.commons.CommonsMultipartFile;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import javax.servlet.http.HttpSession;import java.io.*;import java.net.URLEncoder;@Controllerpublic class FileUploader &#123; @RequestMapping(&quot;/upload&quot;) public String upload(@RequestParam(&quot;file&quot;) CommonsMultipartFile file, HttpSession session) throws IOException &#123; String filename = file.getOriginalFilename(); if (&quot;&quot;.equals(filename))&#123; return &quot;redirect:/index.jsp&quot;; &#125; System.out.println(&quot;上传文件名：&quot; + filename); // 上传路径保存设置 String path = session.getServletContext().getRealPath(&quot;/WEB-INF/upload&quot;); // 如果路径不存在，则创建一个 File realPath = new File(path); if (!realPath.exists())&#123; realPath.mkdir(); &#125; System.out.println(&quot;上传文件保存地址：&quot; + realPath); // 创建流 InputStream is = file.getInputStream(); OutputStream os = new FileOutputStream(realPath + &quot;\\\\&quot; + filename); // 读写 int len = 0; byte[] buffer = new byte[1024]; while((len = is.read(buffer)) &gt; 0)&#123; os.write(buffer, 0, buffer.length); os.flush(); &#125; os.close(); is.close(); return &quot;redirect:/index.jsp&quot;; &#125; @RequestMapping(&quot;/upload2&quot;) public String upload2(@RequestParam(&quot;file&quot;) CommonsMultipartFile file, HttpSession session) throws IOException &#123; String filename = file.getOriginalFilename(); if (&quot;&quot;.equals(filename))&#123; return &quot;redirect:/index.jsp&quot;; &#125; System.out.println(&quot;上传文件名：&quot; + filename); // 上传路径保存设置 String path = session.getServletContext().getRealPath(&quot;/WEB-INF/upload&quot;); // 如果路径不存在，则创建一个 File realPath = new File(path); if (!realPath.exists())&#123; realPath.mkdir(); &#125; System.out.println(&quot;上传文件保存地址：&quot; + realPath); // 通过CommonsMultipartFile的方法直接写入文件 file.transferTo(new File(path + &quot;/&quot; + filename)); return &quot;redirect:/index.jsp&quot;; &#125; @RequestMapping(&quot;/download1&quot;) public void download1(HttpServletRequest request, HttpServletResponse response, String filename) throws IOException &#123; String path = request.getSession().getServletContext().getRealPath(&quot;/WEB-INF/upload&quot;); response.reset(); // 设置页面不缓存，清空buffer response.setCharacterEncoding(&quot;utf-8&quot;); response.setContentType(&quot;multipart/form-data&quot;); // 设置响应头 response.setHeader(&quot;Content-Disposition&quot;, &quot;attachment;filename=&quot; + URLEncoder.encode(filename, &quot;utf-8&quot;)); File file = new File(path + &quot;\\\\&quot; + filename); System.out.println(&quot;下载文件为：&quot; + file.toString()); // 读取文件流 InputStream is = new FileInputStream(file); // 输出文件流 OutputStream os = response.getOutputStream(); // 读写 int len = 0; byte[] buffer = new byte[1024]; while((len = is.read(buffer)) &gt; 0)&#123; os.write(buffer, 0, buffer.length); os.flush(); &#125; os.close(); is.close(); &#125;&#125;","categories":[{"name":"java","slug":"java","permalink":"https://yury757.github.io/categories/java/"}],"tags":[]},{"title":"springboot-study","slug":"java/springboot/springboot-study","date":"2021-08-23T16:00:00.000Z","updated":"2022-10-07T12:56:34.293Z","comments":true,"path":"java/springboot/springboot-study/","link":"","permalink":"https://yury757.github.io/java/springboot/springboot-study/","excerpt":"","text":"约定大于配置！！ 版本：SpringBoot-2.4.3 一、springboot使用简介（1）pom.xmlpom.xml的依赖都在父工程中spring-boot-dependencies中，我们在引入一些springboot依赖时，可以不需要指定版本，因为父工程中指定了建议的版本。也可以写版本号使用我们自己的版本号。 （2）启动器1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 就是一个个功能的开关，我们要使用某个功能，找到相应的启动器starter打开就可以了。如下官网提供了很多启动器： https://docs.spring.io/spring-boot/docs/2.4.3/reference/html/using-spring-boot.html#using-boot-starter （3）启动类springboot为我们写了一个默认的启动类，该启动类加了@SpringBootApplication注解，这个注解主要由以下三个注解组成。 @SpringBootConfiguration：表示这是一个配置类 @EnableAutoConfiguration：启用自动装配 @ComponentScan：组件扫描，自动注册bean 以上三个注解不是必需的，即我们可以挑选使用我们自己需要的功能，或者使用其他注解来启动应用。如下，我们不启用组件扫描，配置了一个属性，还导入了我们自定义的其他配置类。 1234567891011121314151617package com.example.myapplication;import org.springframework.boot.SpringApplication;import org.springframework.context.annotation.ComponentScanimport org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.Import;@Configuration(proxyBeanMethods = false)@EnableAutoConfiguration@Import(&#123; MyConfig.class, MyAnotherConfig.class &#125;)public class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; （4）运行程序12345# 使用jar包直接运行java -jar target/myapplication-0.0.1-SNAPSHOT.jar# 使用maven运行mvn spring-boot:run （5）自动配置Spring Boot自动配置会尝试根据您添加的jar依赖项自动配置您的Spring应用程序。 例如，如果HSQLDB位于类路径上，并且尚未手动配置任何数据库连接bean，则Spring Boot会自动配置内存数据库。springboot支持的自动配置类如下： https://docs.spring.io/spring-boot/docs/2.4.3/reference/html/appendix-auto-configuration-classes.html 可以在配置文件中加入以下设置查看哪些自动配置生效了，哪些没生效。 1debug: true （6）手动配置当我们不想用某个依赖的自动配置时，我们可以在我们的启动程序中加上exclude=&#123;DataSourceAutoConfiguration.class&#125;来排除，然后在配置文件中写上我们需要的配置。其实不用排除也可以，springboot中的自动配置有默认值，直接在配置文件中写我们需要的配置，会覆盖默认值。springboot自动配置的默认值如下： https://docs.spring.io/spring-boot/docs/2.4.3/reference/html/appendix-application-properties.html 123456import org.springframework.boot.autoconfigure.*;import org.springframework.boot.autoconfigure.jdbc.*;@SpringBootApplication(exclude=&#123;DataSourceAutoConfiguration.class&#125;)public class MyApplication &#123;&#125; 配置文件中可以写的配置属性从哪里来？（重要！！） 打开下面这个文件 1\\org\\springframework\\boot\\spring-boot-autoconfigure\\2.4.3\\spring-boot-autoconfigure-2.4.3.jar!\\META-INF\\spring.factories 都是一个个的xxxAutoConfiguration类，点进去基本都会有以下几个注解： 12345678910111213// 表明这是一个配置类，即在spring中讲的使用配置类来配置bean@Configuration(proxyBeanMethods = false)// 条件：必须加载了某个类，这个自动配置类才会生效@ConditionalOnClass(KafkaTemplate.class)// 使用某个属性类当作自动配置的属性@EnableConfigurationProperties(KafkaProperties.class)// 点进去上面那个xxxProperties.class，可以发现这里面有很多属性// 且有下面这个注解，这个注解的功能就是将我们的配置文件的属性和这个类中的属性绑定（参考下面的“使用yaml给bean注入属性”）@ConfigurationProperties(prefix = &quot;spring.kafka&quot;)public class KafkaProperties&#123;&#125; 即xxxAutoConfiguration类就是一个个的配置类，目的是实例化一个个的bean；而xxxProperties类就是属性类，为示例化bean提供属性值，我们在配置文件中可以写的属性就是xxxProperties类中的属性。 （7）配置顺序（重要！！）springboot可以从很多地方来配置，官方给了一个配置的覆盖顺序，后面配置会覆盖前面的配置： 1234567891011121314151617181920212223242526271、Default properties (specified by setting SpringApplication.setDefaultProperties).2、@PropertySource annotations on your @Configuration classes. Please note that such property sources are not added to the Environment until the application context is being refreshed. This is too late to configure certain properties such as logging.* and spring.main.* which are read before refresh begins.3、（主要！）Config data (such as application.properties files)4、A RandomValuePropertySource that has properties only in random.*.5、OS environment variables.6、Java System properties (System.getProperties()).7、JNDI attributes from java:comp/env.8、ServletContext init parameters.9、ServletConfig init parameters.10、Properties from SPRING_APPLICATION_JSON (inline JSON embedded in an environment variable or system property).11、Command line arguments.12、properties attribute on your tests. Available on @SpringBootTest and the test annotations for testing a particular slice of your application.13、@TestPropertySource annotations on your tests.14、Devtools global settings properties in the $HOME/.config/spring-boot directory when devtools is active. 而第3点配置文件application.yaml可以放置在以下位置，加载顺序也是后面的配置会覆盖前面的配置： 12345671、classpath:/2、classpath:/config/3、file:/4、file:/config/ 多环境配置： 可以在文件名后面加对应环境，来设置对应环境的配置，即application-dev.yaml。然后在application.yaml中配置如下设置来修改配置文件： 123spring: profiles: active: dev 或者不用新建application-dev.yaml配置文件，而是直接在application.yaml中加入三根英文横线---来分隔文档版本，并且加上对应的环境，也可以进行配置切换。如下使用的就是dev环境。其实建议使用额外增加一个配置文件application-dev.yaml的形式。 1234567891011121314151617181920server: port: 8084spring: profiles: active: dev---server: port: 8085spring: profiles: dev---server: port: 8086spring: profiles: test 对于多环境配置的加载顺序，官网也还有一个顺序，也是后面的配置会覆盖前面的配置，如下： 12345671、Application properties packaged inside your jar (application.properties and YAML variants).2、Profile-specific application properties packaged inside your jar (application-&#123;profile&#125;.properties and YAML variants).3、Application properties outside of your packaged jar (application.properties and YAML variants).4、Profile-specific application properties outside of your packaged jar (application-&#123;profile&#125;.properties and YAML variants). 最佳的一种方式就是： 在classpath:/下的application.yaml中配置我们的应用，多环境则配置相应的application-&#123;profiles&#125;.yaml，然后启动项目时在命令行中加上spring.profiles.active参数。 二、YAML配置springboot会读取的数据配置文件只有application.properties、application.yaml这两个，修改了文件名则不会生效。 推荐使用yaml配置文件。 1、YAML语法具体语法见这里：https://www.ruanyifeng.com/blog/2016/07/yaml.html 注意点： YAML语法对空格的要求及其严格，一定要小心。 Key-Value键值对中，key后后面冒号后面一定要加一个空格！！ 缩进为两个空格，表示子属性，不能用tab符号。 其中引用的使用建议使用EL表达式，而不是&amp;、*，因为EL表达式不仅可以因为该文件内的属性，还可以引用springboot给我们设置好的其他属性。还可以使用类似三元运算符。 123number: $&#123;random.uuid&#125;version: $&#123;mysql.version&#125;number2: $&#123;value2:123&#125; # 若value2存在则使用value2的值，否则使用123 2、使用yaml给bean注入属性123456789101112131415person: name: yury757 age: 3 number: 10 happy: false birth: 2020/12/02 maps: k1: v1 k2: v2 lists: - code - music dog: name: 旺财3 age: 2 配置了yaml后可以直接在对应的类上面用以下注解，来给bean注入属性。这个注解的作用是将配置文件中的属性值，映射到这个类的属性上。prefix的值就是配置文件中的某一个Key。 1@ConfigurationProperties(prefix = &quot;person&quot;) 123456789101112@Component@ConfigurationProperties(prefix = &quot;person&quot;)public class Person &#123; private String name; private int age; private Integer number; private boolean happy; private Date birth; private Map&lt;String, Object&gt; maps; private List&lt;Object&gt; lists; private Dog dog;&#125; 加了这个注解后IDEA会有一个红色的提示，pom.xml中加入以下依赖，就不会有了。 123456&lt;!-- 用yaml注入对象属性值的依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 这种方式需要对应的类有 无参构造方法 和 每个属性的setter方法，不然会报错。 3、JSR303校验即在属性上加一些注解可以对注入的值进行校验。 导入依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-validation&lt;/artifactId&gt;&lt;/dependency&gt; 普通值校验 1234567891011121314@Min // 验证 Number 和 String 对象是否大等于指定的值 @Max // 验证 Number 和 String 对象是否小等于指定的值 @DecimalMax //被标注的值必须不大于约束中指定的最大值. 这个约束的参数是一个通过BigDecimal定义的最大值的字符串表示.小数存在精度@DecimalMin //被标注的值必须不小于约束中指定的最小值. 这个约束的参数是一个通过BigDecimal定义的最小值的字符串表示.小数存在精度@Digits //验证 Number 和 String 的构成是否合法 @Digits(integer=,fraction=) // 验证字符串是否是符合指定格式的数字，interger指定整数精度，fraction指定小数精度。@Range(min=, max=) // 校验值的大小是否在给定的范围内（可包含）@Range(min=10000,max=50000,message=&quot;range.bean.wage&quot;)private BigDecimal wage;@Valid // 递归的对关联对象进行校验, 如果关联对象是个集合或者数组,那么对其中的元素进行递归校验,如果是一个map,则对其中的值部分进行校验.(是否进行递归验证)@CreditCardNumber // 信用卡验证@Email // 验证是否是邮件地址，如果为null,不进行验证，算通过验证。@ScriptAssert(lang= ,script=, alias=) // 脚本代码段验证，lang为哪种语言@URL(protocol=,host=, port=,regexp=, flags=) // url验证 空检查 1234@Null // 验证对象是否为null@NotNull // 验证对象是否不为null, 无法查检长度为0的字符串@NotBlank // 检查约束字符串是不是Null还有被Trim的长度是否大于0,只对字符串,且会去掉前后空格.@NotEmpty // 检查约束元素是否为NULL或者是EMPTY. Booelan检查 12@AssertTrue // 验证 Boolean 对象是否为 true @AssertFalse // 验证 Boolean 对象是否为 false 长度检查 12@Size(min=, max=) // 验证对象（Array,Collection,Map,String）长度是否在给定的范围之内 @Length(min=, max=) // 验证string字符串的长度是否在给定的范围指内 日期检查 123@Past // 验证 Date 和 Calendar 对象是否在当前时间之前 @Future // 验证 Date 和 Calendar 对象是否在当前时间之后 @Pattern // 验证 String 对象是否符合正则表达式的规则 三、使用springboot开发web12345678910111213141516171819202122232425262728293031&lt;!-- web启动器--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 用yaml注入对象属性值 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;&lt;!-- 属性校验 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-validation&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- thymeleaf模板引擎 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 系统功能监控、统计相关 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 单元测试 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;&lt;/dependency&gt; 1、静态资源存放位置（1）webjars静态资源（一般不这么用） 从webjars导入的静态资源的访问路径是：classpath + 导入的webjars包在META-INF/resources/后面的路径。 （2）其他静态资源 （从左到右优先级依次降低，即同一个静态资源，会先拿最左边的）： 1&#123;&quot;classpath:/resources/&quot;, &quot;classpath:/static/&quot;, &quot;classpath:/public/&quot;&#125; 配置文件中，以下配置的默认值是/**，即把类路径当作根目录，再去找上面的三个文件夹。若修改了这里，比如改成了/test/**，则要通过/test/这个路径才能找到对应的资源。所以千万不要设置下面这个配置。 123spring mvc static-path-pattern: /** 即我们只要将静态资源放在resources、static、public下的任意一个位置就可以了，按照自己喜好分类放置。 注意template目录和外部的目录是无法直接访问到的。 2、模板引擎（1）thymeleaf pom.xml依赖 若springboot版本没有thymeleaf启动器，则引入以下依赖。 123456789&lt;!-- thymeleaf模板引擎，最好使用3.x --&gt;&lt;dependency&gt; &lt;groupId&gt;org.thymeleaf&lt;/groupId&gt; &lt;artifactId&gt;thymeleaf-spring5&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.thymeleaf.extras&lt;/groupId&gt; &lt;artifactId&gt;thymeleaf-extras-java8time&lt;/artifactId&gt;&lt;/dependency&gt; 如有启动器，则直接引入启动器的依赖即可，引入以下依赖反而报错。 12345&lt;!-- thymeleaf模板引擎 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; 同样按上面的方法找到我们需要在配置文件中需要配置的属性，找到下面这个类 123456789101112131415161718192021@ConfigurationProperties(prefix = &quot;spring.thymeleaf&quot;)public class ThymeleafProperties &#123; private static final Charset DEFAULT_ENCODING = StandardCharsets.UTF_8; public static final String DEFAULT_PREFIX = &quot;classpath:/templates/&quot;; public static final String DEFAULT_SUFFIX = &quot;.html&quot;; private boolean checkTemplate = true; private boolean checkTemplateLocation = true; private String prefix = DEFAULT_PREFIX; private String suffix = DEFAULT_SUFFIX; private String mode = &quot;HTML&quot;; private Charset encoding = DEFAULT_ENCODING; private boolean cache = true; private Integer templateResolverOrder; private String[] viewNames; private String[] excludedViewNames; private boolean enableSpringElCompiler; private boolean renderHiddenMarkersBeforeCheckboxes = false; private boolean enabled = true; private final Servlet servlet = new Servlet(); private final Reactive reactive = new Reactive();&#125; 从上面可以看到thymeleaf使用的前缀是classpath:/templates/，后缀是.html，因此我们要把html文件放到templates目录下，才会生效。 在html中加入thymeleaf的命名空间，就可以使用thymeleaf的语法了。 1&lt;html lang=&quot;en&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt; 一定要了解的thymeleaf语法： Variable Expression：$&#123;&#125; Selection Variable Expression：*&#123;&#125; Message Expression：#&#123;&#125; Link Url Expression：@&#123;&#125; Fragment Expression：~&#123;&#125; 3、自定义扩展webmvc 写一个配置类，要实现WebMvcConfigurer接口，并加上@Configuration注解 想加入一个自定义视图解析器，则自定义一个视图解析器类（视图解析器类要实现接口ViewResolver），然后定一个@bean的方法，返回这个类就注册好了。 想加入一个自定义拦截器，则定义一个拦截器（拦截器要实现接口HandlerInterceptor），然后重写addInterceptors方法，把拦截器注册进去，再配置好需要拦截的url pattern即可。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.yury757.config;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.HandlerInterceptor;import org.springframework.web.servlet.View;import org.springframework.web.servlet.ViewResolver;import org.springframework.web.servlet.config.annotation.InterceptorRegistry;import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.util.Locale;// 自定义视图解析器步骤// 写一个配置类，要实现WebMvcConfigurer接口，并加上@Configuration注解@Configurationpublic class MyMvcConfig implements WebMvcConfigurer &#123; // 往IOC容器中注册一个bean @Bean public MyViewResolver getMyViewResolver()&#123; return new MyViewResolver(); &#125; // 自定义视图解析器 public static class MyViewResolver implements ViewResolver &#123; @Override public View resolveViewName(String viewName, Locale locale) throws Exception &#123; return null; &#125; &#125; // 注册拦截器 @Override public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(new MyInterceptor()) .addPathPatterns(&quot;/**&quot;) .excludePathPatterns(&quot;/*.js&quot;, &quot;/*.css&quot;); &#125; // 自定义拦截器 private static class MyInterceptor implements HandlerInterceptor &#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; System.out.println(request.getRequestURI()); return true; &#125; &#125;&#125; 4、国际化（i18n）sources目录下新建一个i18n目录，再新建以下三个配置文件（这里只能使用.properties，不能使用.yaml）： 123login.propertieslogin_en_US.propertieslogin_zh_CN.properties 会发现这三个被绑定在一起了，然后就可以像正常输入配置文件一样，在对应的语言配置中输入变量及对应语言的内容。 1234567891011121314151617181920# login.propertieslogin.btn=登录login.password=密码login.rememberMe=记住我login.tips=请登录login.username=用户名# login_en_US.propertieslogin.btn=Sign Inlogin.password=passwordlogin.rememberMe=remember melogin.tips=Please Login Inlogin.username=username# login_zh_CN.propertieslogin.btn=登录login.password=密码login.rememberMe=记住我login.tips=请登录login.username=用户名 然后在html中就可以使用这里面的变量，不同的模板引擎有不同的写法，如thymeleaf的写法是： 123456#&#123;login.tips&#125;&lt;div class=&quot;text-center mb-4&quot;&gt; &lt;img class=&quot;mb-4&quot; src=&quot;https://getbootstrap.com/docs/4.0/assets/brand/bootstrap-solid.svg&quot; alt=&quot;&quot; width=&quot;72&quot; height=&quot;72&quot;&gt; &lt;h1 class=&quot;h3 mb-3 font-weight-normal&quot; th:text=&quot;#&#123;login.tips&#125;&quot;&gt;&lt;/h1&gt;&lt;/div&gt; 最后还要在webmvc组件中配置国际化处理器，以及将处理器注册到bean中的配置类，如下： 12345678910111213141516171819202122232425262728package com.yury757.config;import org.springframework.web.servlet.LocaleResolver;import org.thymeleaf.util.StringUtils;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.util.Arrays;import java.util.Locale;// 国际化处理器public class MyLocaleResolver implements LocaleResolver &#123; // 解析请求，精简版，不能直接在工作中使用 @Override public Locale resolveLocale(HttpServletRequest request) &#123; String l = request.getParameter(&quot;l&quot;); Locale locale = Locale.getDefault(); if (!StringUtils.isEmpty(l))&#123; String[] s = l.split(&quot;_&quot;); locale = new Locale(s[0], s[1]); &#125; return locale; &#125; @Override public void setLocale(HttpServletRequest request, HttpServletResponse response, Locale locale) &#123;&#125;&#125; 1234567891011121314151617181920212223package com.yury757.config;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.LocaleResolver;import org.springframework.web.servlet.config.annotation.ViewControllerRegistry;import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;@Configurationpublic class MyWebMvcConfigurer implements WebMvcConfigurer &#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; registry.addViewController(&quot;/index.html&quot;).setViewName(&quot;/index&quot;); registry.addViewController(&quot;/&quot;).setViewName(&quot;/index&quot;); &#125; // 注册bean，使用我们自己写的国际化组件 // 注意！！！！！这里方法名要用和类名一样，且首字母小写 @Bean public LocaleResolver localeResolver()&#123; return new MyLocaleResolver(); &#125;&#125; 四、SpringBoot整合其他组件pom中以spring-boot-starter-开头的就是springboot官方的，以-spring-boot-starter结尾的就是对应组件的公司自己写的。 （1）整合Druid数据源123456789101112&lt;!-- log4j2 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j2&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt;&lt;/dependency&gt;&lt;!-- druid数据源 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.2.5&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver type: com.alibaba.druid.pool.DruidDataSource druid: username: root password: root url: jdbc:mysql://localhost:3306/mybatis?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=true&amp;serverTimezone=UTC # 最大等待时间，配置获取连接等待超时，时间单位都是毫秒ms max-wait: 60000 # 最大值 max-active: 20 #最小值 min-idle: 5 #初始化大小 initial-size: 5 #配置一个连接在池中最小生存的时间 min-evictable-idle-time-millis: 60000 #配置间隔多久才进行一次检测，检测需要关闭的空闲连接 time-between-eviction-runs-millis: 300000 test-on-borrow: false test-on-return: false test-while-idle: true pool-prepared-statements: true #最大PSCache连接 max-pool-prepared-statement-per-connection-size: 20 use-global-data-source-stat: true # 通过connectProperties属性来打开mergeSql功能；慢SQL记录 connection-properties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=500 # 配置监控统计拦截的filters，去掉后监控界面sql无法统计， # wall用于防火墙 filter: stat: enabled: true wall: enabled: true log4j2: enabled: true # 配置StatFilter web-stat-filter: # 默认为false，设置为true启动 enabled: true exclusions: &quot;*.js,*.gif,*.jpg,*.bmp,*.png,*.css,*.ico,/druid/*&quot; # 配置StatViewServlet stat-view-servlet: url-pattern: &quot;/druid/*&quot; # 允许哪些ip login-username: root login-password: root # 禁止哪些ip deny: 192.168.1.102 # 是否可以重置 reset-enable: true # 启用 enabled: true （2）整合mybatis12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.1.4&lt;/version&gt;&lt;/dependency&gt; 123mybatis: type-aliases-package: com.yury757.pojo mapper-locations: classpath:mybatis/mapper/*.xml 12345678910111213141516171819202122package com.yury757.mapper;import com.yury757.pojo.User;import org.apache.ibatis.annotations.Mapper;import org.springframework.stereotype.Repository;import org.springframework.web.bind.annotation.RequestParam;import java.util.List;@Mapper@Repositorypublic interface UserMapper &#123; public List&lt;User&gt; selectList(); public User selectById(@RequestParam(&quot;id&quot;) int id); public int addUser(User user); public int updateUser(User user); public int deleteUser(@RequestParam(&quot;id&quot;) int id);&#125; 五、网站安全这部分内容用过滤器和拦截器也可以做到，只是以下两个框架可以使我们的安全组件更高效更简化。 1、SpringSecurity功能：身份验证（Authentication）和访问控制（Authorization） 很重要的几个类或注解： WebSecurityConfigurerAdapter：想要自定义安全策略，只要继承这个类就可以，重写里面的方法即可 AuthenrcationManagerBuilder：自定义认证策略 @EnableWebSecurity：开启WebSecurity模式 1234567@EnableWebSecuritypublic class MySecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity http) throws Exception &#123; super.configure(http); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.yury757.config;import org.springframework.security.config.annotation.authentication.builders.AuthenticationManagerBuilder;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;import javax.sql.DataSource;@EnableWebSecuritypublic class MySecurityConfig extends WebSecurityConfigurerAdapter &#123; private DataSource dataSource; // 自定义授权规则 @Override protected void configure(HttpSecurity http) throws Exception &#123; // 首页所有人可以访问，功能页只有对应权限的人才能访问 http.authorizeRequests() .antMatchers(&quot;/&quot;).permitAll() .antMatchers(&quot;/level1/**&quot;).hasRole(&quot;vip1&quot;) .antMatchers(&quot;/level2/**&quot;).hasRole(&quot;vip2&quot;) .antMatchers(&quot;/level3/**&quot;).hasRole(&quot;vip3&quot;); // 没有权限，跳到登录页面 http.formLogin().loginPage(&quot;/toLogin&quot;).loginProcessingUrl(&quot;/login&quot;).defaultSuccessUrl(&quot;/index&quot;); // 使用自己的登录页面和自己的登录处理逻辑时，要禁用csrf防护 // 千万不要禁用csrf，及找其他方式处理登录页面 http.csrf().disable(); // 开启注销功能 http.logout(); // 开启“记住我”功能，实际上就是丢了一个“rememberMe”的cookie，默认保存两周 http.rememberMe(); &#125; // 自定义认证规则 // 在spring security5中，需要不能直接使用密码，要加密使用 @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; // 这些数据正常应该从数据库里面读取 auth.inMemoryAuthentication().passwordEncoder(new BCryptPasswordEncoder()) .withUser(&quot;yury757&quot;).password(new BCryptPasswordEncoder().encode(&quot;123456&quot;)).roles(&quot;vip2&quot;, &quot;vip3&quot;) .and() .withUser(&quot;root&quot;).password(new BCryptPasswordEncoder().encode(&quot;123456&quot;)).roles(&quot;vip1&quot;, &quot;vip2&quot;, &quot;vip3&quot;) .and() .withUser(&quot;guest&quot;).password(new BCryptPasswordEncoder().encode(&quot;123456&quot;)).roles(&quot;vip1&quot;); &#125;&#125; 2、Shiro也是一个安全组件。可以脱离web使用。 重要的三个对象： Subject：应用代码直接交互的对象，即外部浏览器或爬虫调用我们服务器api的用户。 SecurityManager：安全管理器，管理所有的subject Realm：连接数据","categories":[{"name":"java","slug":"java","permalink":"https://yury757.github.io/categories/java/"}],"tags":[]},{"title":"spring-study","slug":"java/spring/Spring-study","date":"2021-08-23T16:00:00.000Z","updated":"2022-10-07T12:56:48.880Z","comments":true,"path":"java/spring/Spring-study/","link":"","permalink":"https://yury757.github.io/java/spring/Spring-study/","excerpt":"","text":"代码地址：yury757/SpringStudy (github.com) spring：约定大于配置！ 一、IOC（控制反转）1、什么是IOC之前都是我们手动new一个对象（比如new一个Dao层对象），然后使用这个对象的属性的方法。 而IOC就是不用我们去new这个对象，我们只要定义一些配置，然后把创建对象的工作交给spring框架处理，我们需要使用时直接把对象从IOC容器中取出来即可。 因此IOC（控制反转）的含义就是：spring中，对象创建的权利从我们程序员手动创建控制管理，转变为由spring框架去创建控制管理。 DI（依赖注入）的含义就是：spring框架在创建类的实例时，这个类的所有属性需要私有化，并且设置getter、setter方法，spring框架就可以通过setter方法给对应属性注入值。若没有相应的setter方法，则会报错。 配置如下： 12345678910111213141516&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!-- 使用spring来创建对象，在spring中，这些对象都叫做bean --&gt; &lt;!-- id：唯一id --&gt; &lt;!-- class：需要new的类型 --&gt; &lt;!-- property：对象的属性 --&gt; &lt;!-- property.name：对象属性名 --&gt; &lt;!-- property.value：对象属性设置值 --&gt; &lt;bean id=&quot;hello&quot; class=&quot;org.yuyr757.pojo.Hello&quot;&gt; &lt;property name=&quot;str&quot; value=&quot;spring_value&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 取出对象的代码如下： 12345// 获取spring的上下文对象ApplicationContext context = new ClassPathXmlApplicationContext(&quot;beans.xml&quot;);// 我们的对象都在spring管理，我们要使用的话，直接去spring容器里面取Hello hello = (Hello)context.getBean(&quot;hello&quot;);System.out.println(hello.toString()); 和Mybatis有点像，都是通过配置文件来配置创建对象所需要的东西，然后把创建对象的工作交给框架来做。 只不过Mybatis是根据接口创建对象，这个对象的类型的java代码我们甚至没写过，而且对象里面只有方法（CRUD）。而spring的创建对象只是单纯的根据我们写好的java类去new一个实例，然后我们需要什么就取什么。 2、通过bean创建对象的四种方式123456789101112131415161718192021222324&lt;!-- 无参构造，再调用相应属性的setter方法 --&gt;&lt;bean id=&quot;user1&quot; class=&quot;org.yuyr757.pojo.User&quot;&gt; &lt;property name=&quot;id&quot; value=&quot;1&quot;/&gt; &lt;property name=&quot;name&quot; value=&quot;test_user_name1&quot;/&gt;&lt;/bean&gt;&lt;!-- 有参构造，使用构造方法参数下标 --&gt;&lt;bean id=&quot;user2&quot; class=&quot;org.yuyr757.pojo.User&quot;&gt; &lt;constructor-arg index=&quot;0&quot; value=&quot;2&quot;/&gt; &lt;constructor-arg index=&quot;1&quot; value=&quot;test_user_name2&quot;/&gt;&lt;/bean&gt;&lt;!-- 有参构造，使用构造方法参数类型 --&gt;&lt;!-- 不建议使用 --&gt;&lt;bean id=&quot;user3&quot; class=&quot;org.yuyr757.pojo.User&quot;&gt; &lt;constructor-arg type=&quot;int&quot; value=&quot;3&quot;/&gt; &lt;constructor-arg type=&quot;java.lang.String&quot; value=&quot;test_user_name3&quot;/&gt;&lt;/bean&gt;&lt;!-- 有参构造，使用构造方法参数参数名 --&gt;&lt;bean id=&quot;user4&quot; class=&quot;org.yuyr757.pojo.User&quot;&gt; &lt;constructor-arg name=&quot;id&quot; value=&quot;4&quot;/&gt; &lt;constructor-arg name=&quot;name&quot; value=&quot;test_user_name4&quot;/&gt;&lt;/bean&gt; 3、spring创建对象的时间值得注意的是，bean默认为单例模式。对于单例模式的bean，当程序启动时，spring就会立即给帮我们把对象创建好了，而不是等我们调用getBean时创建的。 12345678910111213141516171819202122232425262728293031323334353637package org.yuyr757.pojo;public class User &#123; private int id; private String name; public User() &#123; System.out.println(&quot;调用了无参构造方法&quot;); &#125; public User(int id, String name) &#123; System.out.println(&quot;调用了有参构造方法&quot;); this.id = id; this.name = name; &#125; public String getName() &#123; System.out.println(&quot;调用了getName方法&quot;); return name; &#125; public void setName(String name) &#123; System.out.println(&quot;调用了setName方法&quot;); this.name = name; &#125; public int getId() &#123; System.out.println(&quot;调用了getId方法&quot;); return id; &#125; public void setId(int id) &#123; System.out.println(&quot;调用了setId方法&quot;); this.id = id; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;id=&quot; + id + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 12345678910111213141516171819202122232425262728293031public class TestHello &#123; public static final ApplicationContext context; static&#123; context = new ClassPathXmlApplicationContext(&quot;beans.xml&quot;); // 在这里打断点调试 // 上面这条语句结束后，就立即打印了下面几句话 // 调用了无参构造方法 // 调用了setId方法 // 调用了setName方法 // 调用了有参构造方法 // 调用了有参构造方法 // 调用了有参构造方法 &#125; @Test public void testUser()&#123; System.out.println(&quot;=============================&quot;); User user1 = (User)context.getBean(&quot;user1&quot;); System.out.println(user1.toString()); System.out.println(&quot;=============================&quot;); User user2 = (User)context.getBean(&quot;user2&quot;); System.out.println(user2.toString()); System.out.println(&quot;=============================&quot;); User user3 = (User)context.getBean(&quot;user3&quot;); System.out.println(user3.toString()); System.out.println(&quot;=============================&quot;); User user4 = (User)context.getBean(&quot;user4&quot;); System.out.println(user4.toString()); System.out.println(&quot;=============================&quot;); &#125;&#125; 4、spring配置文件（1）bean标签 id：唯一id，用于获取到这个对象的id class：需要new的类型，要写全限定类名 property：定义对象的属性的标签 constructor-arg：定义构造函数的标签 name：对象属性名或构造函数的参数名 value：普通值 ref：引用一个bean array：注入一个数组 list：注入一个列表 map：注入一个映射表 set：注入一个集合 null：注入一个null指针 props：注入一个properties对象 scope：作用域 （2）import标签导入其他bean配置文件。 适用于团队开发，不同的人开发的bean不同，最终汇总的时候用一个applicationContext.xml引入各个bean.xml即可。 123&lt;import resource=&quot;beans.xml&quot;/&gt;&lt;import resource=&quot;beans2.xml&quot;/&gt;&lt;import resource=&quot;beans3.xml&quot;/&gt; 5、注入方式（1）普通方式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106package org.yuyr757.pojo;import java.util.*;public class Student &#123; private String name; private Address address; private String[] books; private List&lt;String&gt; hobbies; private Map&lt;String, String&gt; card; private Set&lt;String&gt; games; private String wife; private Properties info; @Override public String toString() &#123; return &quot;Student&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, address=&quot; + address + &quot;, books=&quot; + Arrays.toString(books) + &quot;, hobbies=&quot; + hobbies + &quot;, card=&quot; + card + &quot;, games=&quot; + games + &quot;, info=&quot; + info + &quot;, wife=&#x27;&quot; + wife + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Address getAddress() &#123; return address; &#125; public void setAddress(Address address) &#123; this.address = address; &#125; public String[] getBooks() &#123; return books; &#125; public void setBooks(String[] books) &#123; this.books = books; &#125; public List&lt;String&gt; getHobbies() &#123; return hobbies; &#125; public void setHobbies(List&lt;String&gt; hobbies) &#123; this.hobbies = hobbies; &#125; public Map&lt;String, String&gt; getCard() &#123; return card; &#125; public void setCard(Map&lt;String, String&gt; card) &#123; this.card = card; &#125; public Set&lt;String&gt; getGames() &#123; return games; &#125; public void setGames(Set&lt;String&gt; games) &#123; this.games = games; &#125; public Properties getInfo() &#123; return info; &#125; public void setInfo(Properties info) &#123; this.info = info; &#125; public String getWife() &#123; return wife; &#125; public void setWife(String wife) &#123; this.wife = wife; &#125; public Student() &#123; &#125; public Student(String name, Address address, String[] books, List&lt;String&gt; hobbies, Map&lt;String, String&gt; card, Set&lt;String&gt; games, Properties info, String wife) &#123; this.name = name; this.address = address; this.books = books; this.hobbies = hobbies; this.card = card; this.games = games; this.info = info; this.wife = wife; &#125;&#125; 123456789101112131415161718192021222324252627package org.yuyr757.pojo;public class Address &#123; private String address; @Override public String toString() &#123; return &quot;Address&#123;&quot; + &quot;address=&#x27;&quot; + address + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125; public String getAddress() &#123; return address; &#125; public void setAddress(String address) &#123; this.address = address; &#125; public Address() &#123; &#125; public Address(String address) &#123; this.address = address; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!-- 测试特别复杂类型的注入 --&gt; &lt;bean id=&quot;student&quot; class=&quot;org.yuyr757.pojo.Student&quot;&gt; &lt;!-- 普通值，value --&gt; &lt;property name=&quot;name&quot; value=&quot;yuyr757&quot;/&gt; &lt;!-- bean注入，ref --&gt; &lt;property name=&quot;address&quot; ref=&quot;address&quot;/&gt; &lt;!-- 数组注入，array --&gt; &lt;property name=&quot;books&quot;&gt; &lt;array&gt; &lt;value&gt;红楼梦&lt;/value&gt; &lt;value&gt;水浒传&lt;/value&gt; &lt;value&gt;西游记&lt;/value&gt; &lt;value&gt;三国演义&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;!-- 列表注入，list --&gt; &lt;property name=&quot;hobbies&quot;&gt; &lt;list&gt; &lt;value&gt;听歌&lt;/value&gt; &lt;value&gt;写代码&lt;/value&gt; &lt;value&gt;看电影&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!-- 映射表注入，map --&gt; &lt;property name=&quot;card&quot;&gt; &lt;map&gt; &lt;entry key=&quot;身份证&quot; value=&quot;111111111111111111&quot;/&gt; &lt;entry key=&quot;银行卡&quot; value=&quot;222222222222222222&quot;/&gt; &lt;/map&gt; &lt;/property&gt; &lt;!-- 集合注入，set --&gt; &lt;property name=&quot;games&quot;&gt; &lt;set&gt; &lt;value&gt;魂斗罗&lt;/value&gt; &lt;value&gt;冒险岛&lt;/value&gt; &lt;value&gt;七龙珠&lt;/value&gt; &lt;/set&gt; &lt;/property&gt; &lt;!-- null注入，null --&gt; &lt;property name=&quot;wife&quot;&gt; &lt;null/&gt; &lt;/property&gt; &lt;!-- properties对象注入，props，注意和map的区别 --&gt; &lt;property name=&quot;info&quot;&gt; &lt;props&gt; &lt;prop key=&quot;学号&quot;&gt;U201300001&lt;/prop&gt; &lt;prop key=&quot;性别&quot;&gt;男&lt;/prop&gt; &lt;prop key=&quot;username&quot;&gt;root&lt;/prop&gt; &lt;prop key=&quot;password&quot;&gt;root&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;address&quot; class=&quot;org.yuyr757.pojo.Address&quot;&gt; &lt;property name=&quot;address&quot; value=&quot;我是一个地址&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; （2）拓展方式12345678910111213141516171819&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:c=&quot;http://www.springframework.org/schema/c&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!-- 以上加入这个这个拓展标签 --&gt; &lt;!-- xmlns:p=&quot;http://www.springframework.org/schema/p&quot; --&gt; &lt;!-- 加入这个标签后可以直接在后面使用 p:属性名 来定义属性 --&gt; &lt;bean id=&quot;user5&quot; class=&quot;org.yuyr757.pojo.User&quot; p:id=&quot;5&quot; p:name=&quot;user_test_5&quot;/&gt; &lt;!-- 以上加入这个这个拓展标签 --&gt; &lt;!-- xmlns:p=&quot;http://www.springframework.org/schema/c&quot; --&gt; &lt;!-- 加入这个标签后可以直接在后面使用 c:属性名/下标 来定义构造方法的参数的值 --&gt; &lt;bean id=&quot;user6&quot; class=&quot;org.yuyr757.pojo.User&quot; c:id=&quot;6&quot; c:name=&quot;user_test_6&quot;/&gt; &lt;bean id=&quot;user7&quot; class=&quot;org.yuyr757.pojo.User&quot; c:_0=&quot;7&quot; c:_1=&quot;user_test_7&quot;/&gt;&lt;/beans&gt; 6、bean的作用域（1）singleton Scope（单例，默认）从服务器启动到服务器消灭，全局只创建一个对象。当使用多线程时，多个线程拿到的是同一个对象（注意线程的安全）。 （2）prototype（多例）每次调用getBean方法，都会重新new一个对象。 （3）request、session、application这三个只能在web应用中使用，和servlet中的不同作用域的context差不多。 7、bean的自动装配即spring会在容器中自动寻找创建某个对象的依赖，并装配到这个对象的属性中。有以下方式： （1）在xml中显式配置 byName：在容器中自动查找对应属性的setter方法名中set后面的值相同的bean的id byType：在容器中自动查找对应属性类型相同的bean，使用这种方式，一定要保证相同类型的对象只有一个bean 123456789101112131415161718192021&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;dog&quot; class=&quot;org.yuyr757.pojo.Dog&quot;/&gt; &lt;bean id=&quot;cat&quot; class=&quot;org.yuyr757.pojo.Cat&quot;/&gt;&lt;!-- &lt;bean id=&quot;cat2&quot; class=&quot;org.yuyr757.pojo.Cat&quot;/&gt;--&gt; &lt;!-- byName：在容器中自动查找对应属性的setter方法名中set后面的值相同的bean的id --&gt; &lt;bean id=&quot;people1&quot; class=&quot;org.yuyr757.pojo.People&quot; autowire=&quot;byName&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;这是我的名字1&quot;/&gt; &lt;/bean&gt; &lt;!-- byType：在容器中自动查找对应属性类型相同的bean --&gt; &lt;!-- 当容器中有两个相同类型的对象时，不能使用byType --&gt; &lt;bean id=&quot;people2&quot; class=&quot;org.yuyr757.pojo.People&quot; autowire=&quot;byType&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;这是我的名字2&quot;/&gt; &lt;/bean&gt; &lt;/beans&gt; （2）使用注解要在applicationContext.xml中加入以下支持。 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;context:annotation-config/&gt;&lt;/beans&gt; xml配置和注解的一般结合方式：在xml中配置bean，在注入属性时使用注解。 8、注解（1）普通注解 @Component：作用目标：类。把该类当作一个bean对象，即相当于在配置中加入了一个bean标签，默认单例。 @AutoWired：作用目标：很多，一般用于属性。为该属性通过setter方法注入一个值，相当于在配置文件中加入了一个property标签。先通过byType的方法注入的，当IOC容器中有多个相同类型的对象时，再使用byName的方式来注入，而这个name默认就是根据变量名来的。 @Qualifier：作用目标：很多，一般用于属性。通过byName自动注入，该注解有一个value属性，指定去找对应名字的bean。可以和@AutoWired配合使用。 @Resources：作用目标：很多，一般用于属性。相当于以上两个注解的结合。有一个name属性，用于匹配bean的名字，不填。 @value：作用目标：很多，一般用于属性。为该属性通过setter方法注入一个普通的确定的值。 （2）衍生注解 Repository：这是Component注解的别名，用于表示这个类是Dao层的类 Service：这是Component注解的别名，用于表示这个类是Service层的类 Controller：这是Component注解的别名，用于表示这个类是Controller层的类 Scope：作用域，singleton、prototype等 9、使用java类来配置spring即不需要再xml中配置spring，而是在一个SpringConfig类中配置。 @configuration：在类上加入这个注解，则表明这是一个配置类。 @Bean：在方法的的上面加这个注解，相当于xml中的一个bean标签，方法的名字就是bean标签的id，方法的返回值就是bean标签的class属性。 @Import：引入其他配置类 123456789101112131415161718192021222324252627package org.yuyr757.config;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.Import;import org.yuyr757.pojo.Address;import org.yuyr757.pojo.User;@Configuration@Import(MyConfig2.class)public class MyConfig &#123; @Bean public Address address()&#123; return new Address(&quot;1&quot;); &#125; @Bean public Address address2()&#123; return new Address(&quot;2&quot;); &#125; @Bean public User getUser()&#123; return new User(); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344package org.yuyr757.pojo;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Value;public class User &#123; @Value(&quot;yuyr757&quot;) private String name; @Autowired private Address address; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, address=&quot; + address + &#x27;&#125;&#x27;; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Address getAddress() &#123; return address; &#125; public void setAddress(Address address) &#123; this.address = address; &#125; public User() &#123; &#125; public User(String name, Address address) &#123; this.name = name; this.address = address; &#125;&#125; 二、AOPAOP：Aspect-Oriented Programming，面向切面编程 1、代理模式（1）静态代理角色： 抽象角色：一般使用接口或抽象类来解决 真实角色：被代理的角色 代理角色：代理真是角色，并做一些附属操作 客户：访问代理角色的人 以租房为例。 很多客户（租户）直接找到真实角色（房东），完成一个操作（租房）。但是要完成这个操作要很多其他繁琐的事情（比如每次都要带客户看房、签合同等），真实角色（房东）不想做这些他认为没有意义的操作。于是真实角色（房东）找到一个同样可以做这个操作（租房）的代理角色（中介），让代理角色（中介）去和客户（租户）完成看房、签合同等其他操作，真实角色（房东）就可以专心做租房这一个操作了。抽象角色指一类人，这类人都可以完成租房这个操作。在这里真实角色（房东）和代理角色（中介）都是同一类抽象角色。 注意在代码中理解四类角色。 示例一 12345package org.yuyr757.Demo1;// 抽象角色public interface Rent &#123; public void rent();&#125; 12345678910111213141516package org.yuyr757.Demo1;// 客户public class Client &#123; public static void main(String[] args) &#123; // 直接找房东租房 Host host = new Host(); host.rent(); System.out.println(&quot;======================&quot;); // 找中介租房 // 通过代理可以做一些附属操作 Proxy proxy = new Proxy(host); proxy.rent(); &#125;&#125; 1234567package org.yuyr757.Demo1;// 房东public class Host implements Rent&#123; public void rent() &#123; System.out.println(&quot;房东要出租房子&quot;); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334package org.yuyr757.Demo1;// 中介public class Proxy implements Rent &#123; private Host host; public Host getHost() &#123; return host; &#125; public void setHost(Host host) &#123; this.host = host; &#125; public Proxy() &#123; &#125; public Proxy(Host host) &#123; this.host = host; &#125; public void seeHouse()&#123; System.out.println(&quot;中介带看房&quot;); &#125; public void payFee()&#123; System.out.println(&quot;收取中介费&quot;); &#125; public void rent() &#123; this.seeHouse(); host.rent(); this.payFee(); &#125;&#125; 示例二： 12345678package org.yuyr757.Demo2;public interface UserService &#123; public void add(); public void delete(); public void update(); public void query();&#125; 12345678910111213141516171819package org.yuyr757.Demo2;public class UserServiceImpl implements UserService&#123; public void add() &#123; System.out.println(&quot;增加了一个用户&quot;); &#125; public void delete() &#123; System.out.println(&quot;删除了一个用户&quot;); &#125; public void update() &#123; System.out.println(&quot;修改了一个用户&quot;); &#125; public void query() &#123; System.out.println(&quot;查询了一个用户&quot;); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344package org.yuyr757.Demo2;public class UserServiceProxy implements UserService&#123; private UserService userService; public UserServiceProxy() &#123; &#125; public UserServiceProxy(UserService userService) &#123; this.userService = userService; &#125; public UserService getUserService() &#123; return userService; &#125; public void setUserService(UserService userService) &#123; this.userService = userService; &#125; public void add() &#123; log(&quot;使用了add&quot;); this.userService.add(); &#125; public void delete() &#123; log(&quot;使用了delete&quot;); this.userService.delete(); &#125; public void update() &#123; log(&quot;使用了update&quot;); this.userService.update(); &#125; public void query() &#123; log(&quot;使用了query&quot;); this.userService.query(); &#125; public void log(String message)&#123; System.out.println(message); &#125;&#125; 123456789package org.yuyr757.Demo2;public class Client &#123; public static void main(String[] args) &#123; UserServiceImpl userService = new UserServiceImpl(); UserServiceProxy userServiceProxy = new UserServiceProxy(userService); userServiceProxy.add(); &#125;&#125; 优点： 可以使真实角色专注他自己的业务，其他业务交给其他角色来做，实现了分工 有良好的扩展性，可以在不修改其他功能的基础上新增其他功能 缺点：一个真实角色就要产生一个代理角色，代码量会翻倍。有没有一种方法可以避免写这么多代理类，或者自动生成代理类。这就是动态代理。 （2）动态代理（十分重要！） 动态代理和静态代理的角色一样。 动态代理的代理角色（代理类）是动态生成的，不是我们自己写的。 动态代理有两类： 基于接口——JDK动态代理 基于类——cglib java字节码实现——javassist 需要了解两个类/接口： InvocationHandler：调用处理程序 Proxy：代理类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public interface InvocationHandler &#123; /** * Processes a method invocation on a proxy instance and returns * the result. This method will be invoked on an invocation handler * when a method is invoked on a proxy instance that it is * associated with. * * @param proxy （代理实例，即生成的代理对象，要用该对象去调用某个方法）the proxy instance that the method was invoked on * * @param method （在代理实例上调用的接口方法的实例，即要调用的方法）the &#123;@code Method&#125; instance corresponding to * the interface method invoked on the proxy instance. The declaring * class of the &#123;@code Method&#125; object will be the interface that * the method was declared in, which may be a superinterface of the * proxy interface that the proxy class inherits the method through. * * @param args （参数数组）an array of objects containing the values of the * arguments passed in the method invocation on the proxy instance, * or &#123;@code null&#125; if interface method takes no arguments. * Arguments of primitive types are wrapped in instances of the * appropriate primitive wrapper class, such as * &#123;@code java.lang.Integer&#125; or &#123;@code java.lang.Boolean&#125;. * * @return the value to return from the method invocation on the * proxy instance. If the declared return type of the interface * method is a primitive type, then the value returned by * this method must be an instance of the corresponding primitive * wrapper class; otherwise, it must be a type assignable to the * declared return type. If the value returned by this method is * &#123;@code null&#125; and the interface method&#x27;s return type is * primitive, then a &#123;@code NullPointerException&#125; will be * thrown by the method invocation on the proxy instance. If the * value returned by this method is otherwise not compatible with * the interface method&#x27;s declared return type as described above, * a &#123;@code ClassCastException&#125; will be thrown by the method * invocation on the proxy instance. * * @throws Throwable the exception to throw from the method * invocation on the proxy instance. The exception&#x27;s type must be * assignable either to any of the exception types declared in the * &#123;@code throws&#125; clause of the interface method or to the * unchecked exception types &#123;@code java.lang.RuntimeException&#125; * or &#123;@code java.lang.Error&#125;. If a checked exception is * thrown by this method that is not assignable to any of the * exception types declared in the &#123;@code throws&#125; clause of * the interface method, then an * &#123;@link UndeclaredThrowableException&#125; containing the * exception that was thrown by this method will be thrown by the * method invocation on the proxy instance. * * @see UndeclaredThrowableException */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768// 这个类提供了创建动态代理类和实例的方法，这个方法是静态的，通过这些方法创建的类都继承了Proxy这个类public class Proxy implements java.io.Serializable&#123; /* -------------------属性------------------- */ // final的属性基本都不用管，因为我们无法做修改 private static final long serialVersionUID = -2222568056686623797L; private static final Class&lt;?&gt;[] constructorParams = &#123; InvocationHandler.class &#125;; private static final WeakCache&lt;ClassLoader, Class&lt;?&gt;[], Class&lt;?&gt;&gt; proxyClassCache = new WeakCache&lt;&gt;(new KeyFactory(), new ProxyClassFactory()); private static final Object key0 = new Object(); // 这个是我们需要传入的对象，就是上面说的那个接口类 protected InvocationHandler h; /* -------------------构造方法------------------- */ private Proxy() &#123;&#125;; protected Proxy(InvocationHandler h); /* -------------------静态方法------------------- */ // 生成代理类，会调用getProxyClass0方法 @CallerSensitive public static Class&lt;?&gt; getProxyClass(ClassLoader loader, Class&lt;?&gt;... interfaces); // 检查代理权限 private static void checkProxyAccess(Class&lt;?&gt; caller, ClassLoader loader, Class&lt;?&gt;... interfaces); // 重要！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！ // 从类加载器中生成一个代理类，文档说生成代理类之前必须检查代理权限（checkProxyAccess） private static Class&lt;?&gt; getProxyClass0(ClassLoader loader, Class&lt;?&gt;... interfaces); // 重要！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！ // 生成一个代理类的实例 // loader：真实对象（目标对象）的类加载器 // interfaces：真实对象（目标对象）的接口组成的数组 // h：实现了上面那个InvocationHandler接口的对象 @CallerSensitive public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h); // 检查caller类和proxyclass类是否在同一个包内，如果不在同一个包内，再检查相关权限 private static void checkNewProxyPermission(Class&lt;?&gt; caller, Class&lt;?&gt; proxyClass); // 判断一个类是否是一个由Proxy类生成的代理类 public static boolean isProxyClass(Class&lt;?&gt; cl); // 传入一个对象参数，取出这个对象中的InvocationHandler属性，就是上面的h public static InvocationHandler getInvocationHandler(Object proxy); // native，非java实现，不用管 private static native Class&lt;?&gt; defineClass0(ClassLoader loader, String name, byte[] b, int off, int len); /* -------------------内部private类------------------- */ private static final class Key1 extends WeakReference&lt;Class&lt;?&gt;&gt;&#123;&#125; private static final class Key2 extends WeakReference&lt;Class&lt;?&gt;&gt;&#123;&#125; private static final class KeyX&#123;&#125; private static final class KeyFactory implements BiFunction&lt;ClassLoader, Class&lt;?&gt;[], Object&gt;&#123;&#125; // 创建代理类的工厂，如果工厂中有相应接口的代理类的缓存，则会返回一个代理类的复制，否则会重新创建一个代理类 private static final class ProxyClassFactory implements BiFunction&lt;ClassLoader, Class&lt;?&gt;[], Class&lt;?&gt;&gt;&#123;&#125;&#125; 动态代理的本质：JVM在运行时动态创建class字节码并加载的过程。要实现的接口和调用接口方法的handler，可以生成一个class字节码，然后由对应的类加载器加载calss字节码，就可以在内存中生成一个类对象（代理类）了。当代理对象调用对应方法时，handler会将方法转发给自己的invoke方法。于是我们就可以在invoke方法中加入增强方法的代码。 用动态代理来增强示例二如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package org.yuyr757.DynamicProxy2;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;public class ProxyInvocationHandler implements InvocationHandler &#123; private Object target; /** * 设置真实对象，相当于代理对象要和真实对象签定委托代理的契约，代理对象才可以有权限对做真实对象才能做的事情 * 就像现实生活中，租房中介要拿到房东的授权委托书，中介才可以有权去代理房东租房 */ public void setTarget(Object target) &#123; this.target = target; &#125; public ProxyInvocationHandler() &#123; &#125; public ProxyInvocationHandler(Object target) &#123; this.target = target; &#125; /** * 生成一个代理对象，即生成一个租房中介 * @loader 这个中介的类加载器和房东是一样的，即他们的级别是一样的 * @interfaces 这个中介的接口和房东是一样的，即他们都应该有相同的动作 * @h 这个中介拿到一个InvocationHandler对象，即每当客户有租房的动作时，租房的动作会通过InvocationHandler转发到自己的invoke方法，代理对象就可以在invoker方法中做一些额外操作 * 此外newProxyInstance方法内部还会授予相关代理权限，不然任何一个没有权限的人都可以代理房东去租房 */ public Object getProxy()&#123; return Proxy.newProxyInstance( this.target.getClass().getClassLoader(), this.target.getClass().getInterfaces(), this ); &#125; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; this.log(method.getName()); Object returnObject = method.invoke(target, args); return returnObject; &#125; public void log(String msg)&#123; System.out.println(&quot;调用了&quot; + msg + &quot;方法&quot;); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package org.yuyr757.DynamicProxy2;import org.yuyr757.Demo2.UserService;import org.yuyr757.Demo2.UserServiceImpl;import java.lang.reflect.Field;import java.lang.reflect.Method;import java.util.Arrays;public class Client &#123; public static void main(String[] args) &#123; UserService userservice = new UserServiceImpl(); ProxyInvocationHandler handler = new ProxyInvocationHandler(userservice); UserService proxy = (UserService)handler.getProxy(); System.out.println(&quot;=============查看生成的类的具体信息=============&quot;); Class proxyClass = proxy.getClass(); System.out.println(&quot;类名称：&quot; + proxyClass.getName()); System.out.println(&quot;类加载器：&quot; + proxyClass.getClassLoader()); System.out.println(&quot;类接口：&quot; + Arrays.toString(proxyClass.getInterfaces())); System.out.println(&quot;类的父类：&quot; + proxyClass.getSuperclass()); System.out.println(&quot;public方法和父类的public方法：&quot;); for (Method method : proxyClass.getMethods()) &#123; System.out.println(method.toString()); &#125; System.out.println(&quot;所有本类的方法：&quot;); for (Method method : proxyClass.getDeclaredMethods()) &#123; System.out.println(method.toString()); &#125; System.out.println(&quot;所有本类的属性：&quot; ); for (Field declaredField : proxyClass.getDeclaredFields()) &#123; System.out.println(declaredField); &#125; System.out.println(&quot;=============测试方法=============&quot;); proxy.add(); proxy.delete(); proxy.update(); proxy.query(); System.out.println(&quot;=============代理其他实现了该接口的类只需要修改目标对象即可=============&quot;); handler.setTarget(new UserServiceImpl2()); UserService proxy2 = (UserService)handler.getProxy(); proxy2.add(); proxy2.delete(); proxy2.update(); proxy2.query(); &#125;&#125; 动态代理的优点：动态代理代理的是接口，一般是一类业务，所有实现了该接口的类都可以被代理，减少了代码量。 缺点：效率稍微低一些。因为要在运行时根据接口动态生成字节码，再重新加载字节码。 2、spring-aop几个重要概念： 切入点（pointcut）：即我们原有的功能或业务逻辑，一堆方法的集合 切面（aspect）：一个切入到我们原有功能里面的新功能的集合（模块），一个类 通知（advisor）：切面要完成的工作，即类中的方法 连接点（joinpoint）：在切入点具体执行的某一个方法 通知的五种类型： 前置通知（Before advice）：即在目标方法执行前加一个增强方法 正常返回通知（After returning advice）：在连接点正常执行完成后执行，如果连接点抛出异常，则不会执行。 异常返回通知（After throwing advice）：在连接点抛出异常后执行。 返回通知（After (finally) advice）：在连接点执行完成后执行，不管是正常执行完成，还是抛出异常，都会执行返回通知中的内容。 环绕通知（Around advice）：即把目标方法包裹在该通知方法内，比如我们使用IDEA有一个快捷键是ctrl + alt + T，即用一个方法把目标方法包裹（surrounding）起来。使用这种通知时，要把目标方法传入我们的通知方法中。最强的通知类型，完全可以使用这一个通知，然后在通知方法里面定义具体的实现，来满足以上四个通知（下面有示例）。前四个通知和环绕通知最好不要一起使用， 即要么使用前四个通知，要么只使用环绕通知。若都使用了，比如同时使用了前置通知和环绕通知，则目标方法执行前的增强方法好像是根据xml配置的顺序决定的。反正别一起使用就对了。【！！！推荐使用环绕通知，自己写增强方法，因为在有返回值和报异常同时存在的情况下，环绕通知有更强的实现，以上四个通知都做不到】 （1）方式一：使用spring aop接口实现配置文件中不需要aspect，因为spring会去找实现了相应接口的类当作aspect，需要配置pointcut和advisor。 1234567891011&lt;!-- 配置AOP --&gt;&lt;!-- 方式一：使用原生spring aop的api接口 --&gt;&lt;aop:config&gt; &lt;!-- 配置切入点，即需要增强的目方法 --&gt; &lt;!-- expression：表达式，execution(要执行的位置，修饰词 返回值 列名 方法名 参数) --&gt; &lt;aop:pointcut id=&quot;pointcut&quot; expression=&quot;execution(* org.yuyr757.services.UserServiceImpl.*(..))&quot;/&gt; &lt;!-- 执行环绕增加 --&gt; &lt;aop:advisor advice-ref=&quot;logBefore&quot; pointcut-ref=&quot;pointcut&quot;/&gt; &lt;aop:advisor advice-ref=&quot;logAfter&quot; pointcut-ref=&quot;pointcut&quot;/&gt;&lt;/aop:config&gt; （2）方式二：自定义类+xml配置（建议使用）要自定义类当作aspect，配置文件中定义aspect标签引用自定义类，再配置pointcut、before、after等。 123456789&lt;!-- 方式二：使用自定义类来实现aop --&gt;&lt;aop:config&gt;&lt;!-- 需要配置切面 --&gt; &lt;aop:aspect ref=&quot;diyPoint&quot;&gt; &lt;aop:pointcut id=&quot;point&quot; expression=&quot;execution(* org.yuyr757.services.UserServiceImpl.*(..))&quot;/&gt; &lt;aop:before method=&quot;before&quot; pointcut-ref=&quot;point&quot;/&gt; &lt;aop:after method=&quot;after&quot; pointcut-ref=&quot;point&quot;/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; （3）方式三：使用自定义类+注解123456789101112131415161718package org.yuyr757.DiyAop;import org.aspectj.lang.annotation.After;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;@Aspectpublic class DiyPointCut2 &#123; @Before(&quot;execution(* org.yuyr757.services.UserServiceImpl.*(..))&quot;) public void before()&#123; System.out.println(&quot;=========方法执行前2=========&quot;); &#125; @After(&quot;execution(* org.yuyr757.services.UserServiceImpl.*(..))&quot;) public void after()&#123; System.out.println(&quot;=========方法执行后2=========&quot;); &#125;&#125; 12345&lt;!-- 方式三：使用自定义+注解 --&gt;&lt;aop:aspectj-autoproxy/&gt;&lt;!-- proxy-target-class默认为false，false代表使用jdk自己的动态代理实现，true代表使用cglib实现，一般用false即可 --&gt;&lt;aop:aspectj-autoproxy proxy-target-class=&quot;false&quot;/&gt; （4）示例1234567891011121314151617181920212223242526272829303132333435package org.yuyr757.DiyAop;import org.aspectj.lang.ProceedingJoinPoint;public class DiyPointCut &#123; public void before()&#123; System.out.println(&quot;=========before通知=========&quot;); &#125; public void after()&#123; System.out.println(&quot;=========after通知=========&quot;); &#125; public void afterReturning()&#123; System.out.println(&quot;=========afterReturning通知=========&quot;); &#125; public void afterThrowing()&#123; System.out.println(&quot;=========afterThrowing通知=========&quot;); &#125; public Object around(ProceedingJoinPoint joinPoint) throws Throwable &#123; Object object = 0; try&#123; System.out.println(&quot;=========around通知前=========&quot;); object = joinPoint.proceed(); System.out.println(&quot;=========around通知-afterReturning=========&quot;); &#125;catch (IllegalArgumentException e)&#123; System.out.println(&quot;=========around通知-afterThrowing=========&quot;); &#125;finally &#123; System.out.println(&quot;=========around通知后=========&quot;); &#125; return object; &#125;&#125; 1234567891011 &lt;aop:config&gt; &lt;!-- 需要配置切面 --&gt; &lt;aop:aspect ref=&quot;diyPoint&quot;&gt; &lt;aop:pointcut id=&quot;point&quot; expression=&quot;execution(* org.yuyr757.services.UserServiceImpl.*(..))&quot;/&gt;&lt;!-- &lt;aop:before method=&quot;before&quot; pointcut-ref=&quot;point&quot;/&gt;--&gt;&lt;!-- &lt;aop:after-returning method=&quot;afterReturning&quot; pointcut-ref=&quot;point&quot;/&gt;--&gt;&lt;!-- &lt;aop:after-throwing method=&quot;afterThrowing&quot; pointcut-ref=&quot;point&quot;/&gt;--&gt; &lt;aop:around method=&quot;around&quot; pointcut-ref=&quot;point&quot;/&gt;&lt;!-- &lt;aop:after method=&quot;after&quot; pointcut-ref=&quot;point&quot;/&gt;--&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt; 123456789101112131415161718192021package org.yuyr757.services;public class UserServiceImpl implements UserService&#123; public int add(int num) &#123; if (num &lt; 50) throw new IllegalArgumentException(); System.out.println(&quot;增加了一个用户&quot;); return num; &#125; public void delete() &#123; System.out.println(&quot;删除了一个用户&quot;); &#125; public void update() &#123; System.out.println(&quot;修改了一个用户&quot;); &#125; public void select() &#123; System.out.println(&quot;查询了一个用户&quot;); &#125;&#125; 12345678package org.yuyr757.services;public interface UserService &#123; public int add(int num); public void delete(); public void update(); public void select();&#125; 1234567891011121314151617181920212223242526272829import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import org.yuyr757.services.UserService;public class TestAop1 &#123; public static final ApplicationContext context; static&#123; context = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); &#125; @Test public void testAopAroundReturning()&#123; System.out.println(&quot;【开始测试正常返回时的通知结果】&quot;); UserService userService = context.getBean(&quot;userServiceImpl&quot;, UserService.class); int num = userService.add(188); System.out.println(num); &#125; @Test public void testAopAroundThrowing()&#123; System.out.println(&quot;【开始测试报异常时的通知结果】&quot;); UserService userService = context.getBean(&quot;userServiceImpl&quot;, UserService.class); int num = userService.add(1); System.out.println(num); &#125;&#125; 三、整合spring和mybatis需要用到mybatis-spring这个包。 此外对于每个mapper接口我们必须手动写一个实现类，然后注册到spring的bean中，实现类使用的SqlSession要使用SqlSessionTemplate。其他的都和mybatis一样配置 12345678910111213141516171819202122232425262728293031&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 使用spring数据源代替mybatis数据源 --&gt; &lt;bean id=&quot;datasource&quot; class=&quot;org.springframework.jdbc.datasource.DriverManagerDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/mybatis?useUnicode=true&amp;amp;characterEncoding=utf-8&amp;amp;useSSL=true&amp;amp;serverTimezone=UTC&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt; &lt;/bean&gt; &lt;!-- sqlSessionFactory --&gt; &lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;datasource&quot;/&gt; &lt;!-- 绑定mybatis配置文件 --&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:mybatis-config.xml&quot;/&gt; &lt;!-- 一般这里只需要绑定配置文件就可以，mybatis的配置还是在mybatis-config.xml中配置 --&gt; &lt;/bean&gt; &lt;!-- 这个sqlSessionTemplate是SqlSession的一个实现类，我们以后就不用手动调用openSession()方法来获得SqlSession对象 --&gt; &lt;bean id=&quot;sqlSessionTemplate&quot; class=&quot;org.mybatis.spring.SqlSessionTemplate&quot;&gt; &lt;!-- 因为这个类没有setter方法，所以只能通过有参构造方法来初始化 --&gt; &lt;constructor-arg index=&quot;0&quot; ref=&quot;sqlSessionFactory&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 四、spring的事务管理1、编程式事务管理手动在程序中写try catch来实现事务就是编程式事务管理。 2、声明式事务管理把事务交给IOC容器管理就是声明式事务管理。配置如下： 12345678910111213141516171819202122&lt;!-- 声明式事务管理 --&gt;&lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;datasource&quot;/&gt;&lt;/bean&gt;&lt;!-- 结合AOP实现事务织入 --&gt;&lt;!-- 配置事务通知的的类：需要导入tx命名空间 --&gt;&lt;tx:advice id=&quot;txAdvisor&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;!-- name：给哪些方法配置事务，propagation：配置事务的传播特性 --&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;add*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;delete*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;update*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;select*&quot; read-only=&quot;true&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt;&lt;!-- 配置事务的切入点 --&gt;&lt;aop:config&gt; &lt;aop:pointcut id=&quot;txPointCut&quot; expression=&quot;execution(* org.yuyr757.mapper.*.*(..))&quot;/&gt; &lt;aop:advisor advice-ref=&quot;txAdvisor&quot; pointcut-ref=&quot;txPointCut&quot;/&gt;&lt;/aop:config&gt;","categories":[{"name":"java","slug":"java","permalink":"https://yury757.github.io/categories/java/"}],"tags":[]},{"title":"Java_JUC-study","slug":"java/java_JUC/Java_JUC-study","date":"2021-08-23T16:00:00.000Z","updated":"2022-10-07T12:55:10.847Z","comments":true,"path":"java/java_JUC/Java_JUC-study/","link":"","permalink":"https://yury757.github.io/java/java_JUC/Java_JUC-study/","excerpt":"","text":"一、基础1、进程和线程进程（Process）是计算机中的程序关于某数据集合上的一次运行活动，是系统进行资源分配和调度的基本单位，是操作系统结构的基础。程序是指令、数据及其组织形式的描述，进程是程序的实体。狭义地说，进程就是一个正在运行的程序。 线程（Thread）是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。 2、并行和并发并行：一组程序按独立异步的速度执行，无论从微观还是宏观，程序都是一起执行的。即CPU可以同时在两个线程上工作。 并发：在同一个时间段内，两个或多个程序执行，宏观上来看是一起执行的，但微观上来看是交替执行的。比如CPU在线程1上工作一段时间，又切换到线程2上工作一段时间，因此宏观上来看两个程序都执行了。 知乎有一个举例说得好： 你吃饭吃到一半，电话来了，你一直到吃完了以后才去接，这就说明你不支持并发也不支持并行。 你吃饭吃到一半，电话来了，你停了下来接了电话，接完后继续吃饭，这说明你支持并发。 你吃饭吃到一半，电话来了，你一边打电话一边吃饭，这说明你支持并行。 并发的关键是你有处理多个任务的能力，不一定要同时。并行的关键是你有同时处理多个任务的能力。区分它们最关键的点就是：是否是同时。因此，单核CPU是无法实现并行的，只能通过CPU调度实现并发。而多核CPU才能实现并行。 3、线程的状态1234567891011121314public enum State &#123; // 就绪 NEW, // 运行 RUNNABLE, // 阻塞 BLOCKED, // 等待 WAITING, // 有一个特定时间的等待状态 TIMED_WAITING, // 终止 TERMINATED;&#125; 4、java有权限开线程吗？没有。java的thread.start方法本质调用的是一个native方法，该方法调用底层C++方法来实现开线程。 5、wait和sleep的区别（1）来自不同的类，wait来此Object，而sleep来自Thread； （2）wait会释放锁，而sleep不会释放锁； （3）wait只有在synchronized代码块下使用，而sleep可以在任何地方使用； 二、锁 传统使用synchronized来给对象加锁 12345678class Ticket&#123; private int number = 50; public synchronized void sale()&#123; if (this.number &gt; 0)&#123; System.out.println(Thread.currentThread().getName() + &quot;卖出了第&quot; + (number--) + &quot;张表，剩余&quot; + number + &quot;张票&quot;); &#125; &#125;&#125; 使用Lock接口来实现 1、Lock接口123public class ReentrantLock implements Lock, java.io.Serializable &#123;&#125;public static class WriteLock implements Lock, java.io.Serializable &#123;&#125;public static class WriteLock implements Lock, java.io.Serializable &#123;&#125; （1）ReentrantLock（可重入锁，常用）含义：可以多次获取同一个锁，但是释放也要多次释放。 构造方法： 12public ReentrantLock() &#123; sync = new NonfairSync(); &#125;public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync(); &#125; 有两种锁： 公平锁：对线程公平对待，必须先来后到； 非公平锁：线程竞争锁时不管先来后到，让他们蜂拥上去抢。（默认） 123456789101112131415161718192021222324252627282930public class Test02SaleTicket2 &#123; @Test public void test01()&#123; Ticket ticket = new Ticket(); new Thread(() -&gt;&#123; for (int i = 0; i &lt; 30; i++) ticket.sale(); &#125;, &quot;A&quot;).start(); new Thread(() -&gt;&#123; for (int i = 0; i &lt; 30; i++) ticket.sale(); &#125;, &quot;B&quot;).start(); new Thread(() -&gt;&#123; for (int i = 0; i &lt; 30; i++) ticket.sale(); &#125;, &quot;C&quot;).start(); &#125;&#125;class Ticket2&#123; private int number = 50; Lock lock = new ReentrantLock(); public void sale()&#123; lock.lock(); try &#123; // 业务代码 if (this.number &gt; 0)&#123; System.out.println(Thread.currentThread().getName() + &quot;卖出了第&quot; + (number--) + &quot;张表，剩余&quot; + number + &quot;张票&quot;); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; lock.unlock(); &#125; &#125;&#125; 使用Lock的代码套路： 先调用lock.lock()方法 再用try包裹起来，里面写业务代码 最后finally要调用lock.unlock()方法 Synchronized和Lock的区别 Synchronized是java关键字，而Lock是一个java类 Synchronized无法获取锁的状态，而Lock可以查看锁的状态 Synchronized会自动释放锁，Lock必须手动解锁，如果出了问题而没有释放，则会死锁。 Synchronized，线程1（获得锁），线程2（阻塞），线程1（阻塞），线程2（永远等下去）；而Lock可以使用tryLock方法看是否可以获取锁，若长时间没有获取到锁，则会返回false，而lock()方法如果一直获取不到锁，也会一直阻塞，这就出现了死锁。因此要避免使用lock()方法，使用tryLock()方法。 Synchronized是非公平的可重入锁，不可以中断，而Lock可以手动设置公平和非公平，且可以判断锁状态 Synchronized适合锁少量的代码块同步问题，而Lock适合锁大量同步代码块 即Synchronized是自动版的Lock，而Lock是纯手动配置。 2、生产者消费者问题生产者消费者问题的套路： 先判断等待 再写业务代码 最后通知 （1）使用Synchronized来写12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package net.yury757;import org.junit.Test;public class Test03ProducerConsumer &#123; @Test public void test01() &#123; Data data = new Data(); new Thread(() -&gt; &#123; for(int i = 0; i &lt; 1000; i++)&#123; try &#123; data.plus(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;A&quot;).start(); new Thread(() -&gt; &#123; for(int i = 0; i &lt; 1000; i++)&#123; try &#123; data.minus(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;B&quot;).start(); &#125;&#125;class Data&#123; private int number = 0; public synchronized void plus() throws InterruptedException &#123; if (number != 0)&#123; // 等待 this.wait(); &#125; number++; System.out.println(Thread.currentThread().getName() + &quot; =&gt; &quot; + number); // 通知其他线程，加法做完了 this.notifyAll(); &#125; public synchronized void minus() throws InterruptedException &#123; if (number == 0)&#123; // 等待 this.wait(); &#125; number--; System.out.println(Thread.currentThread().getName() + &quot; =&gt; &quot; + number); // 通知其他线程，减法做完了 this.notifyAll(); &#125;&#125; （2）虚假唤醒（重要）在上面例子中，只有一个生产者和一个消费者，因此生产者只可能被消费者唤醒，而消费者只可能被生产者唤醒。 但是如果有两个生产者和两个消费者的情况下，一个生产者可能唤醒了另外一个生产者，导致生产者的活动执行了两次。这就是虚假唤醒。 解决办法：使用while，而不是if来判断，即某个线程被唤醒后，还要再判断一次是否是真实的被唤醒情况。 （3）使用Lock来写 Synchronized方式 Lock方式 synchronized Lock类 wait方法 Condition类的await方法 notify方法 Condition类的signal方法 Conditioin类就类似一个对象的监视器，await方法可以使调用这个方法的线程阻塞，signal方法可以唤醒某个被阻塞的线程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package net.yury757;import org.junit.Test;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class Test03ProducerConsumer2 &#123; @Test public void test01() &#123; Data data = new Data(); new Thread(() -&gt; &#123; for(int i = 0; i &lt; 1000; i++)&#123; try &#123; data.plus(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;A&quot;).start(); new Thread(() -&gt; &#123; for(int i = 0; i &lt; 1000; i++)&#123; try &#123; data.minus(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;B&quot;).start(); new Thread(() -&gt; &#123; for(int i = 0; i &lt; 1000; i++)&#123; try &#123; data.plus(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;C&quot;).start(); new Thread(() -&gt; &#123; for(int i = 0; i &lt; 1000; i++)&#123; try &#123; data.minus(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;D&quot;).start(); &#125;&#125;class Data2&#123; private int number = 0; Lock lock = new ReentrantLock(); Condition condition = lock.newCondition(); public void plus() &#123; lock.lock(); try&#123; while (number != 0)&#123; condition.await(); &#125; number++; System.out.println(Thread.currentThread().getName() + &quot; =&gt; &quot; + number); condition.signalAll(); &#125;catch (InterruptedException ex)&#123; ex.printStackTrace(); &#125;finally &#123; lock.unlock(); &#125; &#125; public void minus() throws InterruptedException &#123; lock.lock(); try&#123; while (number == 0)&#123; condition.await(); &#125; number--; System.out.println(Thread.currentThread().getName() + &quot; =&gt; &quot; + number); condition.signalAll(); &#125;catch (InterruptedException ex)&#123; ex.printStackTrace(); &#125;finally&#123; lock.unlock(); &#125; &#125;&#125; 3、（重要！）锁到底锁的是什么 普通方法上加锁，锁的是调用该方法的对象（实例）。 静态方法上加锁，锁的是class对象（模板）。 同一个对象，一个线程调用普通同步方法，另一个线程调用普通方法，普通方法的调用不受锁的影响，因为普通方法调用根本不用去获取锁，因此即使该对象被其他线程锁住了，普通方法也可以成功调用。把同步方法理解成需要在房子里才可以执行，普通方法不用进入房子就可以执行，因此普通方法的调用不会受锁的影响，只有同步方法才会受锁的影响，因为如果房子被其他线程“关门”了，他进不去就只能等待。 同一个对象，一个线程调用静态同步方法，另一个线程调用静态方法，静态方法的调用不受锁的影响，原因同上。 同一个对象，一个线程调用静态同步方法，另一个线程调用普通同步方法，普通同步方法的调用不受锁的影响，因为静态方法锁的是class对象（模板），普通对象（实例）和class对象（模板）不在一起，静态方法在class对象（模板）中，而普通方法在普通对象中。 三、集合类的安全性1、List的安全性12345678910111213141516import java.util.*;import java.util.concurrent.CopyOnWriteArrayList;// 使用ArrayList类，最终打印的长度可能不是10000// 而用CopyOnWriteArrayList类，最终打印的长度就是10000public class Test06UnsafeList &#123; public static void main(String[] args) &#123; List&lt;String&gt; objects = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10000; i++) &#123; new Thread(() -&gt;&#123; objects.add(UUID.randomUUID().toString().substring(0, 5)); System.out.println(objects.size()); &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; ArrayList类是非线程安全的，解决List的安全性的几个解决方案： 使用Vector类，Vector类是线程安全的List实现类，基于synchronized实现 使用Collections.synchronized(new ArrayList&lt;&gt;())，生成的SynchronizedList就是在arraylist外再包了一层同步方法而已。 使用JUC的CopyOnWriteArrayList类，基于ReentrantLock实现，效率比synchronized高。（用这个） 2、Set的安全性12345678910111213141516import java.util.Set;import java.util.UUID;import java.util.concurrent.CopyOnWriteArraySet;public class Test07UnsafeSet &#123; public static void main(String[] args) &#123; Set&lt;String&gt; objects = new CopyOnWriteArraySet&lt;&gt;(); for (int i = 0; i &lt; 10000; i++) &#123; new Thread(() -&gt;&#123; objects.add(UUID.randomUUID().toString().substring(0, 5)); System.out.println(objects.size()); &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; 同样HashSet也是非线程安全的，解决方法： 使用Collections.synchronizedSet(new HashSet&lt;&gt;()) 使用CopyOnWriteArraySet，基于CopyOnWriteArrayList实现，效率比HashSet低（用这个） 3、Map的安全性12345678910111213import java.util.*;public class Test08UnsafeMap &#123; public static void main(String[] args) &#123; Map&lt;String, String&gt; objects = new HashMap&lt;&gt;(); for (int i = 0; i &lt; 10000; i++) &#123; new Thread(() -&gt;&#123; objects.put(UUID.randomUUID().toString().substring(0, 5), UUID.randomUUID().toString().substring(0, 5)); System.out.println(objects.size()); &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; HashMap是非线程安全的，解决方法： 使用Collections.synchronizedMap(new HashMap&lt;&gt;()) 使用ConcurrentHashMap（用这个） 四、Callable（1）Callable和Runnable的区别： Callable可以有返回值 Callable可以抛出异常 调用方法不同，Callable由call()调用，Runnable由run()调用 （2）Callable的使用123456789101112131415161718192021package net.yury757;import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.FutureTask;public class Test09Callable implements Callable&lt;String&gt; &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; FutureTask futureTask = new FutureTask(new Test09Callable()); new Thread(futureTask).start(); new Thread(futureTask).start(); String o = (String)futureTask.get(); System.out.println(&quot;main方法：&quot; + o); &#125; @Override public String call() throws Exception &#123; System.out.println(&quot;方法内部：asdfsdf&quot;); return &quot;asdfsdf&quot;; &#125;&#125; 注意点： 同时使用两次new Thread(futureTask).start();时，只会输出一次 futureTask.get();方法可能会被阻塞，如果call()方法内部是耗时操作的话 五、三大常用辅助类1、CountDownLatch这是一个减法计数器类。构造函数接受一个int参数，表示初始数量；countDown()方法将计数器减1；await()方法会阻塞当前线程，只有当计数器为0时，才会被唤醒并继续往下执行。 如下是一个简单测试类，只有当所有线程都完成了工作，才能输出close the door。 12345678910111213141516171819202122package net.yury757;import java.util.concurrent.CountDownLatch;public class Test10CountDownLatch &#123; public static void main(String[] args) throws InterruptedException &#123; CountDownLatch countDownLatch = new CountDownLatch(6); for (int i = 0; i &lt; 6; i++) &#123; new Thread(() -&gt; &#123; System.out.println(&quot;Person &quot; + Thread.currentThread().getName() + &quot; go out&quot;); // 计数器减1 countDownLatch.countDown(); &#125;, String.valueOf(i)).start(); &#125; // 等待计数器归零，才会继续向下执行 countDownLatch.await(); System.out.println(&quot;close the door&quot;); &#125;&#125; 2、CyclicBarrier循环阻塞。使当线程实现全部彼此等待直到达到一个共同的屏障点的辅助类。有点像加法计数器，全部线程都到达了一个准备好了的状态，则就会执行最终线程。 1234567891011121314151617181920212223242526272829package net.yury757;import java.util.concurrent.BrokenBarrierException;import java.util.concurrent.CyclicBarrier;/** * 集齐七颗龙珠召唤神龙 */public class Test11CyclicBarrier &#123; public static void main(String[] args) &#123; CyclicBarrier cyclicBarrier = new CyclicBarrier(7, () -&gt; &#123; System.out.println(&quot;召唤神龙成功&quot;); &#125;); for (int i = 0; i &lt; 7; i++) &#123; final int tmp = i; new Thread(() -&gt; &#123; System.out.println(&quot;收集到了第&quot; + Thread.currentThread().getName() + &quot;颗龙珠&quot;); try &#123; cyclicBarrier.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; 3、Semaphore信号量。类似排队系统，指定一个最大容量，先进来的可以处理，直到到达最大容量，后面的等待，当里面有元素释放时，后面的元素才能进入处理。acquire()放入元素，如果容量满了则等待；release()方法释放元素，唤醒其他等待线程。 123456789101112131415161718192021222324252627package net.yury757;import java.util.concurrent.Semaphore;import java.util.concurrent.TimeUnit;public class Test12Semaphore &#123; public static void main(String[] args) &#123; // 默认线程数量 Semaphore semaphore = new Semaphore(3); for (int i = 0; i &lt; 6; i++) &#123; new Thread(() -&gt; &#123; try &#123; semaphore.acquire(); System.out.println(Thread.currentThread().getName() + &quot;抢到车位&quot;); TimeUnit.SECONDS.sleep(2); System.out.println(Thread.currentThread().getName() + &quot;离开车位&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; semaphore.release(); &#125; &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; 六、读写锁接口：ReadWriteLock 实现类：ReentrantReadWriteLock，维护了一对关联的lock，一个只用于读（read lock，共享锁），一个只用于写（write lock，独占锁），read lock可以允许多个线程同时读，而write lock同一个时间点只允许一个线程写。 读-读：可以共存 读-写：不能同时，需排队 写-写：不能同时，需排队 独占锁（写锁）：一次只能被一个线程占有 共享锁（读锁）：可以被多个线程同时占有 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788package net.yury757;import jdk.internal.org.objectweb.asm.tree.FieldInsnNode;import java.util.HashMap;import java.util.Map;import java.util.UUID;import java.util.concurrent.locks.ReentrantReadWriteLock;public class Test13ReadWriteLock &#123; public static void main(String[] args) &#123; MyCacheLock cache = new MyCacheLock(); for (int i = 0; i &lt; 500; i++) &#123; final int tmp = i; new Thread(() -&gt; &#123; cache.put(tmp + &quot;&quot;, tmp); &#125;, String.valueOf(i)).start(); &#125; for (int i = 0; i &lt; 500; i++) &#123; final int tmp = i; new Thread(() -&gt; &#123; Object o = cache.get(tmp + &quot;&quot;); &#125;, String.valueOf(i)).start(); &#125; &#125;&#125;/** * 自定义缓存 */class MyCache&#123; private volatile Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); public void put(String key, Object object)&#123; System.out.println(Thread.currentThread().getName() + &quot;写入&quot; + key); this.map.put(key, object); System.out.println(Thread.currentThread().getName() + &quot;写入&quot; + key + &quot;OK&quot;); &#125; public Object get(String key)&#123; System.out.println(Thread.currentThread().getName() + &quot;读取&quot; + key); Object o = this.map.get(key); System.out.println(Thread.currentThread().getName() + &quot;读取&quot; + key + &quot;OK&quot;); return o; &#125;&#125;/** * 自定义缓存 */class MyCacheLock&#123; private volatile Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); // 读写锁：更加细粒度地控制读写 private ReentrantReadWriteLock readWriteLock = new ReentrantReadWriteLock(); // 写入操作，同一时间只允许一个线程写入 public void put(String key, Object object)&#123; readWriteLock.writeLock().lock(); try&#123; System.out.println(Thread.currentThread().getName() + &quot;写入&quot; + key); this.map.put(key, object); System.out.println(Thread.currentThread().getName() + &quot;写入&quot; + key + &quot;OK&quot;); &#125;catch (Exception ex)&#123; ex.printStackTrace(); &#125; finally &#123; readWriteLock.writeLock().unlock(); &#125; &#125; // 读取操作，允许多个线程同时读 public Object get(String key)&#123; readWriteLock.readLock().lock(); Object o = null; try&#123; System.out.println(Thread.currentThread().getName() + &quot;读取&quot; + key); o = this.map.get(key); System.out.println(Thread.currentThread().getName() + &quot;读取&quot; + key + &quot;OK&quot;); &#125;catch (Exception ex)&#123; ex.printStackTrace(); &#125;finally &#123; readWriteLock.readLock().lock(); &#125; return o; &#125;&#125; 七、阻塞队列接口：BlockingQueue 1、ArrayBlockingQueue该实现类基于数组实现。 方式 抛出异常 不抛出异常 阻塞等待 超时等待 添加 add(E) offer(E) put(E) offer(E, long, TimeUnit) 移除 remove() poll() take() poll(long, TimeUnit) 检查队首元素 element() peek() - - 2、LinkedBlockingQueue该实现类基于链表实现，和上面的使用方式一样，略。 3、SynchronousQueueSynchronousQueue为同步队列，和其他阻塞队列不太一样，SynchronousQueue在初始化时不需要传入capacity容量参数，可以认为SynchronousQueued的容量为0。 put(E)方法会阻塞，等待其他线程来取出该元素。 take()方法调用时，如果之前没有put进来值，则take会阻塞；如果之前有put进来值，则会取出之前put进来的值。 注意：在测试多线程和sleep时尽量不要用junit测试，junit对他们的支持有些问题，可以直接写一个main方法作为测试类。 八、线程池（重点）1、池化技术线程池、连接池、内存池、对象池…… 池化技术：事先准备好一些资源，有人要用，直接取出来使用，用完之后还回去即可。 好处： 降低资源创建时的CPU等资源消耗 提高响应速度 方便管理 线程池的优点：线程复用、可以控制最大并发数、方便管理线程。 2、七大参数创建线程池应使用如下构造方法自定义创建线程池，而不能使用后面会说的Executors来创建。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * Creates a new &#123;@code ThreadPoolExecutor&#125; with the given initial * parameters. * * @param corePoolSize the number of threads to keep in the pool, even * if they are idle, unless &#123;@code allowCoreThreadTimeOut&#125; is set * @param maximumPoolSize the maximum number of threads to allow in the * pool * @param keepAliveTime when the number of threads is greater than * the core, this is the maximum time that excess idle threads * will wait for new tasks before terminating. * @param unit the time unit for the &#123;@code keepAliveTime&#125; argument * @param workQueue the queue to use for holding tasks before they are * executed. This queue will hold only the &#123;@code Runnable&#125; * tasks submitted by the &#123;@code execute&#125; method. * @param threadFactory the factory to use when the executor * creates a new thread * @param handler the handler to use when execution is blocked * because the thread bounds and queue capacities are reached * @throws IllegalArgumentException if one of the following holds:&lt;br&gt; * &#123;@code corePoolSize &lt; 0&#125;&lt;br&gt; * &#123;@code keepAliveTime &lt; 0&#125;&lt;br&gt; * &#123;@code maximumPoolSize &lt;= 0&#125;&lt;br&gt; * &#123;@code maximumPoolSize &lt; corePoolSize&#125; * @throws NullPointerException if &#123;@code workQueue&#125; * or &#123;@code threadFactory&#125; or &#123;@code handler&#125; is null */public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 参数名 参数类型 解释 corePoolSize int 维持在线程池中的核心线程数，即使是空闲状态，也会保留在线程池中 maximumPoolSize int 线程池中允许容纳的最大线程。提交任务时，若发现任务阻塞队列满了时，线程池就会进行扩容，但不能超过这个最大线程数 keepAliveTime long 当线程池中的线程数超过核心线程数时，空闲线程的留存时间，当线程处于空闲状态超过这个时间时，会移除这个线程，直到总线程数等于核心线程数 unit TimeUnit keepAliveTime参数的单位 workQueue BlockingQueue&lt;Runnable&gt; 任务阻塞队列，当用户提交执行的任务数超过核心线程数时，剩余任务就会在队列中排队等待 threadFactory ThreadFactory 创建线程的工厂类 handler RejectedExecutionHandler 当任务队列达到上限或线程数达到最大线程数时的处理策略 3、线程池的工作流程 execute(Runnable command)提交任务 判断核心线程中是否存在空闲的线程？若存在，则占用一个空闲线程来执行任务；否则进行下一步。 判断任务阻塞队列是否已满？若没有满，则将任务存入任务阻塞队列中等待；否则进行下一步。 判断整个线程池是否已满，若没有满，则创建一个新的线程执行任务；否则执行拒绝策略。 4、四种拒绝策略（1）抛异常 12345678910111213141516171819public static class AbortPolicy implements RejectedExecutionHandler &#123; /** * Creates an &#123;@code AbortPolicy&#125;. */ public AbortPolicy() &#123; &#125; /** * Always throws RejectedExecutionException. * * @param r the runnable task requested to be executed * @param e the executor attempting to execute this task * @throws RejectedExecutionException always */ public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; throw new RejectedExecutionException(&quot;Task &quot; + r.toString() + &quot; rejected from &quot; + e.toString()); &#125;&#125; （2）在当前线程直接运行 12345678910111213141516171819public static class CallerRunsPolicy implements RejectedExecutionHandler &#123; /** * Creates a &#123;@code CallerRunsPolicy&#125;. */ public CallerRunsPolicy() &#123; &#125; /** * Executes task r in the caller&#x27;s thread, unless the executor * has been shut down, in which case the task is discarded. * * @param r the runnable task requested to be executed * @param e the executor attempting to execute this task */ public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; r.run(); &#125; &#125;&#125; （3）剔除最早提交的那个任务，将新任务开启执行 12345678910111213141516171819202122public static class DiscardOldestPolicy implements RejectedExecutionHandler &#123; /** * Creates a &#123;@code DiscardOldestPolicy&#125; for the given executor. */ public DiscardOldestPolicy() &#123; &#125; /** * Obtains and ignores the next task that the executor * would otherwise execute, if one is immediately available, * and then retries execution of task r, unless the executor * is shut down, in which case task r is instead discarded. * * @param r the runnable task requested to be executed * @param e the executor attempting to execute this task */ public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; e.getQueue().poll(); e.execute(r); &#125; &#125;&#125; （4）啥都不做，静默掠过 123456789101112131415public static class DiscardPolicy implements RejectedExecutionHandler &#123; /** * Creates a &#123;@code DiscardPolicy&#125;. */ public DiscardPolicy() &#123; &#125; /** * Does nothing, which has the effect of discarding task r. * * @param r the runnable task requested to be executed * @param e the executor attempting to execute this task */ public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; &#125;&#125; 5、注意线程池的创建不允许使用Executors去创建，而是通过ThreadPoolExecutor去创建。本质上Executors也是通过new ThreadPoolExecutor的方式来创建线程池，但是Executors把很多参数都封装好了，不能自定义某些参数，因此自己通过new ThreadPoolExecutor的方式创建的线程池在管理上更具有自主性，此外通过使用Executors封装好了的参数创建的线程池有诸多弊端，如下： （1）Executors.newFixedThreadPool(int nThreads) 这种创建方式创建的线程池的任务队列大小是Integer.MAX_VALUE，因此如果工作线程满了，如果同时涌来大量任务请求，则会导致任务队列积压，非常容易导致内存溢出、程序崩溃。 12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; （2）Executors.newSingleThreadExecutor() 这种方式和上面那个一样，都是由于创建的是一个Integer.MAX_VALUE大小的任务队列，容易造成任务积压导致内存溢出。 123456public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; （3）Executors.newCachedThreadPool() 这种创建方式的任务队列是SynchronousQueue，线程池最大容量是Integer.MAX_VALUE，而我们知道SynchronousQueue是同步队列，只要有元素put进去，就会阻塞，此时就需要创建一个线程来处理这个请求，而线程池的最大值又设定为Integer.MAX_VALUE，因此同样如果请求突然增加，就会创建大量线程，容易造成内存溢出。 12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; （4）Executors.newScheduledThreadPool(int corePoolSize) 这种创建方式和上面一样，可创建的线程最大容量为Integer.MAX_VALUE，容易会因为大量创建线程而导致内存溢出。 12345678public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) &#123; return new ScheduledThreadPoolExecutor(corePoolSize);&#125;public ScheduledThreadPoolExecutor(int corePoolSize) &#123; super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue());&#125; 总结如下： 以下四种方式都是用的默认的线程创建工厂和默认的拒绝策略。 创建线程池的方式 核心线程数 最大线程数 线程空闲超时时间 任务队列类 任务队列容量 OOM原因 Executors.newFixedThreadPool(int nThreads) 自定义 自定义 0毫秒 LinkedBlockingQueue Integer.MAX_VALUE 任务队列大量堆积导致OOM Executors.newSingleThreadExecutor() 1 1 0毫秒 LinkedBlockingQueue Integer.MAX_VALUE 任务队列大量堆积导致OOM Executors.newCachedThreadPool() 0 Integer.MAX_VALUE 60秒 SynchronousQueue 0 创建大量线程导致OOM Executors.newScheduledThreadPool(int corePoolSize) 自定义 Integer.MAX_VALUE 0微秒 DelayedWorkQueue 0 创建大量线程导致OOM 九、IO密集型程序和CPU密集型程序 CPU密集型也叫计算密集型，指的是系统运作大部分的状况是CPU Loading很高，I/O在很短的时间就可以完成，而CPU还有许多运算要处理。如计算圆周率程序、对视频音频进行编码解码的程序等，绝大部分时间都在进行CPU计算。开发CPU密集型程序尽量使用运行效率更高的编程语言，比如C/C++。 IO密集型是指程序绝大多数时间都花在IO上了，而IO结束后，CPU的计算在很短时间内就可以完成。常见的大部分任务都是IO密集型任务，比如web应用。开发IO密集型程序最好选择开发效率更高的编程语言，如python/java。 十、四大函数式接口函数式接口：只有一个方法的接口。 1、Function1234567891011121314151617181920212223public static void test01()&#123; Function&lt;Integer, String&gt; function = new Function&lt;Integer, String&gt;() &#123; @Override public String apply(Integer o) &#123; StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; o; i++) &#123; sb.append(&quot;1&quot;); &#125; return sb.toString(); &#125; &#125;; Function&lt;Integer, String&gt; function2 = (Integer o) -&gt;&#123; StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; o; i++) &#123; sb.append(&quot;2&quot;); &#125; return sb.toString(); &#125;; System.out.println(function.apply(2)); System.out.println(function2.apply(4));&#125; 2、Predicate12345678910111213public static void test02()&#123; Predicate&lt;Float&gt; predicate1 = new Predicate&lt;Float&gt;() &#123; @Override public boolean test(Float o) &#123; return o &gt; 1000.2f; &#125; &#125;; Predicate&lt;Float&gt; predicate2 = (Float o) -&gt; &#123; return o &gt; 1000.2f; &#125;; System.out.println(predicate1.test(1000f)); System.out.println(predicate1.test(1001f));&#125; 3、Consumer1234567891011121314151617public static void test03()&#123; Consumer&lt;String[]&gt; consumer1 = new Consumer&lt;String[]&gt;() &#123; @Override public void accept(String[] strings) &#123; for (String s : strings) &#123; System.out.println(s); &#125; &#125; &#125;; Consumer&lt;String[]&gt; consumer2 = (String[] strings) -&gt; &#123; for (String s : strings) &#123; System.out.println(s); &#125; &#125;; consumer1.accept(new String[] &#123;&quot;123&quot;, &quot;456&quot;&#125;); consumer2.accept(new String[] &#123;&quot;abc&quot;, &quot;def&quot;&#125;);&#125; 4、Supplier12345678910111213public static void test04()&#123; Supplier&lt;String&gt; supplier1 = new Supplier&lt;String&gt;() &#123; @Override public String get() &#123; return &quot;suppplier1 =&gt; &quot; + UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;); &#125; &#125;; Supplier&lt;String&gt; supplier2 = () -&gt; &quot;suppplier2 =&gt; &quot; + UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;); System.out.println(supplier1.get()); System.out.println(supplier2.get());&#125; 十一、流式计算大数据：存储 + 计算 java集合类、数据库等本质是用来存储东西的，计算都应交给流来操作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class Test19StreamCalculate &#123; public static void main(String[] args) &#123; test01(); &#125; public static void test01()&#123; User u1 = new User(1, &quot;a&quot;, 21); User u2 = new User(2, &quot;b&quot;, 22); User u3 = new User(3, &quot;c&quot;, 23); User u4 = new User(4, &quot;d&quot;, 24); User u5 = new User(5, &quot;e&quot;, 25); User u6 = new User(6, &quot;f&quot;, 26); List&lt;User&gt; userList = Arrays.asList(u1, u2, u3, u4, u5, u6); // Lambda表达式、链式编程、函数式接口、Stream流式计算 userList.stream() .filter(user -&gt; user.getId() % 2 == 0) // 筛选id为偶数的用户 .filter(user -&gt; user.getAge() &gt; 23) // 筛选年龄大于23的用户 .map(user -&gt; user.getName().toUpperCase()) // 只保留名字 .sorted((String s1, String s2) -&gt; s2.compareTo(s1)) // 对名字倒序排序 .limit(1) // 只取第一个 .forEach(System.out::println); // 打印 &#125;&#125;class User&#123; private int id; private String name; private int age; public int getId() &#123; return id; &#125; public String getName() &#123; return name; &#125; public int getAge() &#123; return age; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;id=&quot; + id + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125; public User(int id, String name, int age) &#123; this.id = id; this.name = name; this.age = age; &#125;&#125; 十二、ForkJoin1、原理将一个大任务拆分成多个小任务，每个子任务分别执行，全部子任务结束后获取结果，再将所有结果汇总起来成最终结果。 2、特点：工作窃取子任务分配给其他很多线程后，不同线程执行完成的时间可能不一样，先完成的的线程会把未完成的线程的部分任务拿过来帮忙一起做。 里面维护的是双端队列，可以从两端取任务来执行。 3、示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100package net.yury757;import java.util.concurrent.ExecutionException;import java.util.concurrent.ForkJoinPool;import java.util.concurrent.ForkJoinTask;import java.util.concurrent.RecursiveTask;import java.util.stream.LongStream;public class Test18FockJoin extends RecursiveTask&lt;Long&gt; &#123; private Long start; private Long end; // 临界值 private Long temp = 10000L; public Test18FockJoin(Long start, Long end) &#123; this.start = start; this.end = end; &#125; public Long sum()&#123; long sum = 0L; for (Long i = start; i &lt;= end; i++) &#123; sum += i; &#125; return sum; &#125; @Override protected Long compute() &#123; if ((end - start) &lt; temp)&#123; long sum = 0L; for (Long i = start; i &lt;= end; i++) &#123; sum += i; &#125; return sum; &#125;else&#123; long middle = (start + end) / 2; Test18FockJoin test18FockJoin1 = new Test18FockJoin(start, middle); test18FockJoin1.fork(); Test18FockJoin test18FockJoin2 = new Test18FockJoin(middle + 1, end); test18FockJoin2.fork(); return test18FockJoin1.join() + test18FockJoin2.join(); &#125; &#125; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; test03(); &#125; /** * 普通计算方式 * 500000000500000000 * 3603毫秒 */ public static void test01()&#123; long starttime = System.currentTimeMillis(); Test18FockJoin task = new Test18FockJoin(1L, 10_0000_0000L); Long res = task.sum(); System.out.println(res); long endtime = System.currentTimeMillis(); System.out.println(&quot;花费时间：&quot; + (endtime - starttime) + &quot;毫秒&quot;); &#125; /** * 使用forkjoin * 500000000500000000 * 2133毫秒 */ public static void test02() throws ExecutionException, InterruptedException &#123; long starttime = System.currentTimeMillis(); ForkJoinPool forkJoinPool = new ForkJoinPool(); Test18FockJoin task = new Test18FockJoin(1L, 10_0000_0000L); ForkJoinTask&lt;Long&gt; submit = forkJoinPool.submit(task); Long res = task.get(); System.out.println(res); long endtime = System.currentTimeMillis(); System.out.println(&quot;花费时间：&quot; + (endtime - starttime) + &quot;毫秒&quot;); &#125; /** * 使用Stream并行流 * 500000000500000000 * 215毫秒 */ public static void test03()&#123; long starttime = System.currentTimeMillis(); long res = LongStream.rangeClosed(1, 10_0000_0000L).parallel().reduce(0, Long::sum); System.out.println(res); long endtime = System.currentTimeMillis(); System.out.println(&quot;花费时间：&quot; + (endtime - starttime) + &quot;毫秒&quot;); &#125;&#125; 十三、异步回调（Future）Future接口：用来处理异步任务的，提供了检查异步任务是否完成、取消任务、是否被取消了、获取任务结果、在指定延迟内获取返回值等接口方法。 CompletableFuture为其中一个实现类，介绍两个方法： 12345// 没有返回值的异步回调public static CompletableFuture&lt;Void&gt; runAsync(Runnable runnable)&#123;&#125;// 有返回值的异步回调public static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier) &#123;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package net.yury757;import java.util.concurrent.CompletableFuture;import java.util.concurrent.ExecutionException;import java.util.concurrent.Future;import java.util.concurrent.TimeUnit;/** * 异步调用： * 1、异步执行 * 2、成功回调 * 3、失败回调 */public class Test20Future &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; // 无返回值的异步任务 CompletableFuture&lt;Void&gt; completableFuture = CompletableFuture.runAsync(() -&gt; &#123; System.out.println(&quot;异步无返回值1&quot;); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;异步无返回值2&quot;); &#125;); System.out.println(&quot;同步任务&quot;); // 阻塞，直到获取结果 Void unused = completableFuture.get(); System.out.println(&quot;异步无返回值任务结束&quot;); // 有返回值的异步任务 CompletableFuture&lt;Integer&gt; completableFuture2 = CompletableFuture.supplyAsync(() -&gt; &#123; System.out.println(&quot;异步有返回值1&quot;); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;异步有返回值2&quot;); int a = 10 / 0; return 1024; &#125;); Integer res = completableFuture2.whenComplete((t, u) -&gt; &#123; System.out.println(&quot;t =&gt; &quot; + t); System.out.println(&quot;u =&gt; &quot; + u); &#125;).exceptionally((ex) -&gt; &#123; System.out.println(ex.getMessage()); return 233; &#125;).get(); System.out.println(res); &#125;&#125; 十四、JMM（重要）1、什么是JMMJMM是java memory model，即java内存模型，因为在不同硬件厂商和不同操作系统下，程序对内存的访问会有一定的差异，会造成相同代码运行出来的结果不一样，因此JMM屏蔽掉了这些差异，实现了java在不同的平台下都能达到一致的效果。 Java内存模型规定所有的变量都存储在主内存中，包括实例变量，静态变量，但是不包括局部变量和方法参数。每个线程都有自己的工作内存，线程的工作内存保存了该线程用到的变量和主内存的副本拷贝，线程对变量的操作都在工作内存中进行，然后刷回到主存。线程不能直接读写主内存中的变量。 2、八种内存交互操作JMM八种内存交互操作： lock(锁定)，作用于主内存中的变量，把变量标识为线程独占的状态。 read(读取)，作用于主内存的变量，把变量的值从主内存传输到线程的工作内存中，以便下一步的load操作使用。 load(加载)，作用于工作内存的变量，把read操作主存的变量放入到工作内存的变量副本中。 use(使用)，作用于工作内存的变量，把工作内存中的变量传输到执行引擎，每当虚拟机遇到一个需要使用到变量的值的字节码指令时将会执行这个操作。 assign(赋值)，作用于工作内存的变量，它把一个从执行引擎中接受到的值赋值给工作内存的变量副本中，每当虚拟机遇到一个给变量赋值的字节码指令时将会执行这个操作。 store(存储)，作用于工作内存的变量，它把一个从工作内存中一个变量的值传送到主内存中，以便后续的write使用。 write(写入)：作用于主内存中的变量，它把store操作从工作内存中得到的变量的值放入主内存的变量中。 unlock(解锁)：作用于主内存的变量，它把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。 8种内存交互操作制定的规则： 不允许read、load、store、write操作之一单独出现，也就是read操作后必须load，store操作后必须write。 不允许线程丢弃他最近的assign操作，即工作内存中的变量数据改变了之后，必须告知主存。 不允许线程将没有assign的数据从工作内存同步到主内存。 一个新的变量必须在主内存中诞生，不允许工作内存直接使用一个未被初始化的变量。就是对变量实施use、store操作之前，必须经过load和assign操作。 一个变量同一时间只能有一个线程对其进行lock操作。多次lock之后，必须执行相同次数unlock才可以解锁。 如果对一个变量进行lock操作，会清空所有工作内存中此变量的值。在执行引擎使用这个变量前，必须重新load或assign操作初始化变量的值。 如果一个变量没有被lock，就不能对其进行unlock操作。也不能unlock一个被其他线程锁住的变量。 一个线程对一个变量进行unlock操作之前，必须先把此变量同步回主内存。 3、三个特征：可见性、原子性、有序性（1）volatile保证可见性可见性，是指在有多个线程对内存中的同一个变量进行操作时，如果内存中的值被修改了，其他线程可以立即直到这个值被改掉了，并获取最新的值。 如下代码示例，这个程序永远不会结束。即这种情况下是没保证可见性的。 123456789101112131415161718package net.yury757;import java.util.concurrent.TimeUnit;public class Test21Volatile &#123; public static int num = 0; public static void main(String[] args) throws InterruptedException &#123; Thread thread = new Thread(() -&gt; &#123; while (num == 0)&#123; &#125; System.out.println(&quot;线程2：&quot; + num); &#125;); thread.start(); TimeUnit.SECONDS.sleep(1); num = 1; System.out.println(&quot;主线程：&quot; + num); &#125;&#125; 在某些情况下，使用synchronized和lock是可以保证可见性的，但是上面例子不行。 上面这个例子只要在num的定义加上volatile关键字即可。 1public volatile static int num = 0; （2）Atomic类保证原子性原子性：不可分割性，在数据库的事务中，指的是一个事务的所有操作要么全部完成，要么全部不完成。而线程的原子性是指一个线程在执行任务时，是不可分割，不可中断的，且不能被其他线程所干扰，这个线程中的所有操作要么全部成功，要么全部失败。 volatile是无法保证原子性的。 1234567891011121314151617181920212223package net.yury757;public class Test22Atomic &#123; public volatile static int num = 0; public static void add()&#123; num++; &#125; public static void main(String[] args) &#123; for (int i = 0; i &lt; 10; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; 1000; j++) &#123; add(); &#125; &#125;).start(); &#125; while(Thread.activeCount() &gt; 2)&#123; Thread.yield(); &#125; System.out.println(num); &#125;&#125; 即多个线程同时操作同一个资源时，会出现资源错误。对于这种问题，常见的在add方法那里用synchronized和lock来解决。 然而还有一种办法就是使用Atomic类，使用原子类的速度一般情况下比使用锁的效率高。如下： 12345678910111213141516171819202122232425package net.yury757;import java.util.concurrent.atomic.AtomicInteger;public class Test22Atomic &#123; public volatile static AtomicInteger num = new AtomicInteger(); public static void add()&#123; num.getAndIncrement(); // AtomicInteger类的+1方法，底层用的CAS &#125; public static void main(String[] args) &#123; for (int i = 0; i &lt; 10; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; 1000; j++) &#123; add(); &#125; &#125;).start(); &#125; while(Thread.activeCount() &gt; 2)&#123; Thread.yield(); &#125; System.out.println(num); &#125;&#125; 具体探究见CAS章节。 （3）volatile保证有序性（禁止指令重排）什么是有序性，即你编写的每一行源代码被计算机CPU拿去执行的顺序是和源代码顺序一致的。 指令重排是指你写的程序，计算机并不是按照你写的那样去执行的，即计算机会对你的代码重新架构再执行。 1源代码 —&gt; 编译器的优化重排 —&gt; 指令并行也可能会重排 —&gt; 内存系统也可能会重排 —&gt; 执行 计算机对我们的代码做指令重排就没办法保证有序性，指令重排可能会产生一些不可预知的问题。 在Java中，可以使用synchronized或者volatile保证多线程之间操作的有序性，即禁止计算机对某些代码做指令重排。 十五、深究单例模式1、饿汉式单例模式这种模式很少出现线程安全问题，缺点是类一加载就会初始化一个对象，有时候每用到这个对象时，也会一直存在，浪费空间。 123456789101112131415package net.yury757;/** * 饿汉式单例模式 */public class Test23Hungry &#123; private Test23Hungry()&#123; &#125; private final static Test23Hungry hungry = new Test23Hungry(); public static Test23Hungry getInstance()&#123; return hungry; &#125;&#125; 2、懒汉式单例模式懒汉式单例模式是指要用到这个实例时再初始化。如果私有构造方法没有synchronized时，多个线程同时getInstance时会有线程安全问题。 同时加上volatile防止指令重排导致的线程安全问题。 12345678910111213141516171819202122232425262728293031323334353637383940414243package net.yury757;/** * 懒汉式单例模式 */public class Test24Lazy &#123; private Test24Lazy()&#123; System.out.println(Thread.currentThread().getName() + &quot; - OK&quot;); &#125; // 加volatile禁止指令重排导致的线程安全问题 private volatile static Test25DCLLazy lazy = null; // 线程不安全// public static Test24Lazy getInstance()&#123;// if (lazy == null)&#123;// lazy = new Test24Lazy();// &#125;// return lazy;// &#125; // 线程安全 public static Test24Lazy getInstance()&#123; // 双重检测 if (lazy == null)&#123; // 静态方法，因此要锁住class synchronized (Test24Lazy.class)&#123; if (lazy == null)&#123; lazy = new Test24Lazy(); &#125; &#125; &#125; return lazy; &#125; public static void main(String[] args) &#123; for (int i = 0; i &lt; 10; i++) &#123; new Thread(() -&gt; &#123; // 多线程会有问题，可能多次初始化这个类，需要对getInstance方法加锁 Test24Lazy lazy = Test24Lazy.getInstance(); &#125;).start(); &#125; &#125;&#125; 3、反射对单例模式的影响对于例模式，正常情况下是安全的，但是如果通过反射来创建实例，即打破了单例模式。如下： 123456789101112131415161718192021222324package net.yury757;import java.lang.reflect.Constructor;import java.lang.reflect.InvocationTargetException;public class Test25ReflectionHungry &#123; private Test25ReflectionHungry()&#123; System.out.println(Thread.currentThread().getName() + &quot; - OK&quot;); &#125; private volatile static Test25ReflectionHungry hungry = new Test25ReflectionHungry(); public static Test25ReflectionHungry getInstance()&#123; return hungry; &#125; public static void main(String[] args) throws NoSuchMethodException, IllegalAccessException, InvocationTargetException, InstantiationException &#123; Test25ReflectionHungry single = Test25ReflectionHungry.getInstance(); Constructor&lt;Test25ReflectionHungry&gt; constructor = Test25ReflectionHungry.class.getDeclaredConstructor(null); constructor.setAccessible(true); Test25ReflectionHungry single2 = constructor.newInstance(); System.out.println(single); System.out.println(single2); &#125;&#125; 对于饿汉式单例模式，直接把构造函数改成如下即可： 12345678private Test25ReflectionHungry()&#123; synchronized (Test25ReflectionHungry.class)&#123; System.out.println(Thread.currentThread().getName() + &quot; - OK&quot;); if (hungry != null)&#123; throw new RuntimeException(&quot;不要尝试使用反射创建对象!&quot;); &#125; &#125;&#125; 但是对于懒汉式单例模式，改成上面这种，有一个问题就是如果一开始就使用反射来创建对象的话依旧没办法阻止。 因此对于懒汉式单例模式可以尝试通过使用使用如下修改： 但是依旧可以通过反射修改flag的值来破坏单例模式，即使通过对flag的值进行加密等其他操作，通过反射几乎总是可以破坏这种单例模式。 123456789101112131415161718192021222324252627282930313233343536373839404142package net.yury757;import java.lang.reflect.Constructor;import java.lang.reflect.InvocationTargetException;public class Test26ReflectionLazy &#123; private static boolean flag = false; private Test26ReflectionLazy()&#123; synchronized (Test25ReflectionHungry.class)&#123; System.out.println(Thread.currentThread().getName() + &quot; - OK&quot;); if (flag == false)&#123; flag = true; &#125;else&#123; throw new RuntimeException(&quot;不要尝试使用反射创建对象!&quot;); &#125; &#125; &#125; private volatile static Test26ReflectionLazy lazy = null; public static Test26ReflectionLazy getInstance()&#123; // 双重检测 if (lazy == null)&#123; // 静态方法，因此要锁住class synchronized (Test24Lazy.class)&#123; if (lazy == null)&#123; lazy = new Test26ReflectionLazy(); flag = true; &#125; &#125; &#125; return lazy; &#125; public static void main(String[] args) throws NoSuchMethodException, IllegalAccessException, InvocationTargetException, InstantiationException &#123; Constructor&lt;Test26ReflectionLazy&gt; constructor = Test26ReflectionLazy.class.getDeclaredConstructor(null); constructor.setAccessible(true); Test26ReflectionLazy single = constructor.newInstance(); Test26ReflectionLazy single2 = constructor.newInstance(); System.out.println(single); System.out.println(single2); &#125;&#125; 4、通过enum来实现安全的单例模式1234567891011121314151617181920212223242526272829303132333435363738394041package net.yury757;import java.lang.reflect.Constructor;import java.lang.reflect.InvocationTargetException;public enum Test27EnumSingle &#123; INSTANCE; private volatile Test27ReflectionLazy lazy = null; public Test27ReflectionLazy getInstance()&#123; return lazy; &#125; private Test27EnumSingle()&#123; if (lazy == null) &#123; lazy = new Test27ReflectionLazy(); &#125; &#125; public static void main(String[] args) throws NoSuchMethodException, IllegalAccessException, InvocationTargetException, InstantiationException &#123; System.out.println(Test27EnumSingle.INSTANCE.getInstance()); // 通过反射内部类无法获取对象 Constructor&lt;Test27ReflectionLazy&gt; constructor = Test27ReflectionLazy.class.getDeclaredConstructor(null); constructor.setAccessible(true); Object single = constructor.newInstance(); Object single2 = constructor.newInstance(); System.out.println(single); System.out.println(single2); &#125;&#125;class Test27ReflectionLazy &#123; Test27ReflectionLazy()&#123; synchronized (Test27ReflectionLazy.class)&#123; System.out.println(Thread.currentThread().getName() + &quot; - OK&quot;); if (Test27EnumSingle.INSTANCE != null &amp;&amp; Test27EnumSingle.INSTANCE.getInstance() != null)&#123; throw new RuntimeException(&quot;不要尝试使用反射创建对象!&quot;); &#125; &#125; &#125;&#125; 十六、CAS1、乐观锁和悲观锁悲观锁：线程开始执行第一步就是获取锁，一旦获得锁，其他的线程进入后就会阻塞等待锁。synchronized和实现了Lock接口的各种锁都是悲观锁。 乐观锁：线程执行的时候不会加锁，假设没有冲突去完成某项操作，如果因为冲突失败了就重试，最后直到成功为止。如自旋锁。 2、什么是CASCAS（Compare-And-Swap）是比较并交换的意思，它是一条 CPU 并发原语（Unsafe类中的三个方法如下），用于判断内存中某个值是否为预期值，如果是则更改为新的值，这个过程是原子的。 CAS机制当中使用了3个基本操作数：内存地址V（由一个Object和一个long型的offset决定），旧的预期值A，计算后要修改后的新值B。 123456789// var1和var2找到对应的内存地址// var4为预期值// var5为新值// 如果内存中的值和预期值相同，则把内存中的值修改为var5public final native boolean compareAndSwapObject(Object var1, long var2, Object var4, Object var5);public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5);public final native boolean compareAndSwapLong(Object var1, long var2, long var4, long var6); 比如我们在探究Atomic保证原子性时发现，使用int型的num++并不是原子操作，可能产生线程安全问题，而num.getAndIncrement()方法则是原子操作，其底层通过自旋锁+CAS来实现。 123456789101112131415public final int getAndIncrement() &#123; return unsafe.getAndAddInt(this, valueOffset, 1);&#125;// 自旋锁 + CASpublic final int getAndAddInt(Object var1, long var2, int var4) &#123; int var5; do &#123; var5 = this.getIntVolatile(var1, var2); &#125; while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5;&#125;public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5); 解释如下： 先把内存中的这个对象的值取出来，然后把这个值和内存中的值比较，如果相同，则设置为新的值并退出循环，如果不同则继续循环（自旋锁，会一直尝试比较并修改，直到成功为止）。有人会觉得这不明显相同吗，其实并不是。在高并发的情况下，线程1把这个值取出来了，比如是100，下一时刻线程2可能已经把这个值更新掉了，比如是200，如果线程1仍然更新为101的话就错了，应该在线程2的操作的基础上再+1，即201。因此必须把取出来的值和内存上的值再比较一遍再更新，而这个比较再更新的操作则是一个cpu的原子操作，即在cpu内部是一个不可分割的操作，如果比较发现不同，说明别人把这个东西改过了，因此必须重来。 3、ABA问题ABA问题是指，上面说的线程2把内存中的值改成了200，但是如果中间还有一个线程3又把值从200改回到了100，即线程1执行过程中，内存中的值修改过两次，从100到200，又从200回到100。线程1再执行比较时发现值和预期相同，于是就更新到101。 但是这个100已经不是原来的100了。这就是ABA问题。 ABA问题在大多数情况下不会对程序产生太大的影响，通过带时间戳的原子类可以解决ABA问题，其内部维护了一个int型的时间戳，CAS操作需要再对比时间戳，因此可以通过时间戳来判断内存中的值是否被改动过。 1234567891011121314public class AtomicStampedReference&lt;V&gt; &#123; private static class Pair&lt;T&gt; &#123; final T reference; final int stamp; private Pair(T reference, int stamp) &#123; this.reference = reference; this.stamp = stamp; &#125; static &lt;T&gt; Pair&lt;T&gt; of(T reference, int stamp) &#123; return new Pair&lt;T&gt;(reference, stamp); &#125; &#125; private volatile Pair&lt;V&gt; pair;&#125; 十七、自旋锁自旋锁（spinlock）：是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。被“阻塞”的线程并不是真正停了，而是一直在做循环判断的“等待”（busy waiting），直到其他线程解锁。 123456789101112131415161718192021222324252627package net.yury757;import java.util.concurrent.atomic.AtomicReference;public class Test29SpinLockDemo &#123; AtomicReference&lt;Thread&gt; reference = new AtomicReference&lt;&gt;(); // 加锁 public void lock()&#123; Thread thread = Thread.currentThread(); System.out.println(thread.getName() + &quot; =&gt; 尝试获取锁&quot;); while (!reference.compareAndSet(null, thread))&#123;&#125; System.out.println(thread.getName() + &quot; =&gt; 获取锁成功&quot;); &#125; // 解锁 public void unlock()&#123; Thread thread = Thread.currentThread(); System.out.println(thread.getName() + &quot; =&gt; 尝试解锁&quot;); boolean b = reference.compareAndSet(thread, null); if (b)&#123; System.out.println(thread.getName() + &quot; =&gt; 解锁成功&quot;); &#125;else&#123; System.out.println(thread.getName() + &quot; =&gt; 解锁失败，当前线程没有被加锁&quot;); &#125; &#125;&#125; 解释： 当第一个线程获取到锁时，reference为空，因此lock方法中reference.compareAndSet(null, thread)返回true，则while循环失败退出。 当第二线程尝试获取锁时，reference不为空，则reference.compareAndSet(null, thread)返回false，while则会一直循环。 当第一个线程解锁时，会判断reference中的线程引用是否是当前线程，然后将reference置为null。此时下一刻第二个线程的循环就判断成功了，则成功获取到了锁。 当有多个线程尝试竞争锁时，由于reference.compareAndSet(null, thread)是原子操作，因此只可能有一个线程成功获取锁。","categories":[{"name":"java","slug":"java","permalink":"https://yury757.github.io/categories/java/"}],"tags":[]},{"title":"hadoop-study","slug":"bigdata/hadoop/hadoop-study","date":"2021-06-30T16:00:00.000Z","updated":"2022-10-07T12:34:42.098Z","comments":true,"path":"bigdata/hadoop/hadoop-study/","link":"","permalink":"https://yury757.github.io/bigdata/hadoop/hadoop-study/","excerpt":"","text":"安装下载地址：Index of /apache/hadoop/common (tsinghua.edu.cn) 12345678910111213141516171819cd /home/yury# 下载wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/stable/hadoop-3.3.0.tar.gz# 解压tar xzvf hadoop-3.3.0.tar.gz# 以这个目录为工作目录cd hadoop-3.3.0/# 编辑以下文件vi ./etc/hadoop/hadoop-env.sh# 将以下环境变量加入文件export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64# 配置环境变量sudo vi /etc/profileexport HADOOP_HOME=/home/yury/hadoop-3.3.0/export PATH=$&#123;PATH&#125;:$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin 启动1、单机模式hadoop默认情况下是配置成一个单独的java进程的单机模式。以下示例为mapreduce的一个例子，复制解压缩的conf目录以用作输入，然后查找并显示给定正则表达式的每个匹配项。输出被写入给定的输出目录。 hadoop本质就是一个文件系统（hdfs）以及在这个文件系统上的计算（MapReduce），只不过这个文件系统的特殊和优势之处在于适用于分布式和大数据，因此下面这个mapreduce的例子就是对文件处理。 1234mkdir inputcp etc/hadoop/*.xml inputbin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar grep input output &#x27;dfs[a-z.]+&#x27;cat output/* 2、伪分布式伪分布式是指，整体还是在一个服务器中，但是namenode、datanode等服务分别运行在不同的JVM进程中。 修改etc/hadoop/core-site.xml： 1vi ./etc/hadoop/core-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改etc/hadoop/hdfs-site.xml： 1vi ./etc/hadoop/hdfs-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置ssh无密码登录各个服务器，不然后面会提示没有权限 1234567891011# 如果是首次使用该命令，需要输入回复yesssh localhost# 以上命令如果需要密码，则配置以下命令ssh-keygen -t rsa -P &#x27;&#x27; -f ~/.ssh/id_rsacat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keyschmod 0600 ~/.ssh/authorized_keys# 如果是完全分布式，则需将本服务器的公钥复制到其他服务器ssh-copy-id 192.168.0.202ssh-copy-id 192.168.0.203 运行一个测试 每次修改了core-site.xml都要格式化namenode。 12345678910111213141516171819202122232425262728293031323334353637383940414243# 格式化hdfs文件系统bin/hdfs namenode -format# 修改一个配置export HDFS_NAMENODE_USER=&quot;root&quot;export HDFS_DATANODE_USER=&quot;root&quot;export HDFS_SECONDARYNAMENODE_USER=&quot;root&quot;export YARN_RESOURCEMANAGER_USER=&quot;root&quot;export YARN_NODEMANAGER_USER=&quot;root&quot;# 启动一个名称节点和数据节点sbin/start-dfs.sh# 成功启动后可以在下面这个url来查看web管理界面# http://192.168.141.141:9870/# yarn的web管理页面# http://192.168.141.141:8088/jps# jps命令结果如下# 7969 Jps# 7589 DataNode# 7848 SecondaryNameNode# 7369 NameNode# 创建HDFS文件系统的文件夹bin/hdfs dfs -mkdir /userbin/hdfs dfs -mkdir /user/yury# 复制所有xml文件到分布式文件系统的input文件夹中bin/hdfs dfs -mkdir /inputbin/hdfs dfs -put etc/hadoop/*.xml /input# 运行mapreduce例子bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar grep input output &#x27;dfs[a-z.]+&#x27;# 测试输出文件（把分布式系统中的文件拷贝到本地文件系统并查看内容）bin/hdfs dfs -get output outputcat output/*# 或者直接在分布式文件系统上查看输出文件bin/hdfs dfs -cat output/*# 停止进程sbin/stop-dfs.sh 配置yarn 修改etc/hadoop/mapred-site.xml 1vi etc/hadoop/mapred-site.xml 12345678&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.application.classpath&lt;/name&gt; &lt;value&gt;$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*&lt;/value&gt;&lt;/property&gt; 修改etc/hadoop/yarn-site.xml 1vi etc/hadoop/yarn-site.xml 12345678&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt; &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;&lt;/property&gt; 启动yarn 12345# 启动sbin/start-yarn.sh# 关闭sbin/stop-yarn.sh 3、完全分布式配置wokers（3.2.1之前的版本是slaves） 1234vi /etc/hadoop/workers# 将工作结点的ip或者ip别名放入workers文件即可192.168.0.202192.168.0.203 同步配置的其他服务器命令： 12scp etc/hadoop/* yury@192.168.0.202:/home/yury/hadoop-3.3.0/etc/hadoop/scp etc/hadoop/* yury@192.168.0.203:/home/yury/hadoop-3.3.0/etc/hadoop/ 问题datanode没有显示在web界面中","categories":[{"name":"bigdata","slug":"bigdata","permalink":"https://yury757.github.io/categories/bigdata/"}],"tags":[]},{"title":"第一章-MySQL架构与历史","slug":"database/mysql/《高性能MySQL》/第1章-MySQL架构与历史/第一章-MySQL架构与历史","date":"2020-11-30T16:00:00.000Z","updated":"2022-10-07T12:45:45.322Z","comments":true,"path":"database/mysql/《高性能MySQL》/第1章-MySQL架构与历史/第一章-MySQL架构与历史/","link":"","permalink":"https://yury757.github.io/database/mysql/%E3%80%8A%E9%AB%98%E6%80%A7%E8%83%BDMySQL%E3%80%8B/%E7%AC%AC1%E7%AB%A0-MySQL%E6%9E%B6%E6%9E%84%E4%B8%8E%E5%8E%86%E5%8F%B2/%E7%AC%AC%E4%B8%80%E7%AB%A0-MySQL%E6%9E%B6%E6%9E%84%E4%B8%8E%E5%8E%86%E5%8F%B2/","excerpt":"","text":"1.1 MySQL逻辑架构 第一层：连接、线程处理、授权认证、安全等 第二层：缓存、解析器、优化器等。第一层 + 第二层 = 服务器层？？？ 第三层：存储引擎，负责MySQL数据的存储和提取。 1.1.1 连接管理与安全性每个客户端连接都会在服务器中有一个线程，这个连接的查询只会在这个单独的线程中进行。如： 使用命令行登录MySQL服务器后，show processlist就会新增一个线程； 使用workbench打开一个连接后会新增两个线程（不知道为何有两个线程？），workbench的SQL tab和线程无关，开十个SQL tab还是两个线程； 使用datagrip每打开一个query console并查询数据后就会开启一个线程。 1.1.2 优化与执行特殊关键字提示优化器：hint 请求优化器解释优化过程：explain 1.2 并发控制并发控制的问题：多个查询在同一时刻修改数据时产生的问题。 并发控制的两个层面：服务器层、存储引擎层。 1.2.1 读写锁在处理并发的读或写时，可以通过实现一个由两种类型的锁组成的锁系统来解决问题。这两种类型的锁通常被称为共享锁（shared lock）和排他锁（exclusive lock），也叫读锁（read lock）和写锁（write lock）。 读锁：读锁之间是共享的，即读锁之间互不阻塞； 写锁：写锁之间是排他的，即一个写锁会阻塞其他写锁和读锁，注意不仅仅是写锁会被阻塞，读锁也会被阻塞！ 写锁比读锁的优先级高，即在一个锁队列中，一个写锁可能会插队到读锁前面，而读锁不能被插入到写锁前面，想想之前数据结构中学的优先队列。 1.2.2 锁粒度锁粒度：对正在修改的数据片进行锁定的精度，即对一条数据进行修改时，是做到仅锁住这条数据，还是锁住整个表。 锁策略：在锁的开销和数据的安全性之间寻求平衡。一般商业数据库是在表上加行级锁。 表锁：锁定整张表。开销最小， 行级锁：锁定独写的某一条数据。开销最大。 1.3 事务事务（Transaction）：是一组原子性的SQL查询，或者说一个独立的工作单元。 ACID： 原子性（atomicity） 一个事务必须被视为一个不可分割的最小单元，整个事务的操作要么全部提交成功，要么全部失败回滚，不能只执行其中的一部分操作，这就是事务的原子性。 一致性（consistency） 数据库总是从一个一致状态转换到另外一个一致状态。 有一个博客对一致性的解释比较容易理解： 一致性有下面特点： 如果一个操作触发辅助操作（级联，触发器），这些也必须成功，否则交易失败。 如果系统是由多个节点组成，一致性规定所有的变化必须传播到所有节点（多主复制）。如果从站节点是异步更新，那么我们打破一致性规则，系统成为“最终一致性”。 一个事务是数据状态的切换，因此，如果事务是并发多个，系统也必须如同串行事务一样操作。 在现实中，事务系统遭遇并发请求时，这种串行化是有成本的， Amdahl法则描述如下：它是描述序列串行执行和并发之间的关系。一个程序在并行计算情况下使用多个处理器所能提升的速度是由这个程序中串行执行部分的时间决定的。 大多数数据库管理系统选择（默认情况下）是放宽一致性，以达到更好的并发性。 事务的执行不能破坏数据库数据的完整性和一致性，一个事务在执行之前和执行之后，数据库都必须处于一致性状态。 隔离性（isolation） 隔离性是指，一个事务所做的修改在最终提交之前，对其他事务是不可见的（并不是完全不可见，有不同的隔离级别）。 持久性（durability） 事务一旦提交，其所作的修改就会永久保存到数据库中，此时即使系统奔溃，修改的数据也不会丢失。不可能有能做到100%持久性保证的策略。 1.3.1 隔离级别READ UNCOMMITTED（未提交读）、READ COMMITTED（提交读，也是不可重复读）、REPEATABLE READ（可重复读，是MySQL的默认事务隔离级别）、SERIALIZABLE（可串行化）。 这篇博文对隔离级别的讲解很到位（若原文链接失效，可参考我自己的复制内容），可以参考。书中都是文字性的讲解，不太好理解。额外要注意的一点就是SERIALIZABLE会在读的每一行上加锁。 1.3.2 死锁死锁：是指当两个或多个事务在同一资源上相互占用，请求锁定对方占用的资源，每个事务都在等待对方释放锁，从而等待状态永远不会结束形成恶性循环的现象。 书中这个例子很容易理解，如果凑巧两个事务的第一条SQL同时执行，则事务1锁定了stock_id=4的数据（假设叫做第1条数据），事务2锁定了stock_id=3的数据（假设叫做第2条数据），然后事务1等待事务2释放第2条数据的锁，而事务2要执行整个事务提交后才能释放数据2的锁，当时在执行第2条SQL时又要等事务1释放第1条数据的锁，这样就导致两个事务相互等待，且相互等待的状态永远不会结束，导致死锁。 数据库实现了各种死锁检测机制和死锁超时机制。当前InnoDB处理死锁的方法是，将持有最少行级排他锁的事务进行回滚。（书中原文） 死锁的产生有些还和存储引擎的实现方式有关。 1.3.3 事务日志使用事务日志，存储引擎在修改表的数据时，只需要修改其内存拷贝，再把该修改行为记录到持久在硬盘的事务日志中。可以理解为事务提交后并不会直接修改硬盘中的数据，而是先把事务日志写到硬盘的事务日志中，然后让数据在后台慢慢修改到硬盘中。通常称这种日志为预写式日志，修改数据要写入两次数据到硬盘中，第一次是日志，第二次才是数据根据日志进行修改。系统奔溃时，也可以通过硬盘中的事务日志进行数据恢复。 1.3.4 MySQL中的事务如果不是显示地使用start transaction，则默认为每一个SQL语句都是一个事务。MySQL默认采用自动提交事务（autocommit）的方式。 12345678910mysql&gt; SHOW VARIABLES LIKE &#x27;AUTOCOMMIT&#x27;;+---------------+-------+| Variable_name | Value |+---------------+-------+| autocommit | ON |+---------------+-------+1 row in set, 1 warning (0.01 sec)mysql&gt; SET AUTOCOMMIT = 0; -- 显示的将该线程下的提交方式修改为手动提交，即要手动使用COMMIT或ROLLBACKQuery OK, 0 rows affected (0.00 sec) 注意！某些命令在执行前会强制执行COMMIT提交掉当前事务的修改，如ALTER TABLE、LOCK TABLES等。 12345mysql&gt; SET TRANSACTION ISOLATION LEVEL READ COMMITTED; -- 设置下一个事务的隔离级别，可以用在存储过程的start transaction之前。Query OK, 0 rows affected (0.00 sec)mysql&gt; SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED; -- 设置当前会话事务的隔离级别Query OK, 0 rows affected (0.00 sec) 虽然InnoDB会根据事务隔离级别自动锁定修改，但MySQL也支持显式锁定表：LOCK TABLES。但本书建议，只有在使用set transaction = 0的事务中可以使用LOCK TABLES，其他情况应该避免使用使用LOCK TABLES。 1.4 多版本并发控制（MVCC）可以认为MVCC是行级锁的一个变种，但是它很多情况下避免了枷锁操作，虽然实现机制有所不同，但大都实现了非阻塞的读操作，写操作也只锁定必要的行。 下面是本书中通过InnoDB的简化版行为来说明MVCC是如何工作的。 1.5 MySQL的存储引擎 InnoDB引擎：支持事务 MyISAM引擎：不支持事务和行级锁，但读取效率高 Archive：不支持引擎，支持行级锁，插入效率高 CSV引擎：用作数据交换很有用 Memory引擎：不支持行级锁，支持hash索引，查找效率特别高，比MyISAM效率还更高，数据保存在内存中的形式 中间省略大量内容… 如果需要对记录的日志做分析报表，则生成报表的SQL可能会导致日志插入效率下降，怎么办？一种办法是，利用MySQL内置的复制方案，复制一份到从库，然后在从库做查询操作，主库只用于高效的插入操作，因此从库的查询操作就不会影响到主库的插入性能。","categories":[{"name":"mysql","slug":"mysql","permalink":"https://yury757.github.io/categories/mysql/"},{"name":"高性能MySQL","slug":"mysql/高性能MySQL","permalink":"https://yury757.github.io/categories/mysql/%E9%AB%98%E6%80%A7%E8%83%BDMySQL/"}],"tags":[]},{"title":"第二章-MySQL基准测试","slug":"database/mysql/《高性能MySQL》/第2章-MySQL基准测试/第二章 MySQL基准测试","date":"2020-11-30T16:00:00.000Z","updated":"2022-10-07T12:46:03.228Z","comments":true,"path":"database/mysql/《高性能MySQL》/第2章-MySQL基准测试/第二章 MySQL基准测试/","link":"","permalink":"https://yury757.github.io/database/mysql/%E3%80%8A%E9%AB%98%E6%80%A7%E8%83%BDMySQL%E3%80%8B/%E7%AC%AC2%E7%AB%A0-MySQL%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20MySQL%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/","excerpt":"","text":"基准测试：是针对系统设计的一种压力测试。 2.1 为什么需要基准测试基准测试可以观察系统中在不同压力下的行为。有很多用处的可测试点，详见书中。 2.2 基准测试的策略略 2.2.1 测试何种指标 吞吐量：是指单位时间内事务的处理数。 响应时间或延迟：用于测试任务所需的整体时间。根据不同的时间单位可以计算所需的平均响应时间、最小响应时间、最大响应时间的所占百分比，或者将每次响应时间制作成频率分布图、折线图等图表进行分析。 并发性：数据库的并发性和web服务器的并发性有些差异，web服务器的并发性指的是可以一段时间内同时处理多少web请求，而数据库的并发性测试，主要关注于，当并发事务增加时，需要测试的吞吐量是否下降，响应时间是否变长。 可扩展性：指的是给系统增加一倍的工作，在理想情况下可以使吞吐量增加一倍；或者说给系统增加一倍的资源，就可以获得两倍的吞吐量。（这是最理想情况，现实中几乎达不到，即边际效益下降） BENCHMARK(count, expr)函数：用于计算表达式expr共count次，可以用来计时MySQL处理表达式的速度。结果值为0，对于不适当的参数（例如NULL或负重复计数），则为NULL。因此SELECT的结果不重要，而是要看执行花费的时间。下面这个例子可以发现MD5算法比SHA1算法速度更快。 123456789101112131415mysql&gt; SELECT BENCHMARK(1000000, MD5(&#x27;test&#x27;));+---------------------------------+| BENCHMARK(1000000, MD5(&#x27;test&#x27;)) |+---------------------------------+| 0 |+---------------------------------+1 row in set (0.14 sec)mysql&gt; SELECT BENCHMARK(1000000, SHA1(&#x27;test&#x27;));+----------------------------------+| BENCHMARK(1000000, SHA1(&#x27;test&#x27;)) |+----------------------------------+| 0 |+----------------------------------+1 row in set (0.47 sec) http_load：web服务器测试工具，略。 2.5 基准测试案例略。有需要的可以参考书中的测试案例去做基准测试。","categories":[{"name":"mysql","slug":"mysql","permalink":"https://yury757.github.io/categories/mysql/"},{"name":"高性能MySQL","slug":"mysql/高性能MySQL","permalink":"https://yury757.github.io/categories/mysql/%E9%AB%98%E6%80%A7%E8%83%BDMySQL/"}],"tags":[]},{"title":"第四章-Schema与数据类型优化","slug":"database/mysql/《高性能MySQL》/第4章-Schema与数据类型优化/第四章-Schema与数据类型优化","date":"2020-11-30T16:00:00.000Z","updated":"2022-10-07T12:46:20.936Z","comments":true,"path":"database/mysql/《高性能MySQL》/第4章-Schema与数据类型优化/第四章-Schema与数据类型优化/","link":"","permalink":"https://yury757.github.io/database/mysql/%E3%80%8A%E9%AB%98%E6%80%A7%E8%83%BDMySQL%E3%80%8B/%E7%AC%AC4%E7%AB%A0-Schema%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%BC%98%E5%8C%96/%E7%AC%AC%E5%9B%9B%E7%AB%A0-Schema%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%BC%98%E5%8C%96/","excerpt":"","text":"4.1 选择优化的数据类型1、应该尽量使用可以正确存储存储数据的最小数据类型。如能使用tinyint就不要使用int，能使用varchar(20)，就不要使用varchar(100)。 2、应该尽量使用尽可能简单的数据类型，如日期类型尽量使用date、time、datetime，而不是使用varchar存储日期数据，另外应该使用整形存储IP地址，而不是使用varchar。 3、应该尽量避免null，可以保证不出错的情况下尽量把列指定为not null。一方面在于在查询时，如果有null，则需要使用ifnull，而这样的话会使索引失效；另一方面，可为null的列会占用更多的存储空间。 4、int(11)通常是没有意义的；decimal(m,n)中m是指整数部分和小数部分位数之和，n则仅仅指小数部分位数。 5、财务数据建议用decimal存储，可以对小数部分进行精确计算，而在数据量比较大时，可以考虑使用BIGINT代替decimal，并根据需要存储的小数位乘以相应的倍数就行，如需要保留到万分之一，可以将原始数据乘以一百万，再存到bigint里面，因为bigint相比decimal的好处在于计算更精确和计算效率高。（但是这种方式要注意使用该金额时可能会出现忘记除掉相应的倍数的情况，个人建议还是使用decimal，更安全） 6、varchar和char： （1）varchar和char类型消耗的存储空间的字节数由其字符集决定，如使用utf8mb4时，英文字母和数字占用1个字节，而大部分中文占用3个字节，少量特殊字符占用4个字节。使用char_length()和length()可以查看字符串长度和字符串所占用的字节长度。 123456789mysql&gt; select test1, length(test1), char_length(test1) from test_varchar1;+--------+---------------+--------------------+| test1 | length(test1) | char_length(test1) |+--------+---------------+--------------------+| abc | 3 | 3 || 123 | 3 | 3 || 你好啊 | 9 | 3 |+--------+---------------+--------------------+3 rows in set (0.00 sec) （2）varchar(n)是变长，char(n)是定长，即varchar消耗的存储空间是随字符串长度而改变的，char消耗的存储空间是既定的。此外varchar还会消耗1-2个字节存储字符串的长度，而char不会。因此对于存储定长数据，使用char更好，因此不需要额外存储一个字节来保存字符串长度，但在实际业务中这种需求较少，反倒使用enum的都比使用char的多。 （3）char类型会删除末尾的空格再进行存储**（危！）**，而varchar不会删除末尾空格（实际上在MySQL4.1或更老的版本中varchar也会删末尾空格）。 12345678mysql&gt; select test2, test3, char_length(test2), char_length(test3), length(test2), length(test3), concat(&#x27;(&#x27;, test2, &#x27;)&#x27;), concat(&#x27;(&#x27;, test3, &#x27;)&#x27;) from test_varchar2; -- test2为varchar类型，而test3为char类型。+--------+-------+--------------------+--------------------+---------------+---------------+-------------------------+-------------------------+| test2 | test3 | char_length(test2) | char_length(test3) | length(test2) | length(test3) | concat(&#x27;(&#x27;, test2, &#x27;)&#x27;) | concat(&#x27;(&#x27;, test3, &#x27;)&#x27;) |+--------+-------+--------------------+--------------------+---------------+---------------+-------------------------+-------------------------+| ab | ab | 6 | 4 | 6 | 4 | ( ab ) | ( ab) || ab | ab | 6 | 4 | 6 | 4 | ( ab ) | ( ab) |+--------+-------+--------------------+--------------------+---------------+---------------+-------------------------+-------------------------+2 rows in set (0.00 sec) （4）当使用严格的SQL模式时，insert的数据超过varchar和char的最大长度时，都会报错；当启动非严格的SQL模式时，他们则是将超过长度后面的字符删除存储，并予以警告，而不是报错。 （5）最好的策略是根据业务需求选择最适合的类型，只分配真正需要的空间。 7、blob与text都是用于存储长度特别长（超过65535个字节）的数据类型，对他们的排序并不是对整个字符串进行排序，都是对其前max_sort_length个字节的字符进行排序，可以手动设置max_sort_length的值，或者使用order by sustring(column, length)。区别在于blob存储的是字符串的二进制，而text存储的是原始字符串。 8、enum类型会将“数字-字符串”映射关系的“查找表”存储于.frm文件中，而数据中只存储“数字”键，这种双重性容易导致混乱，特别是排序的时候，enum排序是使用内部存储的整数进行排序，而不是定义的字符串进行排序。除非使用FIELD()函数自定义排序顺序。列关联时的效率：enum关联enum &gt; varchar关联varchar &gt; enum和varchar互相关联 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748mysql&gt; create table `enum1`(column1 enum(&#x27;Y&#x27;, &#x27;M&#x27;, &#x27;N&#x27;));Query OK, 0 rows affected (0.01 sec)mysql&gt; insert into enum1 values(&#x27;Y&#x27;), (&#x27;Y&#x27;), (&#x27;N&#x27;), (&#x27;M&#x27;);Query OK, 4 rows affected (0.00 sec)mysql&gt; select column1 from enum1;+---------+| column1 |+---------+| Y || Y || N || M |+---------+4 rows in set (0.00 sec)mysql&gt; select column1 + 1 from enum1;+-------------+| column1 + 1 |+-------------+| 2 || 2 || 4 || 3 |+-------------+4 rows in set (0.00 sec)mysql&gt; select column1 from enum1 order by column1;+---------+| column1 |+---------+| Y || Y || M || N |+---------+4 rows in set (0.00 sec)mysql&gt; select column1 from enum1 order by field(column1, &#x27;M&#x27;, &#x27;N&#x27;, &#x27;Y&#x27;);+---------+| column1 |+---------+| M || N || Y || Y |+---------+4 rows in set (0.00 sec) 9、datetime和timestamp 类型 占用字节数 支持的时间范围 datetime 8 1000-01-01 00:00:00至9999-12-31 23:59:59 timestamp 4 1970-01-01 00:00:00至2038-01-19 23:59:59 10、标识列（即能唯一标识一条数据的字段）数据类型通常用unsigned int auto_increment或UUID()两种：当该标识列索引使用BTREE（innodb默认使用BTREE）时，使用unsigned int auto_increment更好（io速度更快、存储空间更小等），当标识列索引使用hash索引（innodb不显式支持hash索引，但当支持自适应hash索引，等后面讲）时，两者差不多。 11、最好避免使用BIT和SET类型。 4.2 schema设计中的陷阱1、避免过多的列和过多的关联 2、避免NULL，或者使用其他值代替NULL 4.3 范式和反范式设计方面的东西，详见原文或者参考其他博客，如这里 4.4 缓存表和汇总表缓存表：表示存储那些可以比较简单的从schema其他表获取（但每次获取速度都比较慢）数据的表（例如，逻辑上冗杂的数据）。 汇总表：表示存储那些使用group by语句聚合的数据。 使用缓存表的情况是，比如展示一个很详细的业务数据，要关联很多张表并进行相关运算，每次查询速度都比较慢，则可以将定期查询该SQL并放到一张缓存表中，等需要的时候直接取这张缓存表中的数据即可，然后定时维护这张缓存表以更新数据。这种情况数据虽然有延迟，但对于用户来说能很快的看到数据。 使用汇总表的情况是，比如要看网站最近一个月每天的点击量，则要做group by操作，可以每天定时执行一个的SQL，将当天的点击量记录到这张汇总表中，等需要的时候直接where between就行，不用做group by。 物化视图：预先计算并存储在磁盘上的表，并通过各种策略来自动更新该表（视图）。MySQL可以用第三方工具：Justin Swanhart的Flexviews。 计数器表：再比如上面那个网站最近一个月每天的点击量的情况，可以通过定义一张下面所示的表，每次收到用户访问，就随机选一个槽进行更新（避免锁冲突）。再设置一个定时任务，每天将昨天的数据汇总到0号槽，并删除其他槽，这样就是一个统计每一天的访问量的计数器表。 12345678910111213141516171819202122232425262728293031323334353637383940CREATE TABLE `daily_click` ( `day` date NOT NULL, `slot` int unsigned NOT NULL, `cnt` int DEFAULT 0, PRIMARY KEY (`day`,`slot`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_cimysql&gt; insert into daily_click values(now(), round(rand(), 2) * 100, 1) on duplicate key update cnt = cnt + 1; -- 我这里执行了六次Query OK, 1 row affected, 1 warning (0.00 sec)mysql&gt; select * from daily_click;+------------+------+------+| day | slot | cnt |+------------+------+------+| 2020-11-22 | 22 | 1 || 2020-11-22 | 29 | 1 || 2020-11-22 | 48 | 1 || 2020-11-22 | 53 | 2 || 2020-11-22 | 95 | 1 |+------------+------+------+5 rows in set (0.00 sec)mysql&gt; update daily_click as c, (select day, sum(cnt) as cnt, min(slot) as slot from daily_click group by day) as x set c.cnt = if(c.slot = x.slot, x.cnt, 0), c.slot = if(c.slot = x.slot, 0, c.slot) where c.day = x.day and c.day = &#x27;2020-11-22&#x27;;Query OK, 5 rows affected (0.00 sec)Rows matched: 5 Changed: 5 Warnings: 0mysql&gt; delete from daily_click where day = &#x27;2020-11-22&#x27; and slot &lt;&gt; 0;Query OK, 4 rows affected (0.00 sec)mysql&gt; select * from daily_click;+------------+------+------+| day | slot | cnt |+------------+------+------+| 2020-11-22 | 0 | 6 |+------------+------+------+1 row in set (0.00 sec) 4.5 加快ALTER TABLE的速度1、修改列的三种方式： （1）ALTER TABLE tbl_name CHANGE [COLUMN] old_col_name new_col_name column_definition [FIRST | AFTER col_name]：这种方法是整列换成一个新列的定义，包括列名也可以修改，会引起表的重建，即删除旧列，构造新列； （2）ALTER TABLE tbl_name MODIFY [COLUMN] col_name column_definition [FIRST | AFTER col_name]：这种方法也是整列换成一个新列，但是不能修改表名，只能修改属性，也会引起表的重建； （3）ALTER TABLE tbl_name ALTER [COLUMN] col_name {SET DEFAULT {literal | (expr)} | DROP DEFAULT}：这种方法局限性很高，只能修改列的默认值属性，这个语句会直接修改表的.frm文件，不涉及表数据，不会引起表的重建，因此速度很快。 即如果是需要修改的东西实际存在于.frm文件中，都可以通过直接修改.frm文件来进行修改，而不用重建表。注意：《高性能MySQL》这本书对用的MySQL版本是8.0以下的版本，MySQL8.0及以后没有.frm文件了，表结构和表数据都在整个.ibd文件的表空间中。以上三种修改，第三种也是最快的，但是只是底层原理上有了较大的变化，详见MySQL官方文档。 2、修改表结构的技巧： （1）先在一台不提供服务的库上执行alter table操作，然后和提供服务的主库进行切换。过程（个人盲猜的）：停止从主备库同步，备用库执行alter table，重新同步主备库，待同步成功再切换主备库。问题：切换主备库会不会导致整个服务器停顿？ （2）影子拷贝。建一个新的空表，表结构为原表修改后的表结构，在新表中建三个INSERT UPDATE DELETE的触发器，将旧表数据拷贝到新表，最新数据会通过触发器更新过去，然后通过重命名表和删表的方式交换两张表。","categories":[{"name":"mysql","slug":"mysql","permalink":"https://yury757.github.io/categories/mysql/"},{"name":"高性能MySQL","slug":"mysql/高性能MySQL","permalink":"https://yury757.github.io/categories/mysql/%E9%AB%98%E6%80%A7%E8%83%BDMySQL/"}],"tags":[]},{"title":"第三章-服务器性能剖析","slug":"database/mysql/《高性能MySQL》/第3章-服务器性能剖析/第三章-服务器性能剖析","date":"2020-11-30T16:00:00.000Z","updated":"2022-10-07T12:46:11.810Z","comments":true,"path":"database/mysql/《高性能MySQL》/第3章-服务器性能剖析/第三章-服务器性能剖析/","link":"","permalink":"https://yury757.github.io/database/mysql/%E3%80%8A%E9%AB%98%E6%80%A7%E8%83%BDMySQL%E3%80%8B/%E7%AC%AC3%E7%AB%A0-%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%80%A7%E8%83%BD%E5%89%96%E6%9E%90/%E7%AC%AC%E4%B8%89%E7%AB%A0-%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%80%A7%E8%83%BD%E5%89%96%E6%9E%90/","excerpt":"","text":"（1）性能剖析（profiling）：用于测试服务器的时间花费在哪里。 本书对数据库性能的定义：是指完成某件任务所需要的时间度量，即响应时间，每个SQL语句的查询话费的时间。 完成一项任务所需要的时间可以分成两部分：执行时间和等待时间（如IO等待）。 （2）性能剖析有两个步骤： 测量任务所花费的时间； 对结果进行统计分析，均值、方差、分布、计算特定的指标分析等。 （3）性能剖析本身会导致服务器变慢吗？ 当然会，因为要做额外的工作付出额外的开销。但是如果性能剖析可以帮助应用程序运行得更快，那么性能剖析就使值得的。 （4）慢查询日志和通用日志 慢查询日志是指捕获所有查询时间超过long_query_time的查询，将其SQL语句和消耗时间记录到日志中。可以设置long_query_time = 0来捕获所有SQL。慢查询日志是开销最低、精度最高的测量查询时间的工具。 通用日志是指查询请求到服务器时记录，不包含响应时间和执行计划等重要信息。 将慢查询日志生成剖析报告的工具：pt-query-digest（十年前给力，2020年需根据实际情况使用更好的工具）。 （5）SHOW PROFILES SHOW PROFILES 命令可以对每条SQL进行性能剖析，默认是禁用的，使用set profiling = 1打开对当前线程的SQL进行剖析。 SHOW PROFILE FOR QUERY N 命令可以展示记录的id为N的查询每个执行步骤的详细时间记录。 也可以查询每个查询的所有详细步骤信息：SELECT * FROM INFORMATION_SCHEMA.FROFILING; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102mysql&gt; set profiling = 1;Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; select 1;+---+| 1 |+---+| 1 |+---+1 row in set (0.00 sec)mysql&gt; show profiles;+----------+------------+----------+| Query_ID | Duration | Query |+----------+------------+----------+| 1 | 0.00043725 | select 1 |+----------+------------+----------+1 row in set, 1 warning (0.00 sec)mysql&gt; select 2;+---+| 2 |+---+| 2 |+---+1 row in set (0.00 sec)mysql&gt; show profiles;+----------+------------+----------+| Query_ID | Duration | Query |+----------+------------+----------+| 1 | 0.00043725 | select 1 || 2 | 0.00042800 | select 2 |+----------+------------+----------+2 rows in set, 1 warning (0.00 sec)mysql&gt; set profiling = 0;Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; select 3;+---+| 3 |+---+| 3 |+---+1 row in set (0.00 sec)mysql&gt; show profiles;+----------+------------+----------+| Query_ID | Duration | Query |+----------+------------+----------+| 1 | 0.00043725 | select 1 || 2 | 0.00042800 | select 2 |+----------+------------+----------+2 rows in set, 1 warning (0.00 sec)mysql&gt; show profile for query 1;+----------------------+----------+| Status | Duration |+----------------------+----------+| starting | 0.000116 || checking permissions | 0.000007 || Opening tables | 0.000013 || init | 0.000005 || optimizing | 0.000008 || executing | 0.000051 || end | 0.000004 || query end | 0.000005 || closing tables | 0.000002 || freeing items | 0.000043 || cleaning up | 0.000186 |+----------------------+----------+11 rows in set, 1 warning (0.00 sec)mysql&gt; select * from information_schema.profiling;+----------+-----+----------------------+----------+----------+------------+-------------------+---------------------+--------------+---------------+---------------+-------------------+-------------------+-------------------+-------+-----------------------+----------------------+-------------+| QUERY_ID | SEQ | STATE | DURATION | CPU_USER | CPU_SYSTEM | CONTEXT_VOLUNTARY | CONTEXT_INVOLUNTARY | BLOCK_OPS_IN | BLOCK_OPS_OUT | MESSAGES_SENT | MESSAGES_RECEIVED | PAGE_FAULTS_MAJOR | PAGE_FAULTS_MINOR | SWAPS | SOURCE_FUNCTION | SOURCE_FILE | SOURCE_LINE |+----------+-----+----------------------+----------+----------+------------+-------------------+---------------------+--------------+---------------+---------------+-------------------+-------------------+-------------------+-------+-----------------------+----------------------+-------------+| 1 | 2 | starting | 0.000116 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL || 1 | 3 | checking permissions | 0.000007 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | check_access | sql_authorization.cc | 2203 || 1 | 4 | Opening tables | 0.000013 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | open_tables | sql_base.cc | 5590 || 1 | 5 | init | 0.000005 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Sql_cmd_dml::execute | sql_select.cc | 662 || 1 | 6 | optimizing | 0.000008 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | JOIN::optimize | sql_optimizer.cc | 217 || 1 | 7 | executing | 0.000051 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | JOIN::exec | sql_executor.cc | 227 || 1 | 8 | end | 0.000004 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Sql_cmd_dml::execute | sql_select.cc | 715 || 1 | 9 | query end | 0.000005 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | mysql_execute_command | sql_parse.cc | 4547 || 1 | 10 | closing tables | 0.000002 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | mysql_execute_command | sql_parse.cc | 4593 || 1 | 11 | freeing items | 0.000043 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | mysql_parse | sql_parse.cc | 5264 || 1 | 12 | cleaning up | 0.000186 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | dispatch_command | sql_parse.cc | 2159 || 2 | 2 | starting | 0.000130 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL || 2 | 3 | checking permissions | 0.000011 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | check_access | sql_authorization.cc | 2203 || 2 | 4 | Opening tables | 0.000023 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | open_tables | sql_base.cc | 5590 || 2 | 5 | init | 0.000009 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Sql_cmd_dml::execute | sql_select.cc | 662 || 2 | 6 | optimizing | 0.000009 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | JOIN::optimize | sql_optimizer.cc | 217 || 2 | 7 | executing | 0.000012 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | JOIN::exec | sql_executor.cc | 227 || 2 | 8 | end | 0.000004 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Sql_cmd_dml::execute | sql_select.cc | 715 || 2 | 9 | query end | 0.000006 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | mysql_execute_command | sql_parse.cc | 4547 || 2 | 10 | closing tables | 0.000094 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | mysql_execute_command | sql_parse.cc | 4593 || 2 | 11 | freeing items | 0.000112 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | mysql_parse | sql_parse.cc | 5264 || 2 | 12 | cleaning up | 0.000019 | 0.000000 | 0.000000 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | dispatch_command | sql_parse.cc | 2159 |+----------+-----+----------------------+----------+----------+------------+-------------------+---------------------+--------------+---------------+---------------+-------------------+-------------------+-------------------+-------+-----------------------+----------------------+-------------+22 rows in set, 1 warning (0.00 sec) （6）SHOW STATUS SHOW STATUS 命令可以查看计数器统计到的值，如统计了查询的次数、使用某个索引的次数。虽然无法展示使用时间，但还是可以通过观察哪些操作执行的更频繁。最有用的计数器有：句柄计数器（handler counter）、临时文件和表计数器等。 123456789101112131415161718192021222324252627mysql&gt; SHOW STATUS WHERE VARIABLE_NAME LIKE &#x27;Handler%&#x27; OR VARIABLE_NAME LIKE &#x27;Created%&#x27;;+----------------------------+-------+| Variable_name | Value |+----------------------------+-------+| Created_tmp_disk_tables | 0 || Created_tmp_files | 5 || Created_tmp_tables | 3 || Handler_commit | 3 || Handler_delete | 0 || Handler_discover | 0 || Handler_external_lock | 90 || Handler_mrr_init | 0 || Handler_prepare | 0 || Handler_read_first | 0 || Handler_read_key | 3 || Handler_read_last | 0 || Handler_read_next | 0 || Handler_read_prev | 0 || Handler_read_rnd | 0 || Handler_read_rnd_next | 58 || Handler_rollback | 0 || Handler_savepoint | 0 || Handler_savepoint_rollback | 0 || Handler_update | 0 || Handler_write | 55 |+----------------------------+-------+21 rows in set (0.00 sec) FLASH STATUS; – 该命令作用是重置计数器 通过重置计数器，并测试一条SQL，就可以获取这条SQL的执行时各个计数器的值。EXPLAIN 得到的是估计的结果，而通过 SHOW STATUS 统计计数器的值测量到的是实际发生的结果。 （7）间歇性问题 举例：一个SQL查询正常情况下很快，但是有几次不合理的特别慢，手工执行下也很快，EXPLAIN 和 SHOW STATUS 中都正确使用了索引，尝试修改WHERE 条件排除缓存命中的可能，也没有什么问题，这种情况下很难查出来是什么原因导致这几次SQL执行很慢，可能是正在备份，或某种类型的锁，或争用阻塞了SQL的进度等，这种问题就是间歇性问题。书中对间歇性问题的建议是不要花费太多时间去找出问题所在，更不能用试错的方式来解决问题，可以通过以下思路进行尝试： 先确定到底是服务器问题还是单条SQL的问题 使用SHOW GLOBAL STATUS 使用SHOW PROCESSLIST 使用慢查询日志 以上两个方法都是通过不停的捕获命令的输出，观察异常值等其他特征，来进行诊断。第三个方法是查看慢查询日志中吞吐量突然下降时间段结束后的那一部分日志，再具体分析哪个SQL导致缓慢。以上三个方法在书中都有具体例子，有需要可以自行看书。书中建议从第1、2两种方法开始查找问题，这两个方法开销最低。 有必要再捕获诊断数据 触发器：根据实际情况设计触发指标和触发条件，使用合理工具（如pt-talk和pt-collect）收集数据给触发器去判断。 （8）一个诊断案例 略，有需要可以自行看书。 （9）使用information_schema 稍后整理一个information_schema库的专题讲解。","categories":[{"name":"mysql","slug":"mysql","permalink":"https://yury757.github.io/categories/mysql/"},{"name":"高性能MySQL","slug":"mysql/高性能MySQL","permalink":"https://yury757.github.io/categories/mysql/%E9%AB%98%E6%80%A7%E8%83%BDMySQL/"}],"tags":[]},{"title":"第五章-创建高性能的索引","slug":"database/mysql/《高性能MySQL》/第5章-创建高性能的索引/第五章-创建高性能的索引","date":"2020-11-30T16:00:00.000Z","updated":"2022-10-07T12:46:29.928Z","comments":true,"path":"database/mysql/《高性能MySQL》/第5章-创建高性能的索引/第五章-创建高性能的索引/","link":"","permalink":"https://yury757.github.io/database/mysql/%E3%80%8A%E9%AB%98%E6%80%A7%E8%83%BDMySQL%E3%80%8B/%E7%AC%AC5%E7%AB%A0-%E5%88%9B%E5%BB%BA%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E7%B4%A2%E5%BC%95/%E7%AC%AC%E4%BA%94%E7%AB%A0-%E5%88%9B%E5%BB%BA%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E7%B4%A2%E5%BC%95/","excerpt":"","text":"〇、索引的本质索引本质就是一个排好序的数据结构，实现这种快速排序、插入、查找等操作的数据结构有：平衡二叉树、红黑树、多路查找树（其中平衡的多路查找树就是B-Tree）等。 一、索引的类型1、B-Tree索引B-Tree，即平衡的多路查找树。具体解释参考其他文章。B-Tree适用于全键值、键值范围、键前缀查找。匹配原则如下（只列出了比较重要的）： 匹配最左前缀：where条件中的字段会先匹配索引最左边的字段，如果匹配则适用该索引，并依次匹配后面的字段，直到无效为止，前面匹配到的字段依然适用索引；若一开始最左边的字段就不匹配，则不适用于该索引。 匹配列前缀：在满足最左前缀的前提下，可以匹配某一列的值的开头部分。如某一个索引字段为（A, B）两个字段，如以下where都是可以使用该索引的。 123WHERE A = &#x27;a&#x27; AND B LIKE &#x27;b%&#x27;WHERE A LIKE &#x27;a%&#x27; AND B = &#x27;b&#x27;WHERE A LIKE &#x27;a%&#x27; AND B LIKE &#x27;b%&#x27; 匹配范围值：这里所说的范围查询包括between、in、&gt;、&lt;。 精确匹配前一列，并范围匹配后面的列。 只访问索引的查询，即索引覆盖。 order by也可以使用索引，且where字段可以和order by字段拼接来匹配索引。 2、hash索引在MySQL中，只有memory引擎显示支持hash索引。hash索引也是memory引擎的默认索引类型。hash索引有以下特征： 对索引字段使用=，&lt;=&gt;，IN速度特别快，但不支持&gt;和&lt;。 无法使用hash索引来加快order by的速度，因为其不是按照哈希值顺序存储的。 无法使用like匹配前缀来加快查询速度。 hash索引只存储哈希值和行指针，不存储字段值。 当出现hash冲突时，必须遍历冲突值来获取匹配结果。 自适应哈希索引InnoDB引擎有一个特殊的功能叫做“自适应哈希索引（adaptive hash index)”。当InnoDB注意到某些索引值被使用得非常频繁时，它会在内存中基于B-Tree索引之上再创建一个哈希索引，这样就让B-Tree索引也具有哈希索引的一些优点，比如快速的哈希查找。这是一个完全自动的、内部的行为，用户无法控制或者配置，不过如果有必要，完全可以关闭该功能。 自定义哈希索引可以通过自定义哈希函数，将对应的字段值通过通过一个自定义哈希函数计算出哈希值列，将这些哈希值额外存在一个字段中。 这样的好处就是，当需要检索的那些字段很长时，需要的索引也很大，性能较低，而哈希值很短，索引也很小，性能较高。查询时需要查询匹配值的哈希值和匹配值两个字段，哈希值快速缩小范围，当出现冲突时，使用匹配值精确匹配。 12345678910111213141516171819202122CREATE TABLE pseudohash( id int unsigned NOT NULL auto_increment, url varchar(255) NOT NULL, url_crc int unsigned NOT NULL DEFAULT 0, primary key(id));DELIMITER //CREATE TRIGGER pseudohash_crc_ins BEFORE INSERT ON pseudohash FOR EACH ROW BEGINSET NEW.url_crc = crc32(NEW.url);END;//CREATE TRIGGER pseudohash_crc_upd BEFORE UPDATE ON pseudohash FOR EACH ROW BEGINSET NEW.url_crc = crc32(NEW.url);END;//DELIMITER ;insert into pseudohash(url) values (&#x27;http://www.mysql.com&#x27;);select * from pseudohash where url_crc = CRC32(&#x27;http://www.mysql.com&#x27;) and url = &#x27;http://www.mysql.com&#x27;; 3、空间数据索引（R-Tree）MyISAM引擎支持R-Tree，可以用于存储空间相关数据，如地理数据。 4、全文索引（FULLTEXT）全文索引是一种特殊类型的索引，它查找的是文本中的关键词，而不是直接比较索引中的值，更类似于搜索引擎做的事。 简单介绍几个全文索引的特点，有需要去查看官方文档或其他文章。 只能构建在char、varchar、text类型上 通过全文索引的查询有自己特殊的语法match(index_column) again(&#39;xxxx&#39;) 全文索引的检索有最小搜索长度和最大搜索长度限制（可以通过修改my.ini修改配置），表的行数量条件要求，以及各种殷勤和版本限制等。 二、索引的优点 大大减少了服务器需要扫描的数据量 帮助服务器避免了排序或临时表 将随机IO变为顺序IO 额外的IO知识 IO请求的时间主要有三块：寻道时间、旋转延迟、数据传输时间。寻道时间 &gt; 旋转延迟 &gt; 数据传输时间。 顺序IO是指读写操作的访问地址连续，寻址时间极大减少，性能高。随机IO是指读写操作访问地址不连续，磁头需要不停的移动，时间都浪费在磁头寻址上了，所以性能较差。 详见：https://tech.meituan.com/2017/05/19/about-desk-io.html 三、高性能的索引策略1、提高索引的选择性索引的选择性，是指不重复的索引值和表的总记录数的比值，即count(distinct column) / count(1)的值。 索引的选择性越高，则查询效率越高，因为选择性高的索引可以让MySQL在查询时过滤掉更多的行。对应的可以在explain中查看filtered字段，filtered越高说明通过索引过滤掉的行越多，索引效率越高。 在char、varchar、text上可以构建前缀索引，即指定长度来对字符串的前几个字符构建索引。text必须使用前缀索引，varchar可以不指定前缀长度。然而varchar很长时，会导致索引很大效率低，构建一个较短的前缀索引还是有必要的。关键在于，要使前缀索引对记录的过滤率（选择性）尽可能向完整长度索引对记录的过滤率（选择性）靠近。这样既达到了提高效率节省空间的作用，还实现了高效的数据查询。 123456789SELECT COUNT(DISTINCT city)/COUNT(*) FROM sakila.city_demo;SELECTCOUNT(DISTINCT LEFT(city, 3))/COUNT(*) AS sel3，COUNT(DISTINCT LEFT(city, 4))/COUNT(*) AS sel4，COUNT(DISTINCT LEFT(city, 5))/COUNT(*) AS sel5，COUNT(DISTINCT LEFT(city, 6))/COUNT(*) AS sel6，COUNT(DISTINCT LEFT(city, 7))/COUNT(*) AS sel7FROM sakila.city_demo； 2、多列索引应该构建合适的多列索引，而不是对每一列构建一个索引。 3、索引顺序将选择性最高的索引放在索引最前列。 4、聚簇索引聚簇索引：在B+Tree的基础上，每个叶子节点不仅有索引字段值，还有该索引字段对应的实际的数据行。因为无法把数据行放在两个不同的地方，因此一个表只能有一个聚簇索引，实际上聚簇索引“就是”表本身。聚簇索引并不是一种基本索引类型，而是一种数据分布策略。 对于InnoDB，如果有主键，则主键就是聚簇索引；如果未定义主键，则会使用unique键字段作为聚簇索引；如果都没有，则会隐式定义一个主键作为聚簇索引。 二级索引定位行需要两次查找，第一次定位到二级索引上保存的即聚簇索引键，第二次通过聚簇索引键定位到实际的行数据。 5、InnoDB和MyISAM的数据分布对比（1）MyISAM 不支持聚簇索引，数据是按照插入顺序存储在磁盘中的。 在数据之外还有一个行号，用于查找该行的指针。 主键索引和二级索引没有本质区别，分布方式都是把索引列排序后加上行号，因此无论是主键索引还是二级索引，都要通过两次寻址才能获取数据。 （2）InnoDB 数据是按照主键大小排序插入，若没有设置主键，则会给定一个隐式自增主键。这个主键代替MyISAM的行号。 支持聚簇索引，聚簇索引已经包含了整个表的数据，且还包含了事务ID、用于事务和MVCC的回滚指针等其他数据。 二级索引只有索引列和主键列。因此通过主键查找行只要一次寻址，而通过二级索引还是需要两次寻址。 （3）Tips在IO密集的应用中，最好避免使用随机的主键（如UUID）来做聚簇索引的主键，原因如下：。 每次生成的UUID不一定比之前的大，所以InnoDB无法简单地将新行插入到索引的最后面，要找到合适的位置，就会导致大量的随机IO 因为写入是乱序的，InnoDB不得不频繁地做页分裂操作，以便为新的行分配空间。页分裂会导致移动大量数据，一次插入最少需要修改三个页而不是一个页。 由于频繁的页分裂，页会变得稀疏并被不规则地填充，所以最终数据会有碎片。 （4）auto_increment的缺点在高并发的情况下，在InnoDB中按主键顺序插入可能会造成明显的争用。一方面主键的上界会成为热点，因为所有的插入操作都在这里，高并发可能会导致间隙锁（简单来说就是主键键值之间的距离的锁，该锁可以阻塞insert操作，防止幻读）竞争。另一方面，auto_increment的锁机制也是一个热点。如果遇到插入争用问题，则可能需要考虑重新设计表结构或者应用，或者更改innodb_autoinc_lock_mode配置。 6、覆盖索引定义：如果一个索引包含（或者说覆盖）所有需要查询的字段，则称这个情况为“覆盖索引”。 覆盖索引是针对某一个sql语句来判断的，所以只能称之为一种特殊情况。 出现“覆盖索引”时，查询数据不需要回表，因为索引中已经有了我们需要的数据，因此覆盖索引的效率很高。 7、使用索引来优化排序Tips 1出现“覆盖索引”时，在explain中的extra字段会出现“using index”值。而explain中的type字段中的“index”意思是访问数据的类型，例如全表扫描、范围扫描、索引扫描、常数引用等，对于索引扫描，意思是，访问数据只需要扫描索引，不需要回表查找数据。 Tips 2如果查询需要关联多张表，则只有当order by子句引用的字段都是第一张表时，才能使用索引进行排序。","categories":[{"name":"mysql","slug":"mysql","permalink":"https://yury757.github.io/categories/mysql/"},{"name":"高性能MySQL","slug":"mysql/高性能MySQL","permalink":"https://yury757.github.io/categories/mysql/%E9%AB%98%E6%80%A7%E8%83%BDMySQL/"}],"tags":[]},{"title":"MySQL备份-Linux版","slug":"database/mysql/MySQL备份-Linux版/MySQL备份-linux版","date":"2020-11-30T16:00:00.000Z","updated":"2022-10-07T12:46:44.454Z","comments":true,"path":"database/mysql/MySQL备份-Linux版/MySQL备份-linux版/","link":"","permalink":"https://yury757.github.io/database/mysql/MySQL%E5%A4%87%E4%BB%BD-Linux%E7%89%88/MySQL%E5%A4%87%E4%BB%BD-linux%E7%89%88/","excerpt":"","text":"一、该文档适用版本MySQL：8.0.22 Linux：18.04.5 LTS 二、MySQL在Linux上各种的默认路径 MySQL变量 路径 解释 datadir /var/lib/mysql 存放数据文件、log_bin日志文件等 log-error /var/log/mysql/error.log 错误日志文件 /etc/mysql 存放MySQL配置文件 basedir /usr/ character_sets_dir /usr/share/mysql-8.0/charsets/ 字符集路径 general_log_file /var/lib/mysql/yuyr757.log lc_messages_dir /usr/share/mysql-8.0/ log_bin_basename /var/lib/mysql/binlog bin_log日志文件路径和文件名前缀 log_bin_index /var/lib/mysql/binlog.index bin_log日志文件索引 log_error /var/log/mysql/error.log 报错日志 plugin_dir /usr/lib/mysql/plugin/ 插件路径 pid_file /var/run/mysqld/mysqld.pid relay_log_basename /var/lib/mysql/yuyr757-relay-bin relay_log_index /var/lib/mysql/yuyr757-relay-bin.index secure_file_priv /var/lib/mysql-files/ slow_query_log_file /var/lib/mysql/yuyr757-slow.log socket /var/run/mysqld/mysqld.sock sql_log_off OFF tmpdir /tmp 12345678/usr/bin 客户端程序和脚本 /usr/sbin mysqld 服务器 /var/lib/mysql 日志文件，数据文件/usr/share/doc/packages 文档 /usr/include/mysql 包含( 头) 文件 /usr/lib/mysql 库 /usr/share/mysql 错误消息和字符集文件 /usr/share/sql-bench 基准程序 三、热备份（不需要关闭服务器）1、mysqldumpmysqldump用法： 123456Usage: mysqldump [OPTIONS] database [tables]OR mysqldump [OPTIONS] --databases [OPTIONS] DB1 [DB2 DB3...]OR mysqldump [OPTIONS] --all-databases [OPTIONS]Default options are read from the following files in the given order:/etc/my.cnf /etc/mysql/my.cnf ~/.my.cnf [OPTIONS]参数很多，和mysql命令参数差不多，有需要自行查看mysqldump --help，以下讲解常用参数。 1234567891011121314-u, --user：用户名-p, --password：密码，该参数不能带值-h, --host：服务器地址-P, --port：端口-A, --all-databses：所有数据库-S, --socket：用来连接服务器的socket文件-B, --databses：指定要导出的数据库，默认情况下生成的sql文件是不带create database if not exists语句的，加上--database就会有创建数据库的语句。-R, --routines：导出数据库中的procedures和functions--triggers：导出对应表的触发器-E, --events：导出所有事件-w, --where：按照指定where条件下的数据--single-transaction：通过开启一个事务导出所有表来创建一致的快照（个人理解为导出的一瞬间创建一个快照，然后导出的数据都是基于那个快照版本，可以保持数据一致性）。仅适用于存储在支持多版本的存储引擎中的表（当前仅InnoDB支持）；-d, --no-data：不导出数据-t, --no-create-info：不重建数据库和表结构，即不输出drop database并create database，drop table并create table语句 下面是对一个test数据库的全备份 1mysqldump -u root -p --single-transaction --routines --events --triggers --databases test &gt; /tmp/test.sql 2、select into outfile注意：如果secure_file_priv变量不为空，则outfile的文件只能导出到那个变量的文件夹。 12345678910# 在一个正常的select语句结束，加上以下语句到sql语句结尾，即可将select出来的数据导出到文件INTO OUTFILE &#x27;/tmp/employee_data_1.txt&#x27;FIELDS TERMINATED BY &#x27;,&#x27; OPTIONALLY ENCLOSED BY &#x27;&quot;&#x27; ESCAPED BY &#x27;\\&#x27; LINES TERMINATED BY &#x27;\\n&#x27;;FIELDS TERMINATED BY # 字段分隔符ENCLOSED BY # 字段引用符，即字段左右两端会加上符号。OPTIONALLY即选择性，int和decimal不会被加上引用符，其他会。LINES TERMINATED BY # 行分隔符ESCAPED BY # 转义符，当文本中的符号和这里设置的符号冲突时，输出到csv时会自动给文本加上转义符，即表示转义符后面的这个符号是纯文本，不用做字段分隔，或字段引用，或行分隔符，或转义作用。在Linux中，显示加上ESCAPED BY &#x27;\\&#x27;时，sql语句没有任何响应。显示加上显示加上ESCAPED BY &#x27;\\\\&#x27;会报错。很迷，无所谓，反正默认为&#x27;\\&#x27;。 示例1（以下\\t和\\n都是手敲进去的，实际上查数据时\\t和\\n都会显示为缩进和换行）： 12345678910111213141516171819202122232425262728mysql&gt; select * from test2;+----+---------+------------+------------------------------+-------------------+| id | value | date | text | decimal_value |+----+---------+------------+------------------------------+-------------------+| 1 | &#x27;sfas1&#x27; | 2021-01-17 | sdfasdf&quot;asdf\\tasdf&quot;as,df\\as\\nf | 436526.7780000000 || 2 | sfsa2 | 2021-01-17 | sdfasdf&quot;asdf\\tasdf&quot;as,df\\as\\nf | 436526.7780000000 || 3 | sfsa3 | 2021-01-17 | sdfasdf&quot;asdf\\tasdf&quot;as,df\\as\\nf | 436526.7780000000 || 4 | NULL | 2021-01-17 | sdfasdf&quot;asdf\\tasdf&quot;as,df\\as\\nf | 436526.7780000000 || 5 | _abcd_ | 2021-01-17 | sdfasdf&quot;asdf\\tasdf&quot;as,df\\as\\nf | 436526.7780000000 |+----+---------+------------+------------------------------+-------------------+5 rows in set (0.00 sec)mysql&gt; select * from test2 into outfile &#x27;/var/lib/mysql-files/test.csv&#x27; FIELDS TERMINATED BY &#x27;,&#x27; OPTIONALLY ENCLOSED BY &#x27;&quot;&#x27; lines terminated by &#x27;\\n&#x27;;Query OK, 5 rows affected (0.00 sec)vim /var/lib/mysql-files/test.csv# 数据如下：1,&quot;&#x27;sfas1&#x27;&quot;,&quot;2021-01-17&quot;,&quot;sdfasdf\\&quot;asdf asdf\\&quot;as,df\\\\as\\f&quot;,436526.77800000002,&quot;sfsa2&quot;,&quot;2021-01-17&quot;,&quot;sdfasdf\\&quot;asdf asdf\\&quot;as,df\\\\as\\f&quot;,436526.77800000003,&quot;sfsa3&quot;,&quot;2021-01-17&quot;,&quot;sdfasdf\\&quot;asdf asdf\\&quot;as,df\\\\as\\f&quot;,436526.77800000004,\\N,&quot;2021-01-17&quot;,&quot;sdfasdf\\&quot;asdf asdf\\&quot;as,df\\\\as\\f&quot;,436526.77800000005,&quot;_abcd_&quot;,&quot;2021-01-17&quot;,&quot;sdfasdf\\&quot;asdf asdf\\&quot;as,df\\\\as\\f&quot;,436526.7780000000 解释： ,虽然为字段分隔符，但有字段引用符的存在，所以不需要转义。没有字段引用符的例子见后面 \\N表示MySQL中的NULL &quot;需要转义，所以csv中表示为\\&quot; \\t不需要被转义，因此在csv中表示为\\t，用vim查看时即显示为若干个空格。 \\n需要转义，所以在csv中为\\\\n，用vim查看时即显示为\\再换行。 示例2：（不带字段引用符） 1234567891011121314mysql&gt; select * from test2 into outfile &#x27;/var/lib/mysql-files/test.csv&#x27; FIELDS TERMINATED BY &#x27;,&#x27; lines terminated by &#x27;\\n&#x27;;Query OK, 5 rows affected (0.00 sec)# 数据如下，可以看到text列中的逗号就带上了\\，而双引号就没有带\\1,&#x27;sfas1&#x27;,2021-01-17,sdfasdf&quot;asdf asdf&quot;as\\,df\\\\as\\f,436526.77800000002,sfsa2,2021-01-17,sdfasdf&quot;asdf asdf&quot;as\\,df\\\\as\\f,436526.77800000003,sfsa3,2021-01-17,sdfasdf&quot;asdf asdf&quot;as\\,df\\\\as\\f,436526.77800000004,\\N,2021-01-17,sdfasdf&quot;asdf asdf&quot;as\\,df\\\\as\\f,436526.77800000005,_abcd_,2021-01-17,sdfasdf&quot;asdf asdf&quot;as\\,df\\\\as\\f,436526.7780000000 3、percona的xtrabackuppercona是一个提供数据库工具一个第三方供应商，xtrabackup是他的一个用于数据库备份的产品，使用方法略，因为我也没用过。 四、冷备份（需要关闭服务器）本人在windows上使用8.0.17版本的MySQL在import tablespace时不要cfg文件，不知道是windows版本的原因，还是8.0.17版本的原因，懒得追究了。 12345678910111213141516171819202122232425262728293031323334353637# 用一个test表来做测试create table test(id int primary key);# 将该步骤确保所有对表的修改全部写入到二进制文件中，并将表加上锁，只允许读操作，不允许写操作flush tables test for export;# 将test.ibd表空间文件和test.cfg元数据文件复制一份命名为tmp.ibd、tmp.cfgcp test.ibd tmp.ibdcp test.cfg tmp.cfg# 解除锁表unlock tables;# 修改表名为你需要的复制的表alter table test rename to test22;# 创建一张原表create table test like test22;# 删除表空间alter table test discard tablespace;# 将备份的表空间文件和元数据文件恢复mv tmp.ibd test.ibdmv tmp.cfg test.cfg# 更改文件的所属用户，否则会出现权限错误chown -R mysql.mysql .# 恢复表空间alter table test import tablespace;# 解除锁表unlock tables;# 待所有恢复都成功了之后，再删除cfg文件rm test.cfg 五、数据恢复1、MySQL中的source命令12# 注意这个是MySQL的命令，而不是Linux命令source /tmp/test.sql # 即执行test.sql中的全部语句 2、MySQL中的load data命令MySQL官网的load data命令语法如下： 123456789101112131415161718192021LOAD DATA [LOW_PRIORITY | CONCURRENT] [LOCAL] INFILE &#x27;file_name&#x27; [REPLACE | IGNORE] INTO TABLE tbl_name [PARTITION (partition_name [, partition_name] ...)] [CHARACTER SET charset_name] [&#123;FIELDS | COLUMNS&#125; [TERMINATED BY &#x27;string&#x27;] [[OPTIONALLY] ENCLOSED BY &#x27;char&#x27;] [ESCAPED BY &#x27;char&#x27;] ] [LINES [STARTING BY &#x27;string&#x27;] [TERMINATED BY &#x27;string&#x27;] ] [IGNORE number &#123;LINES | ROWS&#125;] [(col_name_or_user_var [, col_name_or_user_var] ...)] [SET col_name=&#123;expr | DEFAULT&#125; [, col_name=&#123;expr | DEFAULT&#125;] ...] 基本用法和select into outfile一样，高阶用法具体查看MySQL文档。 示例： 123456789101112131415161718192021222324252627282930313233mysql&gt; drop table test2;Query OK, 0 rows affected (0.00 sec)mysql&gt; CREATE TABLE `test2` ( -&gt; `id` int NOT NULL AUTO_INCREMENT, -&gt; `value` varchar(20) DEFAULT NULL, -&gt; `date` date DEFAULT NULL, -&gt; `text` text, -&gt; `decimal_value` decimal(20,10) DEFAULT NULL, -&gt; PRIMARY KEY (`id`) -&gt; ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;Query OK, 0 rows affected (0.01 sec)mysql&gt; load data infile &#x27;/var/lib/mysql-files/test.csv&#x27; into table test2 FIELDS TERMINATED BY &#x27;,&#x27; lines terminated by &#x27;\\n&#x27;;Query OK, 5 rows affected (0.01 sec)Records: 5 Deleted: 0 Skipped: 0 Warnings: 0mysql&gt; select * from test2;+----+---------+------------+------------------------------+-------------------+| id | value | date | text | decimal_value |+----+---------+------------+------------------------------+-------------------+| 1 | &#x27;sfas1&#x27; | 2021-01-17 | sdfasdf&quot;asdf asdf&quot;as,df\\asf | 436526.7780000000 || 2 | sfsa2 | 2021-01-17 | sdfasdf&quot;asdf asdf&quot;as,df\\asf | 436526.7780000000 || 3 | sfsa3 | 2021-01-17 | sdfasdf&quot;asdf asdf&quot;as,df\\asf | 436526.7780000000 || 4 | NULL | 2021-01-17 | sdfasdf&quot;asdf asdf&quot;as,df\\asf | 436526.7780000000 || 5 | _abcd_ | 2021-01-17 | sdfasdf&quot;asdf asdf&quot;as,df\\asf | 436526.7780000000 |+----+---------+------------+------------------------------+-------------------+5 rows in set (0.00 sec) 注意：在做数据转移时一定要把以下符号记住，否则容易出现数据错误。 12345678910[CHARACTER SET charset_name][&#123;FIELDS | COLUMNS&#125; [TERMINATED BY &#x27;string&#x27;] [[OPTIONALLY] ENCLOSED BY &#x27;char&#x27;] [ESCAPED BY &#x27;char&#x27;]][LINES [STARTING BY &#x27;string&#x27;] [TERMINATED BY &#x27;string&#x27;]] 额外1、禁用log_bin对insert速度的影响1234567891011121314151617181920212223242526272829303132333435mysql&gt; show master status;+---------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+---------------+----------+--------------+------------------+-------------------+| binlog.000002 | 156 | | | |+---------------+----------+--------------+------------------+-------------------+1 row in set (0.00 sec)# 查看binlog文件，因为刚重启了MySQL服务器，所以大小是156字节root@yuyr757:/var/lib/mysql# ll binlog.000002-rw-r----- 1 mysql mysql 156 1月 24 14:40 binlog.000002# 禁用log_binmysql&gt; set sql_log_bin = 0;Query OK, 0 rows affected (0.00 sec)mysql&gt; create table test3 like test;Query OK, 0 rows affected (0.01 sec)mysql&gt; select count(id) from test;+-----------+| count(id) |+-----------+| 3145728 |+-----------+1 row in set (0.99 sec)# 300万条数据插入约8.6秒。mysql&gt; insert into test3(value) select value from test;Query OK, 3145728 rows affected (8.62 sec)Records: 3145728 Duplicates: 0 Warnings: 0# 再次查看binlog文件，还是156字节root@yuyr757:/var/lib/mysql# ll binlog.000002-rw-r----- 1 mysql mysql 156 1月 24 14:40 binlog.000002 下面重启MySQL服务器，不禁用log_bin，看看300万条数据插入需要花多少时间。 12345678910111213141516171819202122232425262728293031323334353637383940root@yuyr757:/home/yuyr757# systemctl restart mysqlroot@yuyr757:/home/yuyr757# mysql -u root -pmysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 8Server version: 8.0.22 MySQL Community Server - GPLCopyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.mysql&gt; use test;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; select count(1) from test;+----------+| count(1) |+----------+| 3145728 |+----------+1 row in set (1.12 sec)mysql&gt; create table test4 like test;Query OK, 0 rows affected (0.01 sec)# 约13.9秒mysql&gt; insert into test4(value) select value from test;Query OK, 3145728 rows affected (13.93 sec)Records: 3145728 Duplicates: 0 Warnings: 0# 查看logbin文件，37911028字节root@yuyr757:/var/lib/mysql# ll binlog.000003-rw-r----- 1 mysql mysql 37911028 1月 24 14:49 binlog.000003 可见，禁用log_bin，insert速度从13.9秒缩短到了8.6秒。提高了38.13%的速度。即一些需要插入大量数据且允许适当禁用log_bin的情况时，可以适当禁用log_bin，来提高插入数据的速度。","categories":[{"name":"mysql","slug":"mysql","permalink":"https://yury757.github.io/categories/mysql/"}],"tags":[]},{"title":"docker","slug":"operating-support/docker/docker","date":"2020-10-09T16:00:00.000Z","updated":"2021-10-31T18:08:42.085Z","comments":true,"path":"operating-support/docker/docker/","link":"","permalink":"https://yury757.github.io/operating-support/docker/docker/","excerpt":"","text":"一、docker安装参见官方文档。 12345678910111213141516171819202122232425262728293031323334353637383940# 卸载原来的dockersudo apt remove docker \\ docker.io \\ docker-engine# 安装允许仓库使用https的依赖包sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common# 安装GPG密钥curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -# 向sources.list中添加Docker软件源（感觉设置ubuntu全局软件源镜像为国内镜像，可能可以不做这一步）sudo add-apt-repository \\ &quot;deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot;# 下载安装docker-ce（社区版）sudo apt install docker-ce docker-ce-cli containerd.io# 设置dockerhub镜像，和linux的apt安装软件要设置镜像一样，国外网站比较慢，用国内的镜像源很快cd /etc/docker # 如果没有docker文件夹，则自建一个docker文件夹vi daemon.json # 修改这个文件，如果没有则自建一个文件，写入以下值。&#123; &quot;registry-mirrors&quot;: [&quot;https://docker.mirrors.ustc.edu.cn&quot;]&#125;:wq # 保存退出systemctl restart docker # 重启docker服务# 运行hello-worldsudo docker run hello-world# 查看docker本地镜像包有哪些sudo docker images# docker的默认工作路径为cd /var/lib/docker 二、docker原理docker通信客户端和服务器之间的通信 外网和docker的通信 docker容器之间的通信 docker为什么比VM加载快？docker是在宿主机OS内核的基础上运行的，不用重新加载OS内核；而VM是抽象了一个OS内核，每次启动都要重新加载新的OS内核，所以慢。 三、docker命令docker帮助命令123docker version # docker版本信息docker info # docker系统的具体信息docker [command] --help # 查看某个命令的帮助 docker命令行帮助文档：https://docs.docker.com/reference/ docker镜像命令123456docker images [OPTIONS] [REPOSITORY[:TAG]] # 查看所有镜像docker search [OPTIONS] TERM # 搜索镜像docker search --filter stars=3000 mysql # 搜索mysql，带筛选参数，=3000其实是大于3000的意思docker pull [OPTIONS] NAME[:TAG|@DIGEST] # 下载镜像docker rmi [OPTIONS] IMAGE [IMAGE...] # 删除镜像，按ID删除docker rmi $(docker images -aq) # 删除所有镜像，$()里面放的是变量 docker容器命令运行容器12345678910111213docker run [OPTIONS] IMAGE [COMMAND] [ARG...] # 在一个新的容器中运行这个镜像（的命令）# 常用参数使用方式如下：--name=&quot;name01&quot; # 给容器命名-d # 后台方式运行-it # 以交互方式运行，进入容器查看内容# 如：docker run -it centos /bin/bash # 以交互方式运行容器，运行参数为/bin/bash-p # 指定容器端口，有以下四种方式 -p ip主机端口:容器端口 -p 主机端口:容器端口（常用） -p 容器端口 容器端口-P # 大写的P，随机指定端口 坑：docker使用-d后台运行，必须要有一个前台进程，若没有前台进程，docker发现没有应用，就会自动停止这个容器。 docker run -d centos这个命令虽然以后台方式运行，但是没有前台控制台进程，docker会自动停止这个容器； docker run -d -it centos这个命令以后台方式运行，且有控制命令台，只不过控制命令台也在后台运行，因此docker启动这个容器后不会自动停止这个容器。 此外，第一个容器停止了之后无法重新启动，因为根源上就没有前台进程；而第二个容器手动停止后，是可以再次启动的，因此有前台进程。 列举容器12docker ps [OPTIONS] # 列举容器，默认显示当前正在运行的容器docker ps -a # 列举所有容器，包括历史运行过的容器 进入容器12docker exec [OPTIONS] CONTAINER COMMAND [ARG...] # 进入容器并以COMMAND命令开启一个新的进程docker attach [OPTIONS] CONTAINER # 进入容器并跳到正在执行的那个进程 退出容器(以交互方式进入容器后的退出)12exit # 退出并停止容器ctrl + P + Q # 退出但不停止容器 删除容器123docker rm [OPTIONS] CONTAINER [CONTAINER...] # 删除容器，默认不能删除正在运行的容器，按ID删除-f # 强制删除，可以删除正在运行的容器docker rm -f $(docker ps -aq) # 删除所有容器 启动和停止容器1234docker start # 启动容器docker restart # 重启容器docker stop # 停止容器docker kill # 杀死容器 其他命令查看日志1docker logs [OPTIONS] CONTAINER # 查看日志 查看容器内的进程1docker top CONTAINER [ps OPTIONS] # 查看容器内的进程，CONTAINER为容器ID 查看容器的元数据1docker inspect [OPTIONS] NAME|ID [NAME|ID...] # 查看容器的元数据 拷贝文件或文件夹12docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|- # 从容器拷贝至宿主机docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH # 从宿主机拷贝至容器 docker命令小结 docker命令练习使用docker安装一个Nginx1234567891011docker search nginx --filter stars=3000 # 找到我们需要的image，也可以去官网上找docker pull nginx # 下载nginxdocker run -p 3344:80 --name nginx01 -d nginx # 启动nginx，-p为端口映射参数，--name命名，-d为后台运行（该程序可以不需要前台进程）curl localhost:3344 # 结果如下的html页面docker exec -it nginx01 /bin/bash # 进入容器whereis nginx# nginx: /usr/sbin/nginx /usr/lib/nginx /etc/nginx /usr/share/nginxcd /etc/nginxls# conf.d fastcgi_params koi-utf koi-win mime.types modules nginx.conf scgi_params uwsgi_params win-utf 1234567891011121314151617181920212223&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt; 使用docker安装一个tomcat12345678docker search --filter stars=300 tomcatdocker pull tomcatdocker run -d -p 3355:8080 --name tomcat01 tomcatdocker exec -it tomcat01 /bin/bash # 进入了tomcat所在的容器cd /usr/local/tomcatcp -r webapps.dist/* webapps # 20210102，官方默认的tomcat是不带webapps这个文件夹的，可以从webapps.dist文件夹里面的所有文件拷贝到一个新建的webapps文件下，就可以了^p ^q # 退出容器curl localhost:3355 # 结果如下html所示（省略了body） 12345678910111213&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;Apache Tomcat/9.0.41&lt;/title&gt; &lt;link href=&quot;favicon.ico&quot; rel=&quot;icon&quot; type=&quot;image/x-icon&quot; /&gt; &lt;link href=&quot;tomcat.css&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot; /&gt; &lt;/head&gt; &lt;body&gt; &lt;div&gt;...省略&lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 四、docker镜像1、镜像是分层下载的，各个镜像之间的层可以共享，各个层的ID为其加密的一个唯一ID。 2、对于一个容器的所有修改，不是修改原来的层，而是会形成一个新的层，push到docker hub后只是在原来的层上面新加了一层，技术原理很像git。 五、docker数据卷将容器内的数据挂载到宿主机上，实现数据同步。这种同步是双向绑定，即任意一方的修改都会同步到另一方。同时这种同步不需要启动容器，容器未启动时，在宿主机的修改也会同步到容器中。 制定路径挂载主机目录地址位置的值一定要以/开头，才是指定路径挂载 1234docker run -it -v 主机目录地址:容器内目录地址# 示例docker run -it -v /home/yuyr757/Documents/testdocker:/home/testdocker centos /bin/bash 练习：安装mysql并实现数据持久化123456789101112131415161718# 启动一个mysql容器，绑定了两个文件夹同步docker run -p 3310:3306 -v /home/yuyr757/Documents/mysql/conf:/etc/mysql/conf.d -v /home/yuyr757/Documents/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=root --name mysql01 -d mysql# 在外部通过3310端口访问mysql服务器，可以进入数据库mysql -u root -proot -h 192.168.141.128 -P 3310# 创建一个数据库create table test;create table test_table(id int unsigned auto_increment primary key, value varchar(20));# 在linux宿主机和linux容器里面的数据文件夹都可以看到数据文件已经创建了。# 在linux宿主机删除容器后会发现数据文件依旧存在！！docker rm -f mysql01root@yuyr757:/home/yuyr757/Documents/mysql/data# ls auto.cnf binlog.index client-key.pem ibdata1 &#x27;#innodb_temp&#x27; private_key.pem sys binlog.000001 ca-key.pem &#x27;#ib_16384_0.dblwr&#x27; ib_logfile0 mysql public_key.pem test binlog.000002 ca.pem &#x27;#ib_16384_1.dblwr&#x27; ib_logfile1 mysql.ibd server-cert.pem undo_001 binlog.000003 client-cert.pem ib_buffer_pool ibtmp1 performance_schema server-key.pem undo_002 具名挂载和匿名挂载具名挂载（常用）：-v name:container_path，name为挂在的卷名称，container_path为容器内路径，这种方式挂载的宿主机路径为：/var/lib/docker/volumes/$&#123;name&#125;/_data文件夹内。 匿名挂载：-v container_path，-v后面只有一个路径，则是匿名挂载，匿名挂载的宿主机路径和上面一致，只是name为匿名ID而已。 1234567891011121314151617181920212223242526272829303132333435363738# 匿名挂载示例如下docker run -P -d --name nginx02 -v /etc/nginx nginx# 通过inspect查看元数据docker inspect nginx02# 里面有一个Mounts的Type为colume即为数据卷，可以看到其name为一串加密的ID&quot;Mounts&quot;: [&#123;&quot;Type&quot;: &quot;volume&quot;,&quot;Name&quot;: &quot;c2c1a7a82624858a64db52998aa17b2bc68b92be1fd6a5ec1ce39f238935b8a2&quot;,&quot;Source&quot;: &quot;/var/lib/docker/volumes/c2c1a7a82624858a64db52998aa17b2bc68b92be1fd6a5ec1ce39f238935b8a2/_data&quot;,&quot;Destination&quot;: &quot;/etc/nginx&quot;,&quot;Driver&quot;: &quot;local&quot;,&quot;Mode&quot;: &quot;&quot;,&quot;RW&quot;: true,&quot;Propagation&quot;: &quot;&quot;&#125;],# 查看数据卷docker volume ls# DRIVER VOLUME NAME# local c2c1a7a82624858a64db52998aa17b2bc68b92be1fd6a5ec1ce39f238935b8a2# 查看卷的元数据docker volume inspect c2c1a7a82624858a64db52998aa17b2bc68b92be1fd6a5ec1ce39f238935b8a2[ &#123; &quot;CreatedAt&quot;: &quot;2021-01-03T14:46:37+08:00&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: null, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/c2c1a7a82624858a64db52998aa17b2bc68b92be1fd6a5ec1ce39f238935b8a2/_data&quot;, &quot;Name&quot;: &quot;c2c1a7a82624858a64db52998aa17b2bc68b92be1fd6a5ec1ce39f238935b8a2&quot;, &quot;Options&quot;: null, &quot;Scope&quot;: &quot;local&quot; &#125;] 12345678910111213141516171819202122232425262728293031323334# 具名挂载示例如下docker run -P -d --name nginx03 -v juming-nginx:/etc/nginx nginxdocker inspect nginx03&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;volume&quot;, &quot;Name&quot;: &quot;juming-nginx&quot;, &quot;Source&quot;: &quot;/var/lib/docker/volumes/juming-nginx/_data&quot;, &quot;Destination&quot;: &quot;/etc/nginx&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Mode&quot;: &quot;z&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;&quot; &#125;],docker volume ls# DRIVER VOLUME NAME# local c2c1a7a82624858a64db52998aa17b2bc68b92be1fd6a5ec1ce39f238935b8a2# local juming-nginxdocker volume inspect juming-nginx[ &#123; &quot;CreatedAt&quot;: &quot;2021-01-03T14:55:23+08:00&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: null, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/juming-nginx/_data&quot;, &quot;Name&quot;: &quot;juming-nginx&quot;, &quot;Options&quot;: null, &quot;Scope&quot;: &quot;local&quot; &#125;] 数据卷权限-v后面的容器目录还跟了一个:ro或:rw，默认是:rw。 :ro：readonly，即容器内的绑定的那个目录是只读的，无法在容器内修改，只能在宿主机修改。 :rw：readwrite，即容器内绑定的那个目录有读写权限。 如下： 12345docker run -P -d --name nginx04 -v juming-nginx04:/etc/nginx:ro nginxdocker exec -it nginx04 /bin/bashcd /etc/nginxmkdir test# mkdir: cannot create directory &#x27;test&#x27;: Read-only file system 挂载数据卷的第二种方式在构建image时即把需要挂载的卷路径写上，启动时即会自动挂载卷。如protainer的官方镜像就会自动挂载两个卷。 数据卷容器容器之间可以同步数据，即为数据卷容器。 1docker run --volumes-from [container] 六、DockerFile基础知识DockerFIle是用来构建docker镜像的命令行脚本文件。就像装双系统或装虚拟机时，要做一个镜像，把一系列装系统时要加入系统的软件或需要执行的命令放到镜像中。 1、每一个指令都是大写 2、每一个指令都会提交一个新的镜像层 如ubuntu镜像的dockerfile脚本如下： 1234567891011121314151617181920FROM scratchADD ubuntu-bionic-core-cloudimg-amd64-root.tar.gz /RUN set -xe \\ &amp;&amp; echo &#x27;#!/bin/sh&#x27; &gt; /usr/sbin/policy-rc.d \\ &amp;&amp; echo &#x27;exit 101&#x27; &gt;&gt; /usr/sbin/policy-rc.d \\ &amp;&amp; chmod +x /usr/sbin/policy-rc.d \\ &amp;&amp; dpkg-divert --local --rename --add /sbin/initctl \\ &amp;&amp; cp -a /usr/sbin/policy-rc.d /sbin/initctl \\ &amp;&amp; sed -i &#x27;s/^exit.*/exit 0/&#x27; /sbin/initctl \\ &amp;&amp; echo &#x27;force-unsafe-io&#x27; &gt; /etc/dpkg/dpkg.cfg.d/docker-apt-speedup \\ &amp;&amp; echo &#x27;DPkg::Post-Invoke &#123; &quot;rm -f /var/cache/apt/archives/*.deb /var/cache/apt/archives/partial/*.deb /var/cache/apt/*.bin || true&quot;; &#125;;&#x27; &gt; /etc/apt/apt.conf.d/docker-clean \\ &amp;&amp; echo &#x27;APT::Update::Post-Invoke &#123; &quot;rm -f /var/cache/apt/archives/*.deb /var/cache/apt/archives/partial/*.deb /var/cache/apt/*.bin || true&quot;; &#125;;&#x27; &gt;&gt; /etc/apt/apt.conf.d/docker-clean \\ &amp;&amp; echo &#x27;Dir::Cache::pkgcache &quot;&quot;; Dir::Cache::srcpkgcache &quot;&quot;;&#x27; &gt;&gt; /etc/apt/apt.conf.d/docker-clean \\ &amp;&amp; echo &#x27;Acquire::Languages &quot;none&quot;;&#x27; &gt; /etc/apt/apt.conf.d/docker-no-languages \\ &amp;&amp; echo &#x27;Acquire::GzipIndexes &quot;true&quot;; Acquire::CompressionTypes::Order:: &quot;gz&quot;;&#x27; &gt; /etc/apt/apt.conf.d/docker-gzip-indexes \\ &amp;&amp; echo &#x27;Apt::AutoRemove::SuggestsImportant &quot;false&quot;;&#x27; &gt; /etc/apt/apt.conf.d/docker-autoremove-suggestsRUN [ -z &quot;$(apt-get indextargets)&quot; ]RUN mkdir -p /run/systemd &amp;&amp; echo &#x27;docker&#x27; &gt; /run/systemd/containerCMD [&quot;/bin/bash&quot;] DockerFile指令可见参考如下博客：https://www.cnblogs.com/nuccch/p/10828666.html 1docker history [image] # 查看某个镜像构建的所有历史过程，很像git log CMD命令和ENTRYPOINT的区别： CMD只会执行最后一条CMD命令，且docker run的参数会覆盖CMD中的参数； ENTRYPOINT所有命令都会执行，且docker run的参数会追加到ENTRYPOINT命令后面。 七、docker网络docker网络基础docker网络和vmware虚拟机的网络相似，docker默认使用网桥虚拟网络。 docker容器与宿主机之间、docker容器之间都是通过docker0这个路由器来通信的。 docker网络模型如下： docker网络命令12345678910111213docker network COMMAND# 列举所有网络docker network ls# 查看某个网络详情docker network inspect [network_id]# 创建一个自定义网络docker network create# 查看linex的端口映射/转发iptables -t nat -L -n 自定义网络 问题：在微服务中，通过是通过服务名称访问服务的，如果一个mysql服务挂了，重启一个mysql的docker服务后，ip地址就可能换了，因此如果可以通过docker的名称就能访问到docker服务就好了。 –link可以解决，但不推荐。推荐使用自定义网络。 网络模式 bridge：桥接（docker默认） none：不使用网络 host：主机模式，和宿主机共享网络 container：容器网络连通（用的少） 123456789101112131415161718192021# 直接启动容器时，其实会有一个默认参数--net bridge如下，即使用docker默认的桥接网络docker run -d -P --name tomcat01 --net bridge tomcat# 创建一个自定义网络# --driver：网络模式# --subnet：子网范围# 192.168.0.0/16可用的子网为192.168.0.2-192.168.255.255# 192.168.0.0/24可用的子网为192.168.0.2-192.168.0.255# --gateway：网关地址，所有子网和网络中其他地址通信都要通过该地址来路由docker network create --driver bridge --subnet 192.168.0.0/16 --gateway 192.168.0.1 mynet# 查看网络docker network ls# NETWORK ID NAME DRIVER SCOPE# 40b40bd10d88 bridge bridge local# c76560fbd32a host host local# b7176c250a27 mynet bridge local # 这个就是我们新建的自定义网络# 9cdc4457c126 none null local# 将tomcat放在我们自己的网络中docker run -d -P --name tomcat01 --net mynet tomcat 注意：使用自定义网络就可以直接使用容器名字ping通其他容器，不需要配置任何其他东西。而docker自带的网络docker0卻不具备这样的功能。 构建自定义网络还一个好处就是，可以把不同的服务集群部署在不同的网络下，保证了集群是安全的健康的。 网络连通 两个不同网络之间的容器如何ping通？即一个容器使用docker0，另一个容器使用我们创建的自定义网络。都不在同一个网段下，无法通过各自的网关ping通，那这个如何做到呢？ docker提供了一个方法，可以将容器连接到一个网络。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# 将一个容器连接到另一个网络下docker network connect [OPTIONS] NETWORK CONTAINER# 使用docker0创建一个容器docker run -d -P --name tomcat-default-01 tomcat# 将tomcat-default-01容器和mynet网络打通docker network connect mynet tomcat-default-01 # 命令结束后什么事情都没发生# 查看mynet网络详情，会发现是直接把这个容器又挂在了mynet网络下docker network inspect b7176c250a27&quot;Containers&quot;: &#123; &quot;349c6846987344c00f2bd138698ceca3651a41add5456aebe6a2b8727955d7f8&quot;: &#123; &quot;Name&quot;: &quot;tomcat-default-01&quot;, &quot;EndpointID&quot;: &quot;fda6d449a211b053e084898d0e0b53008b0dc9e0735d18f29e1f7a59a2a4ae2b&quot;, &quot;MacAddress&quot;: &quot;02:42:c0:a8:00:03&quot;, &quot;IPv4Address&quot;: &quot;192.168.0.3/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125;, &quot;fca88679607e20464de21c5fe0599e40fe221e534df766a4df27efd99ab66f6b&quot;: &#123; &quot;Name&quot;: &quot;tomcat01&quot;, &quot;EndpointID&quot;: &quot;c06c4eb1d7f29ed4ca8cc10a7ca79c2e3b993d787fc7e993e0baf026e7dfbace&quot;, &quot;MacAddress&quot;: &quot;02:42:c0:a8:00:02&quot;, &quot;IPv4Address&quot;: &quot;192.168.0.2/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125;&#125;,# 查看tomcat-default-01容器详情，会发现它有两个网络docker inspect tomcat-default-01&quot;Networks&quot;: &#123; &quot;bridge&quot;: &#123; &quot;IPAMConfig&quot;: null, &quot;Links&quot;: null, &quot;Aliases&quot;: null, &quot;NetworkID&quot;: &quot;40b40bd10d8870bf241428acac690dcdda5e6a47a80f924da1027eecf84bf28d&quot;, &quot;EndpointID&quot;: &quot;ac4f9011bdc4a67fc3b1ce34b37687dc7ff7c38a5ab8f92467d1f69aa9c1b9e8&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot;, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;, &quot;DriverOpts&quot;: null &#125;, &quot;mynet&quot;: &#123; &quot;IPAMConfig&quot;: &#123;&#125;, &quot;Links&quot;: null, &quot;Aliases&quot;: [ &quot;349c68469873&quot; ], &quot;NetworkID&quot;: &quot;b7176c250a272ee1db707007378b611d507fd92c46f7ef1f4e5049ae817019ad&quot;, &quot;EndpointID&quot;: &quot;fda6d449a211b053e084898d0e0b53008b0dc9e0735d18f29e1f7a59a2a4ae2b&quot;, &quot;Gateway&quot;: &quot;192.168.0.1&quot;, &quot;IPAddress&quot;: &quot;192.168.0.3&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;MacAddress&quot;: &quot;02:42:c0:a8:00:03&quot;, &quot;DriverOpts&quot;: &#123;&#125; &#125;&#125; 这种方式即是一个容器两个ip 八、实战：部署一个redis集群需要部署的redis集群模型如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160# 先建一个redis集群网络docker network create --subnet 172.38.0.0/16 redis# 通过脚本创建六个redis配置文件for port in $(seq 1 6); \\do \\mkdir -p /home/yuyr757/Documents/redis/node-$&#123;port&#125;/conftouch /home/yuyr757/Documents/redis/node-$&#123;port&#125;/conf/redis.confcat &lt;&lt; EOF &gt;/home/yuyr757/Documents/redis/node-$&#123;port&#125;/conf/redis.confport 6379bind 0.0.0.0cluster-enabled yescluster-config-file yescluster-node-timeout 5000cluster-announce-ip 172.38.0.1$&#123;port&#125;cluster-announce-port 6379cluster-announce-bus-port 16379appendonly yesEOFdone# 启动redisfor port in $(seq 1 6); \\do \\docker run -p 637$&#123;port&#125;:6379 -p 1637$&#123;port&#125;:16379 --name redis-$&#123;port&#125; \\-v /home/yuyr757/Documents/redis/node-$&#123;port&#125;/data:/data \\-v /home/yuyr757/Documents/redis/node-$&#123;port&#125;/conf/redis.conf:/etc/redis/redis.conf \\-d --net redis --ip 172.38.0.1$&#123;port&#125; redis redis-server /etc/redis/redis.conf; \\done# 进入redis-1docker exec -it redis-1 /bin/bash# 创建集群redis-cli --cluster create 172.38.0.11:6379 172.38.0.12:6379 172.38.0.13:6379 172.38.0.14:6379 172.38.0.15:6379 172.38.0.16:6379 --cluster-replicas 1# 以下是输出消息：&lt;&lt;&#x27;COMMENT&#x27;&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Master[0] -&gt; Slots 0 - 5460Master[1] -&gt; Slots 5461 - 10922Master[2] -&gt; Slots 10923 - 16383Adding replica 172.38.0.15:6379 to 172.38.0.11:6379Adding replica 172.38.0.16:6379 to 172.38.0.12:6379Adding replica 172.38.0.14:6379 to 172.38.0.13:6379M: 4b8d468f5abc19a98b4a6d22696e33e1b1de2eee 172.38.0.11:6379 slots:[0-5460] (5461 slots) masterM: 52a3ef91ba1ac1bae92da386d8a1d48c319a95e9 172.38.0.12:6379 slots:[5461-10922] (5462 slots) masterM: a3b25de9426dd1d4143870a08740b481f0a2560f 172.38.0.13:6379 slots:[10923-16383] (5461 slots) masterS: 476e79e895cc6eecbb36634273031afdd737c534 172.38.0.14:6379 replicates a3b25de9426dd1d4143870a08740b481f0a2560fS: c3de87b8608fa93d28cf5059b08f0e58c5c0abb5 172.38.0.15:6379 replicates 4b8d468f5abc19a98b4a6d22696e33e1b1de2eeeS: 20401be641326be8a2891c8215317d8c70064986 172.38.0.16:6379 replicates 52a3ef91ba1ac1bae92da386d8a1d48c319a95e9Can I set the above configuration? (type &#x27;yes&#x27; to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join...&gt;&gt;&gt; Performing Cluster Check (using node 172.38.0.11:6379)M: 4b8d468f5abc19a98b4a6d22696e33e1b1de2eee 172.38.0.11:6379 slots:[0-5460] (5461 slots) master 1 additional replica(s)M: a3b25de9426dd1d4143870a08740b481f0a2560f 172.38.0.13:6379 slots:[10923-16383] (5461 slots) master 1 additional replica(s)M: 52a3ef91ba1ac1bae92da386d8a1d48c319a95e9 172.38.0.12:6379 slots:[5461-10922] (5462 slots) master 1 additional replica(s)S: 476e79e895cc6eecbb36634273031afdd737c534 172.38.0.14:6379 slots: (0 slots) slave replicates a3b25de9426dd1d4143870a08740b481f0a2560fS: c3de87b8608fa93d28cf5059b08f0e58c5c0abb5 172.38.0.15:6379 slots: (0 slots) slave replicates 4b8d468f5abc19a98b4a6d22696e33e1b1de2eeeS: 20401be641326be8a2891c8215317d8c70064986 172.38.0.16:6379 slots: (0 slots) slave replicates 52a3ef91ba1ac1bae92da386d8a1d48c319a95e9[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.COMMENT# 进入redis-1docker exec -it redis-1 /bin/bash# 进入redis集群客户端redis-cli -c# 查看redis集群信息127.0.0.1:6379&gt; cluster info&lt;&lt;&#x27;COMMENT&#x27;cluster_state:okcluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:6cluster_size:3cluster_current_epoch:6cluster_my_epoch:1cluster_stats_messages_ping_sent:107cluster_stats_messages_pong_sent:105cluster_stats_messages_sent:212cluster_stats_messages_ping_received:100cluster_stats_messages_pong_received:107cluster_stats_messages_meet_received:5cluster_stats_messages_received:212COMMENT# 查看集群中的结点127.0.0.1:6379&gt; cluster nodes&lt;&lt;&#x27;COMMENT&#x27;a3b25de9426dd1d4143870a08740b481f0a2560f 172.38.0.13:6379@16379 master - 0 1610196549828 3 connected 10923-1638352a3ef91ba1ac1bae92da386d8a1d48c319a95e9 172.38.0.12:6379@16379 master - 0 1610196549324 2 connected 5461-10922476e79e895cc6eecbb36634273031afdd737c534 172.38.0.14:6379@16379 slave a3b25de9426dd1d4143870a08740b481f0a2560f 0 1610196550000 3 connected4b8d468f5abc19a98b4a6d22696e33e1b1de2eee 172.38.0.11:6379@16379 myself,master - 0 1610196548000 1 connected 0-5460c3de87b8608fa93d28cf5059b08f0e58c5c0abb5 172.38.0.15:6379@16379 slave 4b8d468f5abc19a98b4a6d22696e33e1b1de2eee 0 1610196551341 1 connected20401be641326be8a2891c8215317d8c70064986 172.38.0.16:6379@16379 slave 52a3ef91ba1ac1bae92da386d8a1d48c319a95e9 0 1610196550332 2 connectedCOMMENT# 测试redis127.0.0.1:6379&gt; set a b-&gt; Redirected to slot [15495] located at 172.38.0.13:6379OK# 停止redis-3docker stop redis-3# 在redis-1服务器上请求redis，在14服务器上相应了127.0.0.1:6379&gt; get a-&gt; Redirected to slot [15495] located at 172.38.0.14:6379&quot;b&quot;# 查看redis集群的结点信息，发现172.38.0.13已经fail了，本来作为从服务器的14服务器已经变成主服务器了。172.38.0.14:6379&gt; cluster nodes&lt;&lt;&#x27;COMMENT&#x27;476e79e895cc6eecbb36634273031afdd737c534 172.38.0.14:6379@16379 myself,master - 0 1610197181000 7 connected 10923-163834b8d468f5abc19a98b4a6d22696e33e1b1de2eee 172.38.0.11:6379@16379 master - 0 1610197183456 1 connected 0-5460c3de87b8608fa93d28cf5059b08f0e58c5c0abb5 172.38.0.15:6379@16379 slave 4b8d468f5abc19a98b4a6d22696e33e1b1de2eee 0 1610197182445 1 connected52a3ef91ba1ac1bae92da386d8a1d48c319a95e9 172.38.0.12:6379@16379 master - 0 1610197183000 2 connected 5461-10922a3b25de9426dd1d4143870a08740b481f0a2560f 172.38.0.13:6379@16379 master,fail - 1610197047513 1610197045000 3 connected20401be641326be8a2891c8215317d8c70064986 172.38.0.16:6379@16379 slave 52a3ef91ba1ac1bae92da386d8a1d48c319a95e9 0 1610197183052 2 connectedCOMMENT# 再次启动redis-3docker start redis-3# 再次查看redis集群结点，发现redis-3服务器已经变从服务器了。127.0.0.1:6379&gt; cluster nodes52a3ef91ba1ac1bae92da386d8a1d48c319a95e9 172.38.0.12:6379@16379 master - 0 1610197501795 2 connected 5461-10922a3b25de9426dd1d4143870a08740b481f0a2560f 172.38.0.13:6379@16379 myself,slave 476e79e895cc6eecbb36634273031afdd737c534 0 1610197500000 7 connectedc3de87b8608fa93d28cf5059b08f0e58c5c0abb5 172.38.0.15:6379@16379 slave 4b8d468f5abc19a98b4a6d22696e33e1b1de2eee 0 1610197500785 1 connected476e79e895cc6eecbb36634273031afdd737c534 172.38.0.14:6379@16379 master - 0 1610197500000 7 connected 10923-163834b8d468f5abc19a98b4a6d22696e33e1b1de2eee 172.38.0.11:6379@16379 master - 0 1610197500000 1 connected 0-546020401be641326be8a2891c8215317d8c70064986 172.38.0.16:6379@16379 slave 52a3ef91ba1ac1bae92da386d8a1d48c319a95e9 0 1610197500583 2 connected 九、实战：部署一个wordpress博客1234567891011121314151617docker network create --driver bridge --subnet 192.0.0.0/16 --gateway 192.0.0.1 wordpressnetworkdocker run -d -P -e MYSQL_ROOT_PASSWORD=root --name mysql-for-wordpress -v /home/wordpress/mysql/data:/var/lib/mysql -v /home/wordpress/mysql/conf:/etc/mysql/conf.d --network wordpressnetwork mysql# 进入数据库容器，建一个数据库用于wordpress用，名称为wordpress。docker exec -it mysql-for-wordpress /bin/bashmysql -u root -prootcreate table wordpress;exit;docker run -d -P --name wordpress -v /home/wordpress/wordpress:/usr/share/wordpress --network wordpressnetwork wordpress# 这一步可以直接加以下等参数，也可以在前台进入wordpress时设置# -e WORDPRESS_DB_HOST=mysql-for-wordpress# -e WORDPRESS_DB_USER=root# -e WORDPRESS_DB_PASSWORD=root# -e WORDPRESS_DB_NAME=wordpress# -e WORDPRESS_TABLE_PREFIX=wp_","categories":[{"name":"docker","slug":"docker","permalink":"https://yury757.github.io/categories/docker/"}],"tags":[]},{"title":"MySQL查询执行计划详解-explain","slug":"database/mysql/MySQL查询执行计划详解-explain/MySQL查询执行计划详解-explain","date":"2020-09-30T16:00:00.000Z","updated":"2021-10-31T18:25:18.496Z","comments":true,"path":"database/mysql/MySQL查询执行计划详解-explain/MySQL查询执行计划详解-explain/","link":"","permalink":"https://yury757.github.io/database/mysql/MySQL%E6%9F%A5%E8%AF%A2%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%E8%AF%A6%E8%A7%A3-explain/MySQL%E6%9F%A5%E8%AF%A2%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%E8%AF%A6%E8%A7%A3-explain/","excerpt":"","text":"一、前言本文来自官方文档 explain用于解释优化器在执行select、update、delete、insert、replace语句时的执行计划，即它解释了MySQL如何处理SQL语句，包括表如何连接、表的连接顺序、用了哪些索引等。（replace是MySQL对于标准SQL语句的扩展，其他数据库可能没有，replace的使用见这里） 本文使用的表结构和数据如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152CREATE DATABASE test;CREATE TABLE trb1 ( id int auto_increment primary key, name varchar(50), purchased date)PARTITION BY RANGE (ID) ( PARTITION p0 VALUES LESS THAN (3), PARTITION P1 VALUES LESS THAN (7), PARTITION P2 VALUES LESS THAN (9), PARTITION P3 VALUES LESS THAN (11));INSERT INTO trb1 VALUES(1, &#x27;desk organiser&#x27;, &#x27;2003-10-15&#x27;),(2, &#x27;CD player&#x27;, &#x27;1993-11-05&#x27;),(3, &#x27;TV set&#x27;, &#x27;1996-03-10&#x27;),(4, &#x27;bookcase&#x27;, &#x27;1982-01-10&#x27;),(5, &#x27;exercise bike&#x27;, &#x27;2004-05-09&#x27;),(6, &#x27;sofa&#x27;, &#x27;1987-06-05&#x27;),(7, &#x27;popcorn maker&#x27;, &#x27;2001-11-22&#x27;),(8, &#x27;aquarium&#x27;, &#x27;1992-08-04&#x27;),(9, &#x27;study desk&#x27;, &#x27;1984-09-16&#x27;),(10, &#x27;lava lamp&#x27;, &#x27;1998-12-25&#x27;);create table trb2( id2 int auto_increment primary key, id int);insert into trb2(id) values(3), (3), (4);create table trb3( id3 int, name varchar(50), purchased date, primary key(id3, name), index trb3_index1(name, purchased));insert into trb3 values(1, &#x27;desk organiser&#x27;, &#x27;2003-10-15&#x27;),(2, &#x27;CD player&#x27;, &#x27;1993-11-05&#x27;),(3, &#x27;TV set&#x27;, &#x27;1996-03-10&#x27;),(4, &#x27;bookcase&#x27;, &#x27;1982-01-10&#x27;),(5, &#x27;exercise bike&#x27;, &#x27;2004-05-09&#x27;),(6, &#x27;sofa&#x27;, &#x27;1987-06-05&#x27;),(7, &#x27;popcorn maker&#x27;, &#x27;2001-11-22&#x27;),(8, &#x27;aquarium&#x27;, &#x27;1992-08-04&#x27;),(9, &#x27;study desk&#x27;, &#x27;1984-09-16&#x27;),(10, &#x27;lava lamp&#x27;, &#x27;1998-12-25&#x27;); 文章目录： [toc] 二、explain输出格式解释EXPLAIN为SELECT语句中使用的每个表返回一行信息，按照MySQL在处理语句时读取它们的顺序列示。explain的输出列如下。 Column JSON Name Meaning id select_id 查询的唯一标识， select_type None select类型 table table_name 表名，如设置了别名（alias）则展示别名 partitions partitions 查询计划匹配到的分区 type access_type 连接类型 possible_keys possible_keys 可能使用的索引 key key 实际使用的索引 key_len key_length 实际使用的索引的字节长度 ref ref 与索引比较的列 rows rows 估计要检查的行数量 filtered filtered 按表条件过滤的行百分比 Extra None 额外信息 1、idMySQL会给每一个查询分配一个id，归属同一个查询的行则该标识相同，不同的查询按序号顺序列示。注意并不是每有一个select就会有一个独立的id，如下： 12345678910111213141516171819mysql&gt; explain select * from trb1 t1 where exists(select 1 from trb2 t2 where t2.id = t1.id); -- id相同+----+-------------+-------+-------------+------+---------------+------+---------+------+------+----------+-------------------------------------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+-------------+------+---------------+------+---------+------+------+----------+-------------------------------------------------------------------+| 1 | SIMPLE | t2 | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | Start temporary || 1 | SIMPLE | t1 | p0,p1,p2,p3 | ALL | NULL | NULL | NULL | NULL | 10 | 10.00 | Using where; End temporary; Using join buffer (Block Nested Loop) |+----+-------------+-------+-------------+------+---------------+------+---------+------+------+----------+-------------------------------------------------------------------+2 rows in set, 2 warnings (0.00 sec)mysql&gt; explain select t1.*, (select t2.id2 from trb2 t2 where t2.id = t1.id limit 1) as id2 from trb1 t1; -- id不同+----+--------------------+-------+-------------+------+---------------+------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+--------------------+-------+-------------+------+---------------+------+---------+------+------+----------+-------------+| 1 | PRIMARY | t1 | p0,p1,p2,p3 | ALL | NULL | NULL | NULL | NULL | 10 | 100.00 | NULL || 2 | DEPENDENT SUBQUERY | t2 | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 33.33 | Using where |+----+--------------------+-------+-------------+------+---------------+------+---------+------+------+----------+-------------+2 rows in set, 2 warnings (0.00 sec) 2、select_type select_type Value JSON Name Meaning SIMPLE None 简单的select语句，没使用union或子查询 PRIMARY None 最外层的select语句 UNION None union中的第二个或更后面的select语句 DEPENDENT UNION dependent (true) union中的第二个或更后面的select语句，依赖于外部查询（不理解） UNION RESULT union_result union的结果，把union查询当作一个临时表的结果 SUBQUERY None 子查询的第一个select语句 DEPENDENT SUBQUERY dependent (true) 子查询的第一个select语句，依赖于外部查询 DERIVED None 派生表，临时表 DEPENDENT DERIVED dependent (true) 依赖于另外一个表的临时表 MATERIALIZED materialized_from_subquery 物化子查询 UNCACHEABLE SUBQUERY cacheable (false) 一个无法缓存的子查询，外部查询的每一行都要重新执行子查询 UNCACHEABLE UNION cacheable (false) 属于一个uncacheable subquery的union查询的第二个或更后面的查询 看下面这个查询。 id为1的是外部主查询，表名是，即id为3的derived类型的表，即别名为b的表，explain中没把b展示出来。id为2的是一个临时表，其表名为table1，可能是因为使用了union，展示的union中的第一个表名。 id为4的是一个uncacheable union，即无法被缓存的子查询，且存在于一个union中，处于union的第二个或更后面的位置。因为order by rand()，每次都要重新执行这次查询才能获取结果，因此无法被缓存。 id为NULL的是一个union结果，表名为&lt;union3,4&gt;，即union了id为3和4两张表的结果。 id为2的是一个UNCACHEABLE SUBQUERY，解释如id为4的一样。 1234567891011121314mysql&gt; EXPLAIN select b.*, (SELECT table3.id as c from trb1 table3 order by rand() limit 1) AS c FROM ( (select table1.id as a from trb1 table1 order by rand() LIMIT 1) UNION (select table2.id as a from trb1 table2 order by rand() LIMIT 1) ) as b;+----+----------------------+------------+-------------+------+---------------+------+---------+------+------+----------+---------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+----------------------+------------+-------------+------+---------------+------+---------+------+------+----------+---------------------------------+| 1 | PRIMARY | &lt;derived3&gt; | NULL | ALL | NULL | NULL | NULL | NULL | 2 | 100.00 | NULL || 3 | DERIVED | table1 | p0,p1,p2,p3 | ALL | NULL | NULL | NULL | NULL | 10 | 100.00 | Using temporary; Using filesort || 4 | UNCACHEABLE UNION | table2 | p0,p1,p2,p3 | ALL | NULL | NULL | NULL | NULL | 10 | 100.00 | Using temporary; Using filesort || NULL | UNION RESULT | &lt;union3,4&gt; | NULL | ALL | NULL | NULL | NULL | NULL | NULL | NULL | Using temporary || 2 | UNCACHEABLE SUBQUERY | table3 | p0,p1,p2,p3 | ALL | NULL | NULL | NULL | NULL | 10 | 100.00 | Using temporary; Using filesort |+----+----------------------+------------+-------------+------+---------------+------+---------+------+------+----------+---------------------------------+5 rows in set, 1 warning (0.00 sec) 当子查询中依赖外部表来获取结果时，就会有一个dependent，如下。 12345678mysql&gt; explain select t1.*, (select t2.id2 from trb2 t2 where t2.id = t1.id limit 1) as id2 from trb1 t1;+----+--------------------+-------+-------------+------+---------------+------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+--------------------+-------+-------------+------+---------------+------+---------+------+------+----------+-------------+| 1 | PRIMARY | t1 | p0,p1,p2,p3 | ALL | NULL | NULL | NULL | NULL | 10 | 100.00 | NULL || 2 | DEPENDENT SUBQUERY | t2 | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 33.33 | Using where |+----+--------------------+-------+-------------+------+---------------+------+---------+------+------+----------+-------------+2 rows in set, 2 warnings (0.00 sec) DEPENDENT SUBQUERY评估与UNCACHEABLE SUBQUERY评估不同：对于DEPENDENT SUBQUERY，子查询仅针对其外部上下文中变量的每组不同值重新评估一次。对于UNCACHEABLE SUBQUERY，将为外部上下文的每一行重新评估子查询。即DEPENDENT SUBQUERY是有一部分缓存的。 3、tabletable指的是表名或别名，或其他形式名称。（&lt;DERIVED**N**&gt;，&lt;UNION**M**,**N**&gt;，&lt;subquery**N**&gt;，加粗字母均为表对应的id。具体见上面的分析） 4、partitionpartition指的是该查询所使用到的表分区。关于表分区的解释见官方文档。如上面那个trb1表使用了所有的分区，又如下面这个查询只用到了p0、p1分区。 1234567mysql&gt; explain select * from trb1 where id &lt; 5;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+| 1 | SIMPLE | trb1 | p0,p1 | ALL | NULL | NULL | NULL | NULL | 6 | 33.33 | Using where |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+1 row in set, 1 warning (0.00 sec) 5、typetype指的是join type，即表之间是如何连接的。下面从最优到最差的方式排序列出了各种连接方式。 system：指表中只有一条记录，且符合const类型的查询。是一种特殊const类型。 const：指通过primary key或unique查询出来的数据，最多只有一条记录匹配。 eq_ref：指该表通过完整的primary key或uniqeu not null去和其他表相应字段连接时，则该表的join type为qe_ref，另外一张表的join type根据另外一张表的key去判断。这种情况下可以保证最多只能匹配出一条记录。如下，trb1表是eq_ref，而trb2表的join type是ALL，全表扫描。 12345678mysql&gt; explain select * from trb1, trb2 where trb1.id = trb2.id;+----+-------------+-------+-------------+--------+---------------+---------+---------+--------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+-------------+--------+---------------+---------+---------+--------------+------+----------+-------------+| 1 | SIMPLE | trb2 | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | Using where || 1 | SIMPLE | trb1 | p0,P1,P2,P3 | eq_ref | PRIMARY | PRIMARY | 4 | test.trb2.id | 1 | 100.00 | NULL |+----+-------------+-------+-------------+--------+---------------+---------+---------+--------------+------+----------+-------------+2 rows in set, 1 warning (0.00 sec) ref：指该表通过某个索引的最左前缀的部分或完整字段或多字段主键中满足最左前缀的部分字段去和其他表字段连接时，则该表的join type为ref。这种情况下无法保证匹配出最多一条记录。如下： 12345678910111213141516mysql&gt; explain select * from trb3 where id3 = 1;+----+-------------+-------+------------+------+---------------+---------+---------+-------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+---------+---------+-------+------+----------+-------+| 1 | SIMPLE | trb3 | NULL | ref | PRIMARY | PRIMARY | 4 | const | 1 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------+---------+---------+-------+------+----------+-------+1 row in set, 1 warning (0.00 sec)mysql&gt; explain select t3.*, t2.id2 from trb3 t3, trb2 t2 where t3.id3 = t2.id2;+----+-------------+-------+------------+-------+---------------+---------+---------+-------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+---------+---------+-------------+------+----------+-------------+| 1 | SIMPLE | t2 | NULL | index | PRIMARY | PRIMARY | 4 | NULL | 3 | 100.00 | Using index || 1 | SIMPLE | t3 | NULL | ref | PRIMARY | PRIMARY | 4 | test.t2.id2 | 1 | 100.00 | NULL |+----+-------------+-------+------------+-------+---------------+---------+---------+-------------+------+----------+-------------+2 rows in set, 1 warning (0.00 sec) fulltext：使用全文索引。大多业务使用较少，除非那种需要检索大量文本的业务。全文索引仅排在ref后面，说明全文索引的效率很高。全文索引大概意思就是专门用于文本查询的一个索引，只能构建在char、varchar、text类型上，通过全文索引的查询有自己特殊的语法（match(index_column) again(‘xxxx’)），全文索引的检索有最小搜索长度和最大搜索长度限制（当然可以通过修改my.ini修改配置），表的行数量条件要求，以及各种殷勤和版本限制等。关于全文索引的具体介绍见官方文档。 ref_or_null：即在ref情况下，使用索引的后面一个或多个字段使用is null来匹配，如下。 12SELECT * FROM ref_table WHERE key_column=expr OR key_column IS NULL; index_merge：索引合并。索引合并大概意思就是查询使用了多个索引并且可以合并这些索引以查询数据。具体见官方文档。如下就是一个索引合并。 12345678mysql&gt; explain select * from trb3 where id3 = 1 or name = &#x27;CD player&#x27;;+----+-------------+-------+------------+-------------+---------------------+---------------------+---------+------+------+----------+----------------------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------------+---------------------+---------------------+---------+------+------+----------+----------------------------------------------------+| 1 | SIMPLE | trb3 | NULL | index_merge | PRIMARY,trb3_index1 | trb3_index1,PRIMARY | 202,4 | NULL | 2 | 100.00 | Using sort_union(trb3_index1,PRIMARY); Using where |+----+-------------+-------+------------+-------------+---------------------+---------------------+---------+------+------+----------+----------------------------------------------------+1 row in set, 1 warning (0.00 sec) unique_subquery：官方文档说是这种是在IN子查询中使用索引覆盖，以提高效率，如下，但是本人实际测试中并未使用这种join type。 1value IN (SELECT primary_key FROM single_table WHERE some_expr) index_subquery：和上一个相似，区别是不用primary_key，而是使用普通的索引。 1value IN (SELECT key_column FROM single_table WHERE some_expr) range：索引被用作范围查询时可能使用range类型，有以下几点值得注意： join type为range时，key_len为使用的索引的最大长度 join type为range时，ref字段为NULL =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, LIKE, or IN() 都可能会使用range 索引用作范围查询时并不一定使用range，也可能使用其他，优化器会根据实际情况选择。如下第一个SQL即使用了index。 range也要满足最左前缀原则，不满足则可能使用其他类型，如下方代码块中的最后一个SQL。 12345678910111213141516171819202122232425262728293031mysql&gt; explain select * from trb3 where name &gt; &#x27;aa&#x27;;+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+| 1 | SIMPLE | trb3 | NULL | index | trb3_index1 | trb3_index1 | 206 | NULL | 10 | 100.00 | Using where; Using index |+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+1 row in set, 1 warning (0.00 sec)mysql&gt; explain select * from trb3 where name &gt; &#x27;sofa&#x27;;+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+| 1 | SIMPLE | trb3 | NULL | range | trb3_index1 | trb3_index1 | 202 | NULL | 2 | 100.00 | Using where; Using index |+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+1 row in set, 1 warning (0.00 sec)mysql&gt; explain select * from trb3 where name &gt; &#x27;sofa&#x27; and purchased &lt; &#x27;2020-01-01&#x27;;+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+| 1 | SIMPLE | trb3 | NULL | range | trb3_index1 | trb3_index1 | 202 | NULL | 2 | 33.33 | Using where; Using index |+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+1 row in set, 1 warning (0.00 sec)mysql&gt; explain select * from trb3 where purchased &lt; &#x27;2020-01-01&#x27;;+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+| 1 | SIMPLE | trb3 | NULL | index | trb3_index1 | trb3_index1 | 206 | NULL | 10 | 33.33 | Using where; Using index |+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+1 row in set, 1 warning (0.00 sec) index：虽然使用了索引中的字段进行查询，但是不满足最左前缀原则，则MySQL会在索引树中全树扫描，这就是index，如上面代码块中的第一个SQL和最后一个SQL。最后一个SQL使用index好理解，第一个SQL使用index个人认为是优化器发现索引树中的最小值 &gt; 查询条件’aa’，因此name &gt; ‘aa’就等于全树扫描，所以为index。如下当条件为name &gt; ‘ar’时，就使用了range，且任何条件大于索引树中的最小值的查询，都会使用range，而小于则全树扫描。index的效率比ALL高一点点，毕竟扫描全树的IO事件比扫描全表的IO事件更少。 1234567mysql&gt; explain select * from trb3 where name &gt; &#x27;ar&#x27;;+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+| 1 | SIMPLE | trb3 | NULL | range | trb3_index1 | trb3_index1 | 202 | NULL | 9 | 100.00 | Using where; Using index |+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+1 row in set, 1 warning (0.00 sec) ALL：全表扫描，未走任何索引，效率最低。 6、possible key可能使用的索引，没啥好讲的。 7、key实际使用的索引，也没啥好讲。 index hint是指，让MySQL按照我们的给定的索引去查询数据，主要有force index、use index和ignore index，也可以加上for join | order by | group by来指定索引使用的范围，如下SQL。具体使用见官方文档。 force index和use index的区别是：force index会强制使用该索引，但use index是建议MySQL使用该索引，但是优化器还是会根据实际情况来选择是否要全表扫描。 1234567mysql&gt; explain select * from trb3 force index for order by (trb3_index1) where id3 &gt; 3 order by name;+----+-------------+-------+------------+-------+---------------------+---------+---------+------+------+----------+-----------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------------+---------+---------+------+------+----------+-----------------------------+| 1 | SIMPLE | trb3 | NULL | range | PRIMARY,trb3_index1 | PRIMARY | 4 | NULL | 7 | 100.00 | Using where; Using filesort |+----+-------------+-------+------------+-------+---------------------+---------+---------+------+------+----------+-----------------------------+1 row in set, 1 warning (0.00 sec) 8、key_len该字段指的是查询执行时实际使用的索引的总最大字节长度，当所有有多个字段时，可以通过这个来看一个查询具体使用了哪几个字段。int占用4个字节，varchar每一个字符占用4个字节和2个字符存储字符串长度（varchar中文实际上大部分占3个字节，少量才占用四个字节，这里按最大的算），因此varchar(50)占用202个字节。如下通过查看字节长度就可以发现使用了多字段索引中的哪几个字段。 1234567891011121314151617181920212223mysql&gt; explain select * from trb3 where id3 = 3;+----+-------------+-------+------------+------+---------------------+---------+---------+-------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------------+---------+---------+-------+------+----------+-------+| 1 | SIMPLE | trb3 | NULL | ref | PRIMARY,trb3_index1 | PRIMARY | 4 | const | 1 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------------+---------+---------+-------+------+----------+-------+1 row in set, 1 warning (0.00 sec)mysql&gt; explain select * from trb3 where name &gt; &#x27;sdf&#x27;;+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+| 1 | SIMPLE | trb3 | NULL | range | trb3_index1 | trb3_index1 | 202 | NULL | 5 | 100.00 | Using where; Using index |+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+1 row in set, 1 warning (0.00 sec)mysql&gt; explain select * from trb3 where id3 = 3 and name &gt; &#x27;a&#x27;;+----+-------------+-------+------------+-------+---------------------+---------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------------+---------+---------+------+------+----------+-------------+| 1 | SIMPLE | trb3 | NULL | range | PRIMARY,trb3_index1 | PRIMARY | 206 | NULL | 1 | 100.00 | Using where |+----+-------------+-------+------------+-------+---------------------+---------+---------+------+------+----------+-------------+1 row in set, 1 warning (0.00 sec) 9、ref该字段显示了用哪些列或常量来和索引字段去匹配以查询数据。当索引和一个常量匹配时，ref字段为const，当使用索引行进范围查询时，ref字段为NULL。 12345678910111213141516mysql&gt; explain select * from trb2 where id2 in (select id from trb1 where name = &#x27;aquarium&#x27;);+----+-------------+-------+-------------+--------+---------------+---------+---------+---------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+-------------+--------+---------------+---------+---------+---------------+------+----------+-------------+| 1 | SIMPLE | trb2 | NULL | ALL | PRIMARY | NULL | NULL | NULL | 3 | 100.00 | NULL || 1 | SIMPLE | trb1 | p0,P1,P2,P3 | eq_ref | PRIMARY | PRIMARY | 4 | test.trb2.id2 | 1 | 10.00 | Using where |+----+-------------+-------+-------------+--------+---------------+---------+---------+---------------+------+----------+-------------+2 rows in set, 1 warning (0.00 sec)mysql&gt; explain select * from trb3 where id3 = if(id3 &gt; 3, 5, 2);+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+| 1 | SIMPLE | trb3 | NULL | index | NULL | trb3_index1 | 206 | NULL | 12 | 10.00 | Using where; Using index |+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+--------------------------+1 row in set, 1 warning (0.00 sec) 10、rowsrows列表示MySQL认为执行查询必须检查的行数。对于InnoDB表，此数字是估计值，可能并不总是准确的。 11、filtered该列指的是按表条件过滤的表行的估计百分比。最大值为100，这表示未过滤行。值从100减小表示过滤量增加。rows × filtered的值表示与下表连接的行数。例如，如果行数为1000，过滤条件为50.00（50％），则与下表连接的行数为1000×50％= 500。 12、Extra该列展示了SQL执行计划的额外信息，包括太多的内容，大部分是很少见的，以下主要解释几个重要的值得优化的内容： Using filesort：意为MySQL必须额外对检索出来的数据进行一次排序再输出这些数据。排序是通过根据连接类型遍历所有行并存储与WHERE子句匹配的所有行的排序键和指向该行的指针来完成的。即排序会using filesort会遍历所有行，存储通过where条件筛选出的行的排序字段和指向该行的指针，再对排序字段值和指针进行排序，再按照指针顺序输出数据。因此这种排序方式是特别慢的，排序优化见官方文档。 Using index：仅使用索引树中的信息从表中检索列信息，而不必进行其他查找以读取实际行。 Using index condition：通过访问索引集并首先对其进行测试以确定是否需要读取完整的表。除非有必要整表扫描，否则索引信息将用于延迟（“下推push down”）再读取整个表行。索引条件下推（Index Condition Pushdown）是针对MySQL使用using index从表中检索行的情况的一种优化。如果不使用ICP，则存储引擎将遍历索引以在基表中定位行，并将其返回给MySQL服务器，后者将评估这些行的WHERE条件。启用ICP后，如果仅可以使用索引中的列来评估WHERE条件的一部分，则MySQL服务器会将WHERE条件的这一部分下推到存储引擎。然后，存储引擎通过使用索引条目来评估推送的索引条件，并且只有在满足此条件的情况下，才从表中读取行。 ICP可以减少存储引擎必须访问基表的次数以及MySQL服务器必须访问存储引擎的次数。见官方文档。 Using index for group-by：即有一个索引可以可用于检索GROUP BY或DISTINCT查询的所有列，类似于group by的索引覆盖。 Using temporary：使用临时表。 Using where：即存在where条件，且where字段不在任意一个索引中，不能使用索引树进行where匹配，而必须在检查所有行再把满足where条件的数据输出给客户端。 三、explain的扩展输出格式explain输出列中的Extra列实际上并不是explain的，而是show warnings的结果，可以在使用explain后，可以紧跟着使用show warnings命令查看完整的extended information。 8.0.12版本之前，show warnings只适用于select，8.0.12版本之后，它适用于select、delete、update、replace、insert。 show warnings的message列显示了优化器如何限定select语句中的表名和列名，select语句在应用优化器的优化和重写之后的样子（会额外提供一些特殊标记，不一定是有效的SQL），以及其他与优化器处理有关的信息。如下。 123456789101112131415161718mysql&gt; explain select t1.id, t1.id in (select id from trb2) from trb1 t1;+----+-------------+-------+-------------+-------+---------------+---------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+-------------+-------+---------------+---------+---------+------+------+----------+-------------+| 1 | PRIMARY | t1 | p0,P1,P2,P3 | index | NULL | PRIMARY | 4 | NULL | 10 | 100.00 | Using index || 2 | SUBQUERY | trb2 | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | NULL |+----+-------------+-------+-------------+-------+---------------+---------+---------+------+------+----------+-------------+2 rows in set, 1 warning (0.00 sec)mysql&gt; show warnings;+-------+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Level | Code | Message |+-------+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Note | 1003 | /* select#1 */ select `test`.`t1`.`ID` AS `id`,&lt;in_optimizer&gt;(`test`.`t1`.`ID`,`test`.`t1`.`ID` in ( &lt;materialize&gt; (/* select#2 */ select `test`.`trb2`.`id` from `test`.`trb2` where true having true ), &lt;primary_index_lookup&gt;(`test`.`t1`.`ID` in &lt;temporary table&gt; on &lt;auto_key&gt; where ((`test`.`t1`.`ID` = `materialized-subquery`.`id`))))) AS `t1.id in (select id from trb2)` from `test`.`trb1` `t1` |+-------+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) show warnings中message列可能包含的特殊标记见官方文档，如下。 &lt;auto_key&gt;为临时表自动生成的索引。 &lt;cache&gt;(expr)expr表达式执行一次，将结果保存在内存中备用。对于有很多个缓存值，MySQL会创建一个临时表，并显示&lt;temporary table&gt;。 &lt;exists&gt;(query fragment)子查询将转换为EXISTS语句。和子查询优化有关，子查询优化有物化成临时表、semi join和转换成EXISTS语句等优化方法，其中如果使用转换成EXISTS这种优化方式时，可能就会有&lt;exists&gt;标记（盲猜的）。见官方文档8.2.2 Optimizing Subqueries, Derived Tables, View References, and Common Table Expressions。 &lt;in_optimizer&gt; (query fragment)指这是一个内部优化器对象，对用户没有任何意义。 &lt;index_lookup&gt; (query fragment)使用索引查找来处理查询片段以查找合格的行。 &lt;if&gt;(condition, expr1, expr2)if条件，condition条件为真则执行expr1，否则执行expr2。 &lt;is_not_null_test&gt;(expr)用于验证表达式是否为null的测试。 &lt;materialize&gt;(query fragment)物化子查询，见&lt;exists&gt;。 `materialized-subquery`.col_name一个子查询结果被物化成内部临时表后，这个临时表对某一列的引用。 &lt;primary_index_lookup&gt;(query fragment)使用主键查找来处理查询片段以查找合格的行。 &lt;ref_null_helper&gt;(expr)这是一个内部优化器对象，对用户没有任何意义。 /* select#N */ select_stmt指与explain中对应的某一个SELECT语句，N即为explain中的id。 outer_tables semi join (inner_tables)半联接操作。inner_tables显示未拉出的表。见&lt;exists&gt;。 &lt;temporary table&gt;(expr)为缓存结果创建的一个临时表。 四、explain的其他用法explain for connection可以通过show processlist查看连接线程列表，或通过select connection_id()查看当前连接线程的id。 explain for connection connection_id的用处是查看id为connection_id的线程当前正在执行的SQL语句的执行计划。如果那个线程当前没有执行SQL语句，则结果为空；如果那个线程当前执行的SQL语句不是select、update、replace、insert、delete中的任意一个，则会报错。如下： 123456789101112131415161718mysql&gt; show processlist;+-----+-----------------+-----------------+-------+---------+--------+------------------------+------------------+| Id | User | Host | db | Command | Time | State | Info |+-----+-----------------+-----------------+-------+---------+--------+------------------------+------------------+| 4 | event_scheduler | localhost | NULL | Daemon | 733424 | Waiting on empty queue | NULL || 508 | root | localhost:2004 | test | Query | 0 | starting | show processlist || 509 | root | localhost:1748 | xxxx | Sleep | 275 | | NULL || 510 | root | localhost:5639 | xxxx | Sleep | 275 | | NULL || 515 | root | localhost:13576 | xxxx | Sleep | 45170 | | NULL || 516 | root | localhost:13578 | xxxx | Sleep | 45170 | | NULL |+-----+-----------------+-----------------+-------+---------+--------+------------------------+------------------+6 rows in set (0.00 sec)mysql&gt; explain for connection 516;Query OK, 0 rows affected (0.00 sec)mysql&gt; explain for connection 508;ERROR 3012 (HY000): EXPLAIN FOR CONNECTION command is supported only for SELECT/UPDATE/INSERT/DELETE/REPLACE explain table_nameexplain table_name = show columns from table_name = describe table_name 123456789mysql&gt; explain trb1;+-----------+-------------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+-----------+-------------+------+-----+---------+----------------+| ID | int(11) | NO | PRI | NULL | auto_increment || name | varchar(50) | YES | | NULL | || purchased | date | YES | | NULL | |+-----------+-------------+------+-----+---------+----------------+3 rows in set (0.01 sec) 五、总结explain的用处其实就是让MySQL告诉你某一个SQL查询执行时，优化器会怎么优化它，存储引擎会采用怎样的表连接方式，采用哪些索引，执行该SQL必须扫描的行数量（估计数），和其他可以用于SQL优化的信息。通过获取这些信息，我们就可以发现一个SQL语句执行慢的原因，并作出合理的优化。","categories":[{"name":"mysql","slug":"mysql","permalink":"https://yury757.github.io/categories/mysql/"}],"tags":[]},{"title":"MySQL日期格式化","slug":"database/mysql/MySQL日期格式化/MySQL日期格式化","date":"2020-08-31T16:00:00.000Z","updated":"2021-10-31T18:23:46.450Z","comments":true,"path":"database/mysql/MySQL日期格式化/MySQL日期格式化/","link":"","permalink":"https://yury757.github.io/database/mysql/MySQL%E6%97%A5%E6%9C%9F%E6%A0%BC%E5%BC%8F%E5%8C%96/MySQL%E6%97%A5%E6%9C%9F%E6%A0%BC%E5%BC%8F%E5%8C%96/","excerpt":"","text":"参考文档：MySQL官方文档 类型 格式 解释 示例（2020-09-04&nbsp;20:03:07.12345 星期五） 年 %Y 年，4 位 2020 %y 年，2 位 20 月 %m 月，数值(00-12) 09 %c 月，数值(0-12) 9 %M 英文月名 September %b 英文月名（缩写） Sep 日 %d 天，数值(00-31) 04 %e 天，数值(0-31) 4 %j 天 (001-366) 248 %D 带有英文后缀的月中的天 4th 星期 %W 星期名 Friday %a 缩写星期名 Fri %w 周的天 （0=星期日, 6=星期六） 5 时间 %T 时间，24-小时 (hh:mm:ss) 20:03:07 %r 时间，12-小时（hh:mm:ss AM 或 PM） 08:03:07 PM 小时 %H 小时 (00-23) 20 %k 小时 (0-23) 20 %h /&nbsp;%I 小时 (01-12)，和%h应该没区别 08 %l 小时 (1-12) 8 分钟 %i 分钟，数值(00-59) 03 秒 %S /&nbsp;%s 秒(00-59)，大小写应该没区别，似乎是某些历史原因导致 07 上下午 %p AM 或 PM PM 微妙 %f 微秒 123456 第n周 %U 周 (00-53) 星期日是一周的第一天 SELECT DATE_FORMAT('1999-01-03', '%U'); -&gt;&nbsp;01 %u 周 (00-53) 星期一是一周的第一天 SELECT DATE_FORMAT('1999-01-03', '%U'); -&gt;&nbsp;00 年，周 %V 周 (01-53) 星期日是一周的第一天，与 %X 使用 SELECT DATE_FORMAT('1999-01-01', '%X %V'); -&gt; 1998 52（1999年1月1日为星期五，属于1998年的最后一周） SELECT&nbsp;DATE_FORMAT('1999-01-03',&nbsp;'%X %V'); -&gt;&nbsp;1999 01（1999年1月3日为星期日，属于1999年的第一周） %v 周 (01-53) 星期一是一周的第一天，与 %x 使用 &nbsp;SELECT DATE_FORMAT('1999-01-03', '%x %v'); -&gt;&nbsp;1998 53（1999年1月3日为星期日，属于1998年的最后一周） SELECT DATE_FORMAT('1999-01-04', '%x %v'); -&gt; 1999 01（1999年1月4日为星期一，属于1999年的第一周） %X 年，其中的星期日是周的第一天，4 位，与 %V 使用 见上方 %x 年，其中的星期一是周的第一天，4 位，与 %v 使用 见上方","categories":[{"name":"mysql","slug":"mysql","permalink":"https://yury757.github.io/categories/mysql/"}],"tags":[]},{"title":"MySQL中的exists与in的使用","slug":"database/mysql/MySQL中的exists与in的使用/MySQL中的exists与in的使用","date":"2020-05-31T16:00:00.000Z","updated":"2021-10-31T18:20:48.031Z","comments":true,"path":"database/mysql/MySQL中的exists与in的使用/MySQL中的exists与in的使用/","link":"","permalink":"https://yury757.github.io/database/mysql/MySQL%E4%B8%AD%E7%9A%84exists%E4%B8%8Ein%E7%9A%84%E4%BD%BF%E7%94%A8/MySQL%E4%B8%AD%E7%9A%84exists%E4%B8%8Ein%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"该文章总结各种博客文章，再加上自己的理解形成的。 结论： 子查询表大的用exists，子查询表小的用in。 1、exists的用法。 （1）exists子语句返回的是true或者false，当exists中子语句能查询出任意记录行时返回true，查不到任何记录时返回false。exists子语句查询出来的记录行没有任何用，因此很多时候用select 1就行，只需要判断是否能查询出记录即可。 （2）对于外表查询出来的记录，每条记录都会当作exists的条件去查询exists中的子语句，当exists子语句为false时，该条数据则会被丢弃。因此exists子语句中可以有外表的字段和表名。 例如： 12345678910111213141516171819202122232425262728293031323334353637mysql&gt; select * FROM MYSQL.help_keyword where help_keyword_id &lt; 5;+-----------------+--------------+| help_keyword_id | name |+-----------------+--------------+| 0 | HELP_DATE || 1 | HELP_VERSION || 2 | DEFAULT || 3 | SERIAL || 4 | VALUE |+-----------------+--------------+5 rows in set (0.00 sec)-- 首先help_keyword表查询出来的5条记录mysql&gt; select * from mysql.help_keyword hk where hk.help_keyword_id &lt; 5 and exists(select 1 from mysql.help_relation hr where hr.help_keyword_id = hk.help_keyword_id);+-----------------+--------------+| help_keyword_id | name |+-----------------+--------------+| 0 | HELP_DATE || 1 | HELP_VERSION || 2 | DEFAULT || 3 | SERIAL || 4 | VALUE |+-----------------+--------------+5 rows in set (0.00 sec)-- 加上exists语句后，每条记录都要当作exists的条件去执行select 1 from mysql.help_relation hr where hr.help_keyword_id = hk.help_keyword_id。如第一条记录hk.help_keyword_id = 0，则查询select 1 from mysql.help_relation hr where hr.help_keyword_id = 0，如下。可以查询出数据，因此exists返回true，因此在help_keyword表中help_keyword_id = 0的这条记录是合法的。mysql&gt; select 1 from mysql.help_relation hr where hr.help_keyword_id = 0;+---+| 1 |+---+| 1 |+---+1 row in set (0.00 sec)-- 同样对于剩余四条记录都要执行exists子语句，都能查询出记录，因此这四条记录都是合法的。-- 若将exists子语句中的条件修改为如下的100000，而不是外表的字段值，则exists子语句永远不能查出记录，exists子语句返回false，因此这五条记录都会被过滤掉。mysql&gt; select * from mysql.help_keyword hk where hk.help_keyword_id &lt; 5 and exists(select 1 from mysql.help_relation hr where hr.help_keyword_id = 100000);Empty set (0.00 sec) （3）not exists和exists正好相反。当子语句能查询出任意记录时，exists返回true，此时not exists就是false；当子语句不能查询出任意记录时，exists返回false，而not exists就是true； 123-- 还是上面那个例子，换成not exists后，对于每条外表查询出来的数据，not exists都为false，则外表五条记录都会被过滤掉。mysql&gt; select * from mysql.help_keyword hk where hk.help_keyword_id &lt; 5 and not exists(select 1 from mysql.help_relation hr where hr.help_keyword_id = hk.help_keyword_id);Empty set (0.00 sec) （4）从以上可以看出，外表查询出来的记录条数即为exists子语句的查询次数。 2、in的用法 （1）in和exists一样返回true或者false，用于判断某个或某几个字段是否存在于in的子语句查询记录中。如下： 1234567891011mysql&gt; select * from mysql.help_keyword hk where hk.help_keyword_id &lt; 5 and hk.help_keyword_id in (select hr.help_keyword_id from mysql.help_relation hr);+-----------------+--------------+| help_keyword_id | name |+-----------------+--------------+| 0 | HELP_DATE || 1 | HELP_VERSION || 2 | DEFAULT || 3 | SERIAL || 4 | VALUE |+-----------------+--------------+5 rows in set (0.00 sec) （2）in和exists不一样的地方在于，无论外表查询记录为多少条，in子语句只会查询一次，并将结果缓存起来，然后对于遍历外表查询出来的记录，判断in语句是否成立。如下： 123456789101112mysql&gt; select hr.help_keyword_id from mysql.help_relation hr; -- 首先会把该条语句查询出来的记录缓存起来+-----------------+| help_keyword_id |+-----------------+| 0 || 1 |......| 710 || 710 |+-----------------+1635 rows in set (0.00 sec)-- 然后遍历外表查询出来的五条数据，依次判断hk.help_keyword_id是否存在于缓存记录中。 （3）in相当于用or连接的=判断。因此in前面的字段是可以使用索引的。如下key字段都为primarykey。 1234567891011121314151617181920212223242526-- mysql.help_category表有如下索引mysql&gt; show index from mysql.help_category;+---------------+------------+----------+--------------+------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment | Visible | Expression |+---------------+------------+----------+--------------+------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+| help_category | 0 | PRIMARY | 1 | help_category_id | A | 44 | NULL | NULL | | BTREE | | | YES | NULL || help_category | 0 | name | 1 | name | A | 44 | NULL | NULL | | BTREE | | | YES | NULL |+---------------+------------+----------+--------------+------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+2 rows in set (0.00 sec)-- 因此以下两条语句都使用了key_name为name的索引。mysql&gt; explain select * from mysql.help_category hc where hc.name in (&#x27;Contents&#x27;, &#x27;Help Metadata&#x27;, &#x27;Data Types&#x27;);+----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+-------------+| 1 | SIMPLE | hc | NULL | range | name | name | 192 | NULL | 3 | 100.00 | Using where |+----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+-------------+1 row in set, 1 warning (0.00 sec)mysql&gt; explain select * from mysql.help_category hc where hc.name = &#x27;Contents&#x27; or hc.name = &#x27;Help Metadata&#x27; or hc.name = &#x27;Data Types&#x27;;+----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+-------------+| 1 | SIMPLE | hc | NULL | range | name | name | 192 | NULL | 3 | 100.00 | Using where |+----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+-------------+1 row in set, 1 warning (0.00 sec) 3、exists和in的区别 区别在于：in子语句只查询了一次，但是in的子语句会遍历整张help_relation表（遍历了1635次）；exists子语句会查询五次，但是每次查询都有where条件，where条件中可能会使用到索引，因此每次查询可能不用遍历整张表。例如： 12345678910111213141516-- 前提假设：假设true-- 情景1：-- 假设A表：一共100条记录，有主键字段aid，通过主键aid查询记录只需要遍历索引树1次。-- 假设B表：一共10000000条记录，有主键bid，字段aid，字段aid设置了索引，假设通过该索引进行查询时平均需要遍历索引树10次。select * from A where exists (select 1 from B where B.aid = A.aid);-- exists子语句遍历次数：100 * 10 = 1000，整条语句遍历次数：1000 + 100（判断100次ture or false） = 1100。select * from A where aid in (select aid from B);-- in子语句遍历次数：10000000，整条语句遍历次数：10000000 + 1（通过主键aid只需要遍历1次） = 10000001。-- 情景2：-- 假设A表：一共10000000条记录，有主键字段aid，通过主键aid查询记录只需要遍历索引树10次。-- 假设B表：一共100条记录，有主键bid，字段aid，字段aid设置了索引，假设通过该索引进行查询时平均需要遍历索引树2次。select * from A where exists (select 1 from B where B.aid = A.aid);-- exists子语句遍历次数：10000000 * 2 = 20000000，整条语句遍历次数：20000000 + 10000000（判断这么多次true or false）select * from A where aid in (select aid from B);-- in子语句遍历次数：100，整条语句遍历次数：100 + 10（通过主键aid只需要遍历10次） = 110。 情景1可以发现in语句虽然只查询一次，但是会遍历整张表，当B表很大时，in的效率会非常低，也更占用空间。情景2可以发现当A表很大时，虽然exists子语句只需要遍历2次，但是由于外表很大，导致整个查询过程中exists会遍历很多次，导致exists的效率很低。因此，当子查询表远大于外表时用exists，子查询表远小于外表时用in。无论时外表还是子查询里面的内表都要善用索引。","categories":[{"name":"mysql","slug":"mysql","permalink":"https://yury757.github.io/categories/mysql/"}],"tags":[]},{"title":"learn-git","slug":"Git/learngit/learn-git","date":"2019-12-09T16:00:00.000Z","updated":"2021-10-31T18:14:58.354Z","comments":true,"path":"Git/learngit/learn-git/","link":"","permalink":"https://yury757.github.io/Git/learngit/learn-git/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132mkdir &lt;dir&gt; // 创建空文件夹目录cd &lt;dir&gt; // 把路径转到目标目录pwd // 显示当前目录git config --list // 当前库配置列表git config --global --list // 全局配置列表git config --global user.name = &lt;name&gt; // 设置全局的用户名git config --global user.email = &lt;email&gt; // 设置全局的邮箱git init // 把当前目录初始化为一个可管理的资源库git add &lt;file&gt; // 向资源库中新增一个文件git add -f &lt;file&gt; // 想资源库中强制新增一个文件，忽略.gitignore文件中跳过的文件git add -A // 将所有变化添加到暂存区中git add -u // 将更新的内容和删除的文件添加到暂存区，新建的文件不会被添加到暂存区中git add . // 网上很多人说不能将删除的文件添加暂存区，但是测试是可以的，同时也可以将更新的内容和新建的文件增加到暂存区，不知道这个和-A有什么区别，我选择用-Agit status // 资源库当前状态，是否有修改git diff // 查看修改的内容git commit -m &lt;description&gt; // 提交，description为本次提交的说明git log // 查看每次修改的日志git reset --hard head^ // 回到当前上一个版本，^的数量代表回退版本的数量，要回退特别多可以用git reset --hard head~10 // 回退10个版本git reest --hard &lt;commit id&gt; // 前往&lt;commit id&gt;的特定版本，可以回退也可以前进，commit id没必要写全，前几位就可以了，Git会自动去找。当然也不能只写前一两位，因为Git可能会找到多个版本号，就无法确定是哪一个了。一般git hash窗口别关掉，可以看到之前提交。比如一次提交，其id为commitA，如果后来撤销了commitA，但是现在又想恢复commitA看看commit A的修改内容，就可以找到commitA的commit-id恢复就行，但是如果这个窗口删掉了，就不能重新应用commitA的提交了。cat &lt;file&gt; // 打印文件内容git reflog // 显示每次提交记录的情况git restore &lt;file&gt; // 将head版本的文件恢复到工作区git restore --staged &lt;file&gt; // 把暂存区的修改撤销掉（unstage）git rm &lt;file&gt; // 从资源库中移除文件，同样需要commitrm &lt;file&gt; // 从本地删除文件ssh-keygen -t rsa -C &quot;youremail@example.com&quot; // 为本地git创建一个SSH Key，windows用户在user\\.ssh中找到id_rsa和id_rsa.pub，分别是私钥和公钥，公钥可以告诉别人，私钥不能泄露出去；在github中新增这个密钥，就可以实现本地git仓库和github的传输ssh -T git@github.com // 测试密钥是否有用git remote add origin &lt;url&gt; // 将本地仓库添加到github远程仓库，origin是远程仓库的默认名称git pull origin master --allow-unrelated-histories // 如果远程库已经有内容了，要把本地库上传到github，则要先把代码拉到本地库融合解决冲突，然后执行下面的提交git push -u origin master // 将本地仓库的所有内容推送到远程库上，第一次要加“-u”，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以不用加“-u”。git clone &lt;url&gt; // 从github上复制代码文件夹到本地仓库，支持ssh协议和https协议git remote -v // 查看远程仓库连接，头单词为仓库名git remote show &lt;name&gt; // 查看远程name库的信息git remote remove &lt;name&gt; // 断开&lt;name&gt;远程库连接git push origin --delete &lt;name&gt; // 删除远程库origin中的&lt;name&gt;分支git branch -a // 查看远程仓库分支git branch // 查看分支git branch &lt;name&gt; // 创建新分支git switch &lt;name&gt; // 切换到另一个分支git switch -c &lt;name&gt; // 创建并切换到另一个分支git merge &lt;name&gt; // 将name分支的内容合并到当前分支，默认Fast-forward模式下是直接把指向master的指针指向devgit merge --no-ff &lt;name&gt; // 将name分支的内容强制以非快速模式合并到当前分支git branch -d &lt;name&gt; // 将name分支删除，若该分支没有被合并，会删除失败，提示你要合并，如果不想合并，见下一行代码git branch -D &lt;name&gt; // 危险操作！将name分支强制删除，不会提示有没有被合并git log --graph --abbrev-commit --decorate --format=format:&#x27;%C(bold blue)%h%C(reset) - %C(bold green)(%ar)%C(reset) %C(white)%s%C(reset) %C(dim white)- %an%C(reset)%C(bold yellow)%d%C(reset)&#x27; --all // 查看分支日志，--graph参数可以看日志图git stash // 将当前工作区保存另外一个区域（不是暂存区），将head版本的内容恢复到工作区git stash list // 查看stash区域的列表，是一个栈类型的区域，先进先出，最新进入stash区域的修改的序号为0，其他所有修改序号+1git stash apply &lt;stash@&#123;id&#125;&gt; // 将stash区域序号为id的修改恢复到工作区，若不加&lt;stash@&#123;id&#125;&gt;，则默认id=0git stash drop &lt;stash@&#123;id&#125;&gt; // 将stash区域序号为id的修改删除，若不加&lt;stash@&#123;id&#125;&gt;，则默认id=0git stash pop // 将stash区域序号为0的修改恢复到工作区，同时删除该stash修改。git cherry-pick &lt;commit-id&gt; // 将提交id为commit-id的修改应用于当前分支，比如当前在dev分支下开发新功能，突然发现master分支有一个bug，这个bug在dev分支同时也存在，可以将dev分支下的工作区先用stash保存，再切换到master分支，将bug修复，再将该修复的提交同时应用于dev分支，这样两个分支的bug都修复了，再用git stash pop就可以恢复原dev工作区的内容git branch --set-upstream-to=origin/&lt;githubbranch&gt; &lt;localgitbranch&gt; // 将本地分支&lt;localgitbranch&gt;和远程库origin的&lt;githubbranch&gt;分支建立链接，这样git pull拉代码时，就不会因为master之外的其他分支没有链接导致拉代码失败。如：git branch --set-upstream-to=origin/dev devgit branch --set-upstream &lt;localgitbranch&gt; origin/&lt;githubbranch&gt; // 同上，建议用这种，和下面这个代码一起记，方便一些git switch -c &lt;localgitbranch&gt; origin/&lt;githubbranch&gt; // 在本地创建和远程分支对应的分支，应该是会自动链接到远程分支上。多人协作通常的模式是：1、有一个远程库，你先拉下代码（如dev分支），若你本地对应的分支的提交记录的更新程序&gt;=远程库分支的提交记录，则是拉不下来代码的，会一直显示已经是最新的了。2、在本地新建自己的开发分支（如dev2）做自己的开发。2、等自己的开发完成后，将自己的开发分支dev2合并到本地dev中。3、用git push origin dev将开发完成的代码推到远程库dev分支。4、如果失败，看github上的dev分支的提交记录，是不是比你本地的dev新。如是则再要拉一遍代码git pull origin dev，如有冲突在本地解决冲突，然后就可以推到远程库。git tag &lt;name&gt; // 给当前分支的上一次提交打标签，标签名字为namegit tag -a &lt;name&gt; &lt;commit-id&gt; -m &lt;message&gt; // 用-a指定标签名&lt;name&gt;，用-m指定标签信息&lt;massage&gt;，以此给提交ID为&lt;commit-id&gt;的提交打标签，&lt;commit-id&gt;不用打双引号git tag // 显示所有分支的所有标签git show &lt;name&gt; // 显示名为&lt;name&gt;的tag的详细内容git push origin &lt;name&gt; // 把名为&lt;name&gt;的标签推送到远程库origin，&lt;name&gt;标签必须是本地库已经打过的标签git push origin --tags // 推送所有未推送的标签到远程库origingit tag -d &lt;name&gt; // 删除名为&lt;name&gt;的标签git push origin :refs/tags/&lt;name&gt; // 删除远程标签有两部操作：1、执行上一段代码，删除本地标签；2、执行本段代码，将远程的名为&lt;name&gt;的tag删除 .gitignore文件：在仓库根目录新建该文件，在文件中添加你想忽略添加到库中的文件，git就不会把这个文件添加到库中。但是如果你之前这个文件已经在库中了，要先把本地文件剪切至其他文件夹，删除库中的该文件，提交之后，再把文件剪切回来，git status命令就不会提示这个文件untracked。添加忽略的文件夹只需要这样添加：/folder","categories":[{"name":"git","slug":"git","permalink":"https://yury757.github.io/categories/git/"}],"tags":[]}],"categories":[{"name":"redis","slug":"redis","permalink":"https://yury757.github.io/categories/redis/"},{"name":"java","slug":"java","permalink":"https://yury757.github.io/categories/java/"},{"name":"other","slug":"other","permalink":"https://yury757.github.io/categories/other/"},{"name":"mysql","slug":"mysql","permalink":"https://yury757.github.io/categories/mysql/"},{"name":"bigdata","slug":"bigdata","permalink":"https://yury757.github.io/categories/bigdata/"},{"name":"linux","slug":"linux","permalink":"https://yury757.github.io/categories/linux/"},{"name":"JVM","slug":"java/JVM","permalink":"https://yury757.github.io/categories/java/JVM/"},{"name":"postgresql","slug":"postgresql","permalink":"https://yury757.github.io/categories/postgresql/"},{"name":"高性能MySQL","slug":"mysql/高性能MySQL","permalink":"https://yury757.github.io/categories/mysql/%E9%AB%98%E6%80%A7%E8%83%BDMySQL/"},{"name":"docker","slug":"docker","permalink":"https://yury757.github.io/categories/docker/"},{"name":"git","slug":"git","permalink":"https://yury757.github.io/categories/git/"}],"tags":[]}