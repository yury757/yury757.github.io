<!DOCTYPE html>
<html lang="en">

<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8">
	<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
	
	<!-- title -->
	
	<title>
	
		clickhouse | 
	 
	Yury&#39;s Blog
	</title>
	
	<!-- keywords,description -->
	 
		<meta name="description" content="blog of usual study" />
	

	<!-- favicon -->
	
	<link rel="shortcut icon" href="/favicon.ico">
	


	<!-- search -->
	<script>
		var searchEngine = "https://www.google.com/search?q=";
		if(typeof searchEngine == "undefined" || searchEngine == null || searchEngine == ""){
			searchEngine = "https://www.google.com/search?q=";
		}
		var homeHost = "";
		if(typeof homeHost == "undefined" || homeHost == null || homeHost == ""){
			homeHost = window.location.host;
		}
	</script>


	
<link rel="stylesheet" href="/css/main.css">

	
<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css">

	
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/styles/darcula.min.css">

	
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">


	
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/highlight.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/npm/jquery-pjax@2.0.1/jquery.pjax.min.js"></script>

	
<script src="/js/main.js"></script>

	
		
<script src="https://cdn.jsdelivr.net/npm/leancloud-storage/dist/av-min.js"></script>

		
<script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>

	
	
<meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Yury's Blog" type="application/atom+xml">
</head>

<body>
	<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?3efe99c287df5a1d6f0d02d187e403c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<header id="header">
    <a id="title" href="/" class="logo">Yury's Blog</a>

	<ul id="menu">
		<li class="menu-item">
			<a href="/about" class="menu-item-link">ABOUT</a>
		</li>
	
		<li class="menu-item">
			<a href="/tags" class="menu-item-link">标签</a>
		</li>
	

	
		<li class="menu-item">
			<a href="/categories" class="menu-item-link">分类</a>
		</li>
	

		<li class="menu-item">
			<a href="https://github.com/yury757" class="menu-item-link" target="_blank">
				<i class="fa fa-github fa-2x"></i>
			</a>
		</li>
	</ul>
</header>

	
<div id="sidebar">
	<button id="sidebar-toggle" class="toggle" ><i class="fa fa-arrow-right " aria-hidden="true"></i></button>
	
	<div id="site-toc">
		<input id="search-input" class="search-input" type="search" placeholder="按回车全站搜索">
		<div id="tree">
			

			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										Git
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										learngit
									</a>
									
							<ul>
								<li class="file">
									<a href="/Git/learngit/learn-git">
										learn-git
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										bigdata
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										clickhouse
									</a>
									
							<ul>
								<li class="file active">
									<a href="/bigdata/clickhouse/clickhouse">
										clickhouse
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										hadoop
									</a>
									
							<ul>
								<li class="file">
									<a href="/bigdata/hadoop/hadoop-study">
										hadoop-study
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										hbase
									</a>
									
							<ul>
								<li class="file">
									<a href="/bigdata/hbase/HBase-study">
										HBase-study
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										kafka
									</a>
									
							<ul>
								<li class="file">
									<a href="/bigdata/kafka/kafka">
										kafka
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										zookeeper
									</a>
									
							<ul>
								<li class="file">
									<a href="/bigdata/zookeeper/zookeeper-study">
										zookeeper-study
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										computer-science
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										Computer-Networking
									</a>
									
							<ul>
								<li class="file">
									<a href="/computer-science/Computer-Networking/Computer-Networking">
										Computer-Networking
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/computer-science/Computer-Networking/socket">
										socket
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										database
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										mysql
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										MySQL中的exists与in的使用
									</a>
									
							<ul>
								<li class="file">
									<a href="/database/mysql/MySQL%E4%B8%AD%E7%9A%84exists%E4%B8%8Ein%E7%9A%84%E4%BD%BF%E7%94%A8/MySQL%E4%B8%AD%E7%9A%84exists%E4%B8%8Ein%E7%9A%84%E4%BD%BF%E7%94%A8">
										MySQL中的exists与in的使用
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										MySQL备份-Linux版
									</a>
									
							<ul>
								<li class="file">
									<a href="/database/mysql/MySQL%E5%A4%87%E4%BB%BD-Linux%E7%89%88/MySQL%E5%A4%87%E4%BB%BD-linux%E7%89%88">
										MySQL备份-linux版
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										MySQL日期格式化
									</a>
									
							<ul>
								<li class="file">
									<a href="/database/mysql/MySQL%E6%97%A5%E6%9C%9F%E6%A0%BC%E5%BC%8F%E5%8C%96/MySQL%E6%97%A5%E6%9C%9F%E6%A0%BC%E5%BC%8F%E5%8C%96">
										MySQL日期格式化
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										MySQL查询执行计划详解-explain
									</a>
									
							<ul>
								<li class="file">
									<a href="/database/mysql/MySQL%E6%9F%A5%E8%AF%A2%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%E8%AF%A6%E8%A7%A3-explain/MySQL%E6%9F%A5%E8%AF%A2%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%E8%AF%A6%E8%A7%A3-explain">
										MySQL查询执行计划详解-explain
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										MySQL迁移数据目录
									</a>
									
							<ul>
								<li class="file">
									<a href="/database/mysql/MySQL%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE%E7%9B%AE%E5%BD%95/MySQL%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE%E7%9B%AE%E5%BD%95">
										MySQL迁移数据目录
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										《高性能MySQL》
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										第1章-MySQL架构与历史
									</a>
									
							<ul>
								<li class="file">
									<a href="/database/mysql/%E3%80%8A%E9%AB%98%E6%80%A7%E8%83%BDMySQL%E3%80%8B/%E7%AC%AC1%E7%AB%A0-MySQL%E6%9E%B6%E6%9E%84%E4%B8%8E%E5%8E%86%E5%8F%B2/%E7%AC%AC%E4%B8%80%E7%AB%A0-MySQL%E6%9E%B6%E6%9E%84%E4%B8%8E%E5%8E%86%E5%8F%B2">
										第一章-MySQL架构与历史
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										第2章-MySQL基准测试
									</a>
									
							<ul>
								<li class="file">
									<a href="/database/mysql/%E3%80%8A%E9%AB%98%E6%80%A7%E8%83%BDMySQL%E3%80%8B/%E7%AC%AC2%E7%AB%A0-MySQL%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20MySQL%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95">
										第二章 MySQL基准测试
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										第3章-服务器性能剖析
									</a>
									
							<ul>
								<li class="file">
									<a href="/database/mysql/%E3%80%8A%E9%AB%98%E6%80%A7%E8%83%BDMySQL%E3%80%8B/%E7%AC%AC3%E7%AB%A0-%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%80%A7%E8%83%BD%E5%89%96%E6%9E%90/%E7%AC%AC%E4%B8%89%E7%AB%A0-%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%80%A7%E8%83%BD%E5%89%96%E6%9E%90">
										第三章-服务器性能剖析
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										第4章-Schema与数据类型优化
									</a>
									
							<ul>
								<li class="file">
									<a href="/database/mysql/%E3%80%8A%E9%AB%98%E6%80%A7%E8%83%BDMySQL%E3%80%8B/%E7%AC%AC4%E7%AB%A0-Schema%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%BC%98%E5%8C%96/%E7%AC%AC%E5%9B%9B%E7%AB%A0-Schema%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%BC%98%E5%8C%96">
										第四章-Schema与数据类型优化
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										第5章-创建高性能的索引
									</a>
									
							<ul>
								<li class="file">
									<a href="/database/mysql/%E3%80%8A%E9%AB%98%E6%80%A7%E8%83%BDMySQL%E3%80%8B/%E7%AC%AC5%E7%AB%A0-%E5%88%9B%E5%BB%BA%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E7%B4%A2%E5%BC%95/%E7%AC%AC%E4%BA%94%E7%AB%A0-%E5%88%9B%E5%BB%BA%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E7%B4%A2%E5%BC%95">
										第五章-创建高性能的索引
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										postgresql
									</a>
									
							<ul>
								<li class="file">
									<a href="/database/postgresql/postgresql">
										postgresql
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										redis
									</a>
									
							<ul>
								<li class="file">
									<a href="/database/redis/redis%E5%91%BD%E4%BB%A4">
										redis命令
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										java
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										JVM
									</a>
									
							<ul>
								<li class="file">
									<a href="/java/JVM/jvm-1.8">
										jvm-1.8
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										SSM-Build
									</a>
									
							<ul>
								<li class="file">
									<a href="/java/SSM-Build/SSM-Build">
										SSM-Build
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="file">
									<a href="/java/java-%E6%96%87%E4%BB%B6IO%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%AF%B9%E6%AF%94">
										java-文件IO常用操作对比
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										java_JUC
									</a>
									
							<ul>
								<li class="file">
									<a href="/java/java_JUC/Java_JUC-study">
										Java_JUC-study
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										java_NIO
									</a>
									
							<ul>
								<li class="file">
									<a href="/java/java_NIO/Java_NIO-study">
										Java_NIO-study
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="file">
									<a href="/java/java%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE">
										java常用配置
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										mybatis
									</a>
									
							<ul>
								<li class="file">
									<a href="/java/mybatis/mybatis-study">
										mybatis-study
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										springMVC
									</a>
									
							<ul>
								<li class="file">
									<a href="/java/springMVC/SpringMVC-Study">
										SpringMVC-Study
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										springboot
									</a>
									
							<ul>
								<li class="file">
									<a href="/java/springboot/springboot-study">
										springboot-study
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										spring
									</a>
									
							<ul>
								<li class="file">
									<a href="/java/spring/Spring-study">
										Spring-study
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="file">
									<a href="/jetbrains%E7%B3%BB%E5%88%97IDE%E6%8E%A8%E8%8D%90%E8%AE%BE%E7%BD%AE">
										jetbrains系列IDE推荐设置
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										linux
									</a>
									
							<ul>
								<li class="file">
									<a href="/linux/linux%E5%91%BD%E4%BB%A4">
										linux命令
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/linux/%E8%AE%B0%E4%B8%80%E6%AC%A1%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E6%8C%96%E7%9F%BF%E7%9A%84%E7%BB%8F%E5%8E%86">
										记一次服务器被挖矿的经历
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										operating-support
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										docker
									</a>
									
							<ul>
								<li class="file">
									<a href="/operating-support/docker/docker">
										docker
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										project
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										flink_data_warehouse
									</a>
									
							<ul>
								<li class="file">
									<a href="/project/flink_data_warehouse/flink_data_warehouse">
										flink_data_warehouse
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="file">
									<a href="/%E5%B8%B8%E7%94%A8%E7%BB%84%E4%BB%B6%E7%AB%AF%E5%8F%A3%E5%8F%B7">
										常用组件端口号
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
		</div>
	</div>
</div>

	<!-- 引入正文 -->
	<div id="content">
		<h1 id="article-title">

	clickhouse
</h1>
<div class="article-meta">
	
	<span>Yury</span>
	<span>2021-11-21 00:00:00</span>
		<div id="article-categories">
    
		<span>Categories：</span>
            
                
                    <span>
                        <i class="fa fa-folder" aria-hidden="true">
                        <a href="/categories/bigdata/">bigdata</a>
                        </i>
                      
                    </span>
                
            
    

    
		<span>Tags：</span>
            
    
		</div>

</div>

<div id="article-content">
	<p>clickhouse版本：21.7.3.14</p>
<h2 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h2><h3 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h3><p>基于<strong>列存储</strong>的数据库，使用C++编写，主要用于<strong>在线分析处理查询</strong>（OLAP），能够使用<strong>SQL查询</strong>实时生成分析报告。</p>
<p>列式存储的优点：</p>
<ul>
<li>对于列的聚合、计数等统计操作由于行式数据库</li>
<li>由于某一列的数据类型是一样的，在数据压缩上效率更高，压缩比更大，缓存cache也有更大的发挥空间</li>
</ul>
<p>列式存储的缺点：插入、更新速度比行式数据库更慢</p>
<h3 id="2、高吞吐写入能力"><a href="#2、高吞吐写入能力" class="headerlink" title="2、高吞吐写入能力"></a>2、高吞吐写入能力</h3><p>clickhouse采用类<strong>LSM Tree</strong>的结构，数据写入后定期在后台compation。clickhouse在导入数据时全部都是<strong>顺序append写入</strong>，写入后数据段不可更改，在后台compation时也是多个段merge sort后顺序写回磁盘。顺序写的特性，充分利用了磁盘的吞吐能力。</p>
<h3 id="3、数据分区和线程级并行"><a href="#3、数据分区和线程级并行" class="headerlink" title="3、数据分区和线程级并行"></a>3、数据分区和线程级并行</h3><p>clickhouse将数据划分为多个partition，每个partition再进一步划分为多个index granularity（索引粒度），然后通过多个线程分别处理其中一部分来实现并行数据处理。这种设计下，单条query就可以利用整机所有的CPU资源。对于大量数据的查询也能够化整为零的并行处理。</p>
<p><font color="Red">缺点：由于一条SQL就会占用所有cpu，因此对于qps高的业务并不适合。</font></p>
<h3 id="4、使用场景"><a href="#4、使用场景" class="headerlink" title="4、使用场景"></a>4、使用场景</h3><p>不适用于初始数据存储，而适用于最后的宽表存储，用来查询用。</p>
<p>适用于clickhouse的业务：具有复杂统计逻辑的查询sql，需要查询大量数据的sql，并发量低</p>
<p>适用于hbase的业务：业务很简单的查询，一般就是key-value一一对应的查询，并发量高</p>
<p>clickhouse还有一个特点就是，<font color="Red">单表查询速度及其快，但是多表join操作比较慢</font>，在了解join原理后，可以通过优化sql来优化join速度。</p>
<h2 id="二、安装部署"><a href="#二、安装部署" class="headerlink" title="二、安装部署"></a>二、安装部署</h2><h3 id="1、准备工作"><a href="#1、准备工作" class="headerlink" title="1、准备工作"></a>1、准备工作</h3><h4 id="（1）ulimit"><a href="#（1）ulimit" class="headerlink" title="（1）ulimit"></a>（1）ulimit</h4><p>linux取消一些系统资源限制：</p>
<pre><code class="shell"># 查看系统资源限制
ulimit -a
# open files                      (-n) 1024
# max user processes              (-u) 7625

# 修改系统资源限制
sudo vi /etc/security/limits.conf

# 加上以下配置
* soft nofile 65535
* hard nofile 65535
* soft nproc  131072
* hard nproc  131072

# 有些linux系统在/etc/security/limit.d目录下面还有20-npric.conf或90-npric.conf
# 这两个配置会把limits.conf配置覆盖了，所以如果有的话这两个也要加上以上配置
sudo vi /etc/security/limits.d/20-nproc.conf

# 修改之后不需要重启，重新登录该用户即可，再查看ulimit -a是否修改成功
</code></pre>
<h4 id="（2）SELINUX"><a href="#（2）SELINUX" class="headerlink" title="（2）SELINUX"></a>（2）SELINUX</h4><p>sentos取消SELINUX（linux的security enforce）</p>
<p>SELINUX并不是安装里所有linux都会有，没有SELINUX不需要执行以下修改。</p>
<pre><code class="shell"># 查看security enfore
getenforce

sudo vi /etc/selinux/config

# 将SELINUX修改为disabled
SELINUX=disabled

# 重启永久生效，输入以下命令临时生效
setenforce 0
</code></pre>
<h4 id="（3）版本"><a href="#（3）版本" class="headerlink" title="（3）版本"></a>（3）版本</h4><p>clickhouse版本更新比较快</p>
<p>20.6.3 新增explain，类似于MySQL的explain</p>
<p>20.8 新增同步MySQL功能等</p>
<p>我们使用的版本是21.7.3.14</p>
<h3 id="2、下载安装"><a href="#2、下载安装" class="headerlink" title="2、下载安装"></a>2、下载安装</h3><p>下载地址：<a target="_blank" rel="noopener" href="https://repo.clickhouse.com/deb/stable/main/">Index of /clickhouse/deb/stable/main/</a></p>
<pre><code class="shell">cd /home/yury/clickhouse

wget https://repo.clickhouse.com/deb/stable/main/clickhouse-common-static_21.7.3.14_amd64.deb
wget https://repo.clickhouse.com/deb/stable/main/clickhouse-client_21.7.3.14_all.deb
wget https://repo.clickhouse.com/deb/stable/main/clickhouse-server_21.7.3.14_all.deb
# 下面这个包是带有调试信息的包，可以不装
wget https://repo.clickhouse.com/deb/stable/main/clickhouse-common-static-dbg_21.7.3.14_amd64.deb

sudo dpkg -i *.deb

# 一路yyy，最后要输入一个默认用户的密码，可以设置密码，也可以直接回车不设置密码

# 安装成功后，软件被安装到目录下，lib目录在/var/lib/clickhouse/
</code></pre>
<h3 id="3、常用路径"><a href="#3、常用路径" class="headerlink" title="3、常用路径"></a>3、常用路径</h3><p>bin目录：<code>/usr/bin/</code></p>
<p>服务器安装目录：<code>/etc/clickhouse-server/</code></p>
<p>客户端安装目录：<code>/etc/clickhouse-client/</code></p>
<p>配置文件目录：<code>/etc/clickhouse-server/</code>，所有其他配置目录都可以在这个配置文件中设置</p>
<p>日志目录：<code>/var/log/clickhouse-server/clickhouse-server.log</code></p>
<p>报错日志目录：<code>/var/log/clickhouse-server/clickhouse-server.err.log</code></p>
<p>依赖目录：<code>/var/lib/clickhouse/</code></p>
<p>数据目录：<code>/var/lib/clickhouse/</code>，即config.xml配置文件中path标签下</p>
<p>临时文件目录：<code>/var/lib/clickhouse/tmp/</code>，即config.xml配置文件中tmp_path标签下</p>
<h3 id="4、配置"><a href="#4、配置" class="headerlink" title="4、配置"></a>4、配置</h3><p><font color="Red">config.xml中的配置是服务器配置，比如上面说的数据目录、日志目录都在这个配置中，而user.xml则是程序运行参数配置，如cpu、内存在这个里面配置。</font></p>
<p>config.xml配置文档：<a target="_blank" rel="noopener" href="https://clickhouse.com/docs/en/operations/server-configuration-parameters/settings/">Server Settings | ClickHouse Documentation</a></p>
<p>user.xml配置文档：<a target="_blank" rel="noopener" href="https://clickhouse.com/docs/en/operations/settings/settings/">Settings | ClickHouse Documentation</a></p>
<p>修改一个配置，使得其他ip地址也可以访问clickhouse服务器</p>
<pre><code class="shell">sudo vi /etc/clickhouse-server/config.xml

# 将下面这个配置取消注释
&lt;listen_host&gt;::&lt;/listen_host&gt;
</code></pre>
<p><font color="Red">此外再把tcp_port端口改为9003，因为默认的9000端口很容易被其他应用使用，比如hadoop就使用了9000端口。此外用，clickhosue-client连接时用tcp连接，而使用其他连接，比如datagrip或java，都是用的http连接，注意端口的使用。</font></p>
<pre><code class="xml">&lt;tcp_port&gt;9003&lt;/tcp_port&gt;
</code></pre>
<h3 id="5、简单使用"><a href="#5、简单使用" class="headerlink" title="5、简单使用"></a>5、简单使用</h3><pre><code class="shell"># 启动
sudo clickhouse start

# 查看服务器运行状态
sudo clickhouse status
# /var/run/clickhouse-server/clickhouse-server.pid file exists and contains pid = 7116.
# The process with pid = 7116 is running.

# 关闭
sudo clickhouse stop

# 重启
sudo clickhouse restart

# 常加-m参数，这样可以在sql语句中使用换行，否则要用分号分隔
# -h参数为服务器host，默认是localhost
# --port参数为tcp端口，默认是9000
clickhouse-client -m -h myubuntu1 --port 9003
</code></pre>
<p>clickhouse-client还有一个快捷方式的参数query：</p>
<pre><code class="shell">clickhouse-client -m -h 192.168.141.141 --port 9003 --query &quot;show databases;&quot;
default
system
</code></pre>
<p>clickhouse语法比较像MySQL，在某些语法上clickhouse区分大小写，比如数据类型UInt、String，以及表引擎MergeTree等等。</p>
<pre><code class="sql">show databases;
use default;
show tables;

create table test2(
    id UInt32,
    create_time Datetime default now(),
    name String,
    age UInt8,
    money Decimal(24,6) TTL create_time + interval 10 SECOND
)engine=MergeTree
primary key(id)
order by(id, name);
</code></pre>
<p>使用shell命令快速写一个批量插入100万条数据sql文件。</p>
<pre><code class="shell">for i in &#123;1..1000&#125;; do
echo -n &quot;insert into default.test2(id, name, age, money) values (1, &#39;小明&#39;, 14, 2000)&quot; &gt;&gt; test.sql;
for j in &#123;1..1000&#125;; do echo -n &quot;,($&#123;i&#125;$&#123;j&#125;, &#39;小明$&#123;i&#125;$&#123;j&#125;&#39;, $&#123;i&#125;$&#123;j&#125;, $&#123;i&#125;$&#123;j&#125;2000)&quot; &gt;&gt; test.sql; done;
echo &quot;;&quot; &gt;&gt; test.sql;
done;
</code></pre>
<p>执行sql文件（速度贼快）</p>
<pre><code class="shell">clickhouse-client -m -h 192.168.141.141 --port 9003 --multiquery &lt; ./test.sql
</code></pre>
<p>执行几个group查询（速度贼快）</p>
<p><font color="Red">注意，这里substring也是按照字节数来算长度的。</font></p>
<pre><code class="sql">select substring(name, 1, 6), sum(age) from test2 group by substring(name, 1, 6);
select substring(name, 7, 1), sum(age) from test2 group by substring(name, 7, 1);
select id % 2 as flag, sum(age) from test2 group by (id % 2);
</code></pre>
<h2 id="三、数据类型"><a href="#三、数据类型" class="headerlink" title="三、数据类型"></a>三、数据类型</h2><p>文档：<a target="_blank" rel="noopener" href="https://clickhouse.com/docs/en/sql-reference/data-types/">Introduction | ClickHouse Documentation</a></p>
<h3 id="1、整形"><a href="#1、整形" class="headerlink" title="1、整形"></a>1、整形</h3><p>（1）有符号整型</p>
<table>
<thead>
<tr>
<th>clickhouse类型</th>
<th>范围</th>
<th>对应MySQL类型</th>
</tr>
</thead>
<tbody><tr>
<td>Int8</td>
<td>-128 : 127 (2^7)</td>
<td>tinyint</td>
</tr>
<tr>
<td>Int16</td>
<td>-32768 : 32767(2^15)</td>
<td>smallint</td>
</tr>
<tr>
<td>Int32</td>
<td>-2147483648 : 2147483647 (2^31)</td>
<td>int</td>
</tr>
<tr>
<td>Int64</td>
<td>-9223372036854775808 : 9223372036854775807 (2^63)</td>
<td>bigint</td>
</tr>
<tr>
<td>Int128</td>
<td>-2^127 : 2^127-1</td>
<td>无</td>
</tr>
<tr>
<td>Int256</td>
<td>-2^255 : 2^255-1</td>
<td>无</td>
</tr>
</tbody></table>
<p>（2）无符号整形</p>
<p>UInt8、UInt16、UInt32、UInt64，UInt128、UInt256，范围分别是0 : (2^n)-1。</p>
<p>128位和256位的一般用不到。</p>
<p>clickhouse没有布尔类型，官方建议用UInt8来存储，0代表false，1代表true。</p>
<h3 id="2、浮点型"><a href="#2、浮点型" class="headerlink" title="2、浮点型"></a>2、浮点型</h3><table>
<thead>
<tr>
<th>clichouse类型</th>
<th>对应MySQL类型</th>
</tr>
</thead>
<tbody><tr>
<td>Float32</td>
<td>float</td>
</tr>
<tr>
<td>Float64</td>
<td>double</td>
</tr>
</tbody></table>
<h3 id="3、Decimal"><a href="#3、Decimal" class="headerlink" title="3、Decimal"></a>3、Decimal</h3><table>
<thead>
<tr>
<th>clickhouse类型</th>
<th>整数位+小数位</th>
<th>小数位</th>
<th>对应MySQL类型</th>
</tr>
</thead>
<tbody><tr>
<td>Decimal(P,S)</td>
<td>P</td>
<td>S</td>
<td>decimal(P,S)</td>
</tr>
<tr>
<td>Decimal32(S)</td>
<td>9</td>
<td>S</td>
<td>decimal(9,S)</td>
</tr>
<tr>
<td>Decimal64(S)</td>
<td>18</td>
<td>S</td>
<td>decimal(18,S)</td>
</tr>
<tr>
<td>Decimal128(S)</td>
<td>38</td>
<td>S</td>
<td>decimal(38,S)</td>
</tr>
<tr>
<td>Decimal128(S)</td>
<td>76</td>
<td>S</td>
<td>decimal(76,S)</td>
</tr>
</tbody></table>
<p>不同位数长度的Decimal进行运算时，最终结果的位数长度是最大的那个。</p>
<h3 id="4、字符串类型"><a href="#4、字符串类型" class="headerlink" title="4、字符串类型"></a>4、字符串类型</h3><table>
<thead>
<tr>
<th>clickhouse</th>
<th>字节长度</th>
<th>对应MySQL类型</th>
</tr>
</thead>
<tbody><tr>
<td>String</td>
<td>无限制</td>
<td>varchar、所有text、所有blob</td>
</tr>
<tr>
<td>FixedString(N)</td>
<td>N</td>
<td>char(N)</td>
</tr>
</tbody></table>
<p>FixedString(N)类型，<font color="Red">这个N是字节长度，而不是字符长度</font>。当存入字符的字节长度小于N时，会用空字节（<code>\0</code>）补齐。一般很少用FixedString(N)，就像MySQL很少使用char(N)一样。</p>
<p><font color="Red">clickhouse没有编码的概念，即它存储字符串时是以二进制的形式存储。clickhosue在计算长度时，length函数是计算编码后字节的长度，lengthUTF8函数才是计算字符的长度，且只有一个计算UTF8编码的函数，因此服务器一定要使用utf-8编码。</font></p>
<p>clickhouse还有一个专门的存储UUID的类型：<strong>UUID</strong>，以及生成UUID的函数<code>generateUUIDv4()</code>。</p>
<pre><code class="sql">CREATE TABLE t_uuid (x UUID, y String) ENGINE=TinyLog
</code></pre>
<h3 id="5、日期类型"><a href="#5、日期类型" class="headerlink" title="5、日期类型"></a>5、日期类型</h3><table>
<thead>
<tr>
<th>clickhouse类型</th>
<th>字节长度</th>
<th>时间范围</th>
<th>对应MySQL类型</th>
</tr>
</thead>
<tbody><tr>
<td>Date</td>
<td>2</td>
<td>1970-01-01至2148-12-31</td>
<td></td>
</tr>
<tr>
<td>Date32</td>
<td></td>
<td>1925-01-01至2283-11-11</td>
<td></td>
</tr>
<tr>
<td>DateTime([timezone])</td>
<td></td>
<td>1970-01-01 00:00:00至2105-12-31 23:59:59</td>
<td>timestamp</td>
</tr>
<tr>
<td>DateTime64(precision, [timezone])</td>
<td></td>
<td>1925-01-01 00:00:00至2283-11-11 23:59:59</td>
<td></td>
</tr>
<tr>
<td>Interval</td>
<td></td>
<td>无，是一个时间长度含义</td>
<td>interval</td>
</tr>
</tbody></table>
<p>timezone是指时区，默认使用配置文件中设置的时区，或者操作系统时区。</p>
<p>precision是指秒后面的时间精度。</p>
<p>Interval的使用和MySQL的interval类型一样使用。</p>
<pre><code class="sql">select now() + interval 4 DAY + interval 3 HOUR;
┌─plus(plus(now(), toIntervalDay(4)), toIntervalHour(3))─┐
│                                    2021-11-26 01:15:52 │
└────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="6、枚举类型"><a href="#6、枚举类型" class="headerlink" title="6、枚举类型"></a>6、枚举类型</h3><table>
<thead>
<tr>
<th>clickhouse类型</th>
<th>枚举值个数</th>
<th>MySQL类型</th>
</tr>
</thead>
<tbody><tr>
<td>Enum8（别名：Enum）</td>
<td>256</td>
<td>enum</td>
</tr>
<tr>
<td>Enum16</td>
<td>65536</td>
<td>enum</td>
</tr>
</tbody></table>
<pre><code class="sql">CREATE TABLE t_enum
(
    x Enum(&#39;hello&#39; = 1, &#39;world&#39; = 2)
)
ENGINE = TinyLog
</code></pre>
<p>clickhouse和MySQL在存储枚举值时，都是存储的对应的数值，而clickhouse是手动设置枚举值对应的数字值，MySQL则是系统设定的。</p>
<h3 id="7、LowCardinality"><a href="#7、LowCardinality" class="headerlink" title="7、LowCardinality"></a>7、LowCardinality</h3><pre><code>LowCardinality(data_type)
</code></pre>
<p>该数据类型，将存储的值，设置字典索引，实际存储则存储其索引数字即可。</p>
<p>比如LowCardinality(String)类型字段，在insert了“小明”，“小红”，“小光”三个字符时，首先会创建一个字典，将这三个字符写入字典，如下：</p>
<pre><code class="java">&#123;
    1: &quot;小明&quot;,
    2: &quot;小红&quot;,
    3: &quot;小光&quot;,
&#125;
</code></pre>
<p>key为索引序号，value为实际值。而实际存储在数据文件中，则是存储的其索引。</p>
<p><font color="Red">当value有很多重复值时，这种方式不仅可以节省很多存储空间，还可以加快读取速度，以及对该字段进行过滤、分组和某些查询的速度。</font></p>
<p><strong>值得一提的是，微软office2007版本的xlsx类型文件底层的字符串格式的存储也是采用这种方式，专门使用一个sharedString.xml来存储所有字符串，作为一个字典，而在主体的存储文件中使用索引。</strong></p>
<p>但是这种类型，写入速度会比一般普通类型写入慢一些，因此<strong>特别不建议在重复度很低的字段上使用这个类型</strong>。</p>
<p>这种类型和Enum比较类似，但是LowCadinality更加灵活，不仅可以存储String，还可以存储Date、DateTime，只要重复度很高，则可以使用这种类型。</p>
<h3 id="8、Nullable-T"><a href="#8、Nullable-T" class="headerlink" title="8、Nullable(T)"></a>8、Nullable(T)</h3><p>Nullable(T)，可为空的类型，如Nullable(Int8)是可为空值的Int8</p>
<p><font color="Red">使用Nullable会对性能产生影响，业务中可以用一些特殊字符或无意义的值来填充null，从而避免使用Nullable类型。</font></p>
<p>原因：1、Nullable会单独存一个文件；2、null无法使用索引。</p>
<p><font color="Red">注意：每种数据类型都会有自带的默认值，且不是null，如Int8的默认值是0，String的默认值是空字符串，DateTime的默认值是1970-01-01 08:00:00。如果需要为null，则需要使用Nullable(T)类型，Nullable的默认值则是null。</font></p>
<pre><code class="sql">use default;
create table test1(a Int8, b String, c DateTime, d Nullable(Int8), e Nullable(String) , f Nullable(DateTime))engine=TinyLog;
insert into test1 values(1, &#39;nihao&#39;, &#39;2020-12-31&#39;, 1, &#39;nihao&#39;, &#39;2020-12-31&#39;);
insert into test1 values(null, null, null, null, null, null);
select * from test1;
┌─a─┬─b─────┬───────────────────c─┬────d─┬─e─────┬───────────────────f─┐
│ 1 │ nihao │ 2020-12-31 00:00:00 │    1 │ nihao │ 2020-12-31 00:00:00 │
│ 0 │       │ 1970-01-01 08:00:00 │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ  │                ᴺᵁᴸᴸ │
└───┴───────┴─────────────────────┴──────┴───────┴─────────────────────┘
</code></pre>
<p>判断是否为null的格式如下，如果不是Nullable类型，则没有.null这个字段，会报错。</p>
<pre><code class="sql">select * from test1 where d.null = 1;
┌─a─┬─b─┬───────────────────c─┬────d─┬─e────┬────f─┐
│ 0 │   │ 1970-01-01 08:00:00 │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │
└───┴───┴─────────────────────┴──────┴──────┴──────┘
</code></pre>
<h3 id="9、其他类型"><a href="#9、其他类型" class="headerlink" title="9、其他类型"></a>9、其他类型</h3><p>Array(T)，数组</p>
<p>AggregateFunction和SimpleAggregateFunction，比较重要，可以在表引擎那里再理解</p>
<p>Nested，类似于c++的struct</p>
<p>Tuple，即可以存不同数据类型的Array</p>
<p>Expression，存储了lamdba表达式</p>
<p>Set，集合，不可重复</p>
<p>Nothing，官方解释，此数据类型的唯一目的是表示不需要值的情况，好像没啥用</p>
<p>IPV4，ipv4专用类型，基于UInt32存储</p>
<p>IPV6，ipv6专用类型</p>
<p>GEO，坐标类型，包括Point、Ring、Polygon、MultiPolygon</p>
<p>Map(K, V)，映射表类型</p>
<h3 id="10、特殊值"><a href="#10、特殊值" class="headerlink" title="10、特殊值"></a>10、特殊值</h3><p>此外clickhouse还有一些特殊值，如<code>Inf</code>、<code>-Inf</code>和<code>NaN</code>，即<code>infinity</code>、<code>negative infinity</code>和非数字类型。</p>
<pre><code class="sql">select 1 / 0 as a, -1 / 0 as b, 0 / 0 as c;
┌───a─┬────b─┬───c─┐
│ inf │ -inf │ nan │
└─────┴──────┴─────┘
</code></pre>
<h3 id="11、相关函数"><a href="#11、相关函数" class="headerlink" title="11、相关函数"></a>11、相关函数</h3><table>
<thead>
<tr>
<th>函数名</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>toTypeName(field)</td>
<td>获field字段的类型</td>
</tr>
<tr>
<td>cast(value, T) or cast(value as T)</td>
<td>将value强制转换为T类型</td>
</tr>
<tr>
<td>extract(part from date)</td>
<td>从日期中获取年、月、日</td>
</tr>
<tr>
<td>uniq(field)</td>
<td>取distinct</td>
</tr>
</tbody></table>
<pre><code class="sql">select cast(&#39;1&#39;, &#39;Int8&#39;) as x, toTypeName(x) as type;
┌─x─┬─type─┐
│ 1 │ Int8 │
└───┴──────┘

select extract(DAY from now());
┌─toDayOfMonth(now())─┐
│                  21 │
└─────────────────────┘
</code></pre>
<h2 id="四、表引擎"><a href="#四、表引擎" class="headerlink" title="四、表引擎"></a>四、表引擎</h2><p>文档：<a target="_blank" rel="noopener" href="https://clickhouse.com/docs/en/engines/table-engines/">Introduction | ClickHouse Documentation</a></p>
<p>表引擎决定了：</p>
<ul>
<li>数据存储的方式和位置，写到哪里以及从哪里读取数据</li>
<li>支持哪些查询，以及如何支持（有一些特殊的查询需要特殊的表引擎才能支持）</li>
<li>并发数据访问</li>
<li>索引的使用</li>
<li>是否可以使用多线程执行（有些引擎，在查询一条sql时会多线程执行）</li>
<li>数据复制参数</li>
</ul>
<p>表引擎有<strong>集成引擎、日志系列引擎、MergeTree系列引擎和一些特殊引擎</strong>。</p>
<h3 id="1、集成引擎"><a href="#1、集成引擎" class="headerlink" title="1、集成引擎"></a>1、集成引擎</h3><p>集成引擎主要是为了继承其他组件而使用的，比如集成MySQL、jdbc、kafka等。集成的本质就是，将MySQL等源数据和clickhouse表做<strong>一层映射</strong>，然后就可以直接用clickhouse查询源数据。</p>
<h3 id="2、日志系列引擎"><a href="#2、日志系列引擎" class="headerlink" title="2、日志系列引擎"></a>2、日志系列引擎</h3><p>日志引擎是为了需要快速写一些小表（小于100万）而使用的引擎，就像名字一样，多用于日志存储。</p>
<p>如TinyLog，以列文件的形式保存在磁盘上，<strong>不支持索引，没有并发控制</strong>。一般保存少量数据，生产环境很少使用。</p>
<h3 id="3、特殊引擎"><a href="#3、特殊引擎" class="headerlink" title="3、特殊引擎"></a>3、特殊引擎</h3><p>很多，具体见官方文档。</p>
<p>如Memory，内存引擎，将数据以未经压缩的形式直接存储在内存中，<strong>不支持索引，读写操作不会阻塞，简单查询性能非常高</strong>！</p>
<h3 id="4、MergeTree系列（重要！！）"><a href="#4、MergeTree系列（重要！！）" class="headerlink" title="4、MergeTree系列（重要！！）"></a>4、MergeTree系列（重要！！）</h3><p>文档：<a target="_blank" rel="noopener" href="https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/">Introduction | ClickHouse Documentation</a></p>
<h4 id="（1）MergeTree合并树"><a href="#（1）MergeTree合并树" class="headerlink" title="（1）MergeTree合并树"></a>（1）MergeTree合并树</h4><p>这种表引擎在建表时，必须要加order by。</p>
<pre><code class="sql">CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
(
    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1],
    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2],
    ...
    INDEX index_name1 expr1 TYPE type1(...) GRANULARITY value1,
    INDEX index_name2 expr2 TYPE type2(...) GRANULARITY value2,
    ...
    PROJECTION projection_name_1 (SELECT &lt;COLUMN LIST EXPR&gt; [GROUP BY] [ORDER BY]),
    PROJECTION projection_name_2 (SELECT &lt;COLUMN LIST EXPR&gt; [GROUP BY] [ORDER BY])
) ENGINE = MergeTree()
ORDER BY expr
[PARTITION BY expr]
[PRIMARY KEY expr]
[SAMPLE BY expr]
[TTL expr
    [DELETE|TO DISK &#39;xxx&#39;|TO VOLUME &#39;xxx&#39; [, ...] ]
    [WHERE conditions]
    [GROUP BY key_expr [SET v1 = aggr_func(v1) [, v2 = aggr_func(v2) ...]] ] ]
[SETTINGS name=value, ...]
</code></pre>
<p><strong>表底层存储文件夹和文件介绍：</strong></p>
<p><font color="Red">分区是按文件夹存储的，一个分区一个文件夹</font>，文件夹的名称为<code>分区id_最小分区块编号_最大分区块编号_合并层级</code></p>
<p>分区id生成规则：</p>
<ul>
<li>没有设置分区，则默认生成一个all目录作为数据分区</li>
<li>整形分区，以该整形值的字符串形式作为分区id</li>
<li>日期分区，分区id为日期的yyyymmdd形式</li>
<li>其他分区，如String、Float等，以其128位hash值为分区id</li>
</ul>
<p>最小分区块编号：分区的最小分区编号，适用于分区合并</p>
<p>最大分区块编号：分区的最大分区编号，适用于分区合并</p>
<p>合并层级：被合并的次数。<font color="Red">插入数据时，并不会直接将数据插入到对应分区中，而是会生成一个临时分区，等服务器空闲或者一段时间后，再将临时分区合并到原有的分区文件中。和hbase的regionserver类似</font></p>
<p><strong>通过手动执行<code>optimize table xxx final</code>可以手动进行数据合并。</strong></p>
<p><strong>目录文件内各文件介绍：</strong></p>
<ul>
<li>bin文件：数据文件</li>
<li>mrk文件：标记文件，标记文件在idx索引文件和bin文件之间起到了桥梁作用，一般记录列的offset，用于加速查询。以mrk2结尾的文件，表示该表启动了自适应索引间隔。</li>
<li>primary.idx文件：主键索引文件，用于加快查询效率</li>
<li>minmax_create_time.idx：分区键的最大值最小值</li>
<li>checksum.txt：校验文件，用于校验各个文件的正确性。存放各个文件的size和hash值。</li>
<li>count.txt：记录了该表的总数据量</li>
<li>columns.txt：记录了该表的列信息</li>
<li>default_compression_codec.txt：记录了数据文件中使用的压缩编码器</li>
<li>partition.dat：记录了分区信息</li>
</ul>
<p><strong>主键索引</strong></p>
<p>clickhouse的主键索引<strong>并不是唯一索引。主键索引是稀疏索引</strong>，即并不是将这一列的所有值建索引，而是一部分值，查找时通过类似于二分查找的方式确定数据所在区间，再扫描这个区间找到对应的值。</p>
<p>索引有一个index granularity，即索引稀疏粒度，即稀疏索引记录值时每次跳过的行数，默认值为8192。粒度越细，索引数据量存储的越大，查询效率越高；粒度越粗，索引数据量越小，查询效率越低。当很多重复值时，需要适当提高粒度。</p>
<p><strong>order by（MergeTree系列引擎最重要的字段）</strong></p>
<p>主键索引是稀疏索引，这种索引的查找方式需要排序，因此表必须有一个字段进行了排序，这就是为什么order by是MergeTree引擎的必选信息。<font color="Red">主键字段必须是order by的前缀字段</font>，原因和MySQL可以使用最左前缀字段作为索引原因一样。</p>
<p><strong>二级索引（数据跳跃索引）</strong></p>
<p>老版本需要设置<code>set allow_experimental_data_skipping_indices = 1</code>开启。</p>
<p>v20.1.2.4及以后，这个参数已经被删除了</p>
<p>二级索引的原理：以索引粒度为单位，记录每个区间的最大值和最小值，当以这个字段为条件查询时，只需要比较值是否落在这个区间就行（比较两次），若落在这个区间内，再做区间扫描，否则直接跳过。</p>
<p><strong>TTL</strong></p>
<p>数据过期时间，用于管理该数据的生命周期，到期删除。</p>
<p>有字段级别的TTL和表级别的TTL。字段级别只是删除这一个字段，表级别是删除整行数据。</p>
<h4 id="（2）ReplacingMergeTree"><a href="#（2）ReplacingMergeTree" class="headerlink" title="（2）ReplacingMergeTree"></a>（2）ReplacingMergeTree</h4><pre><code class="sql">CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
(
    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
    ...
) ENGINE = ReplacingMergeTree([ver])
[PARTITION BY expr]
[ORDER BY expr]
[PRIMARY KEY expr]
[SAMPLE BY expr]
[SETTINGS name=value, ...]
</code></pre>
<p>完全继承了MergeTree，只是多了一个<strong>去重功能，根据order by字段进行去重</strong>。</p>
<p>去重要删除哪些数据：<font color="Red">建表语句中ReplacingMergeTree([ver])的ver字段值最大的那一条数据被保留，其他数据被删除。没指定这个字段，则按插入顺序来，保留最后的一条数据，类似于<code>upsert</code>。</font></p>
<p>去重时机：并不是实时去重，而是在合并分区时进行去重，即保证<strong>最终一致性</strong>。</p>
<p>去重范围：在同一分区内去重，并不是全表内去重。</p>
<p><strong>这个引擎在实际生产环境中最常用</strong>，通常ver字段都是业务中的插入时间字段，或者其他数据版本控制字段。</p>
<p>示例：</p>
<pre><code class="sql">CREATE TABLE default.test3
(
    `id` UInt32,
    `name` String,
    `money` Decimal(18, 6)
)
ENGINE = ReplacingMergeTree
ORDER BY (id, name)
SETTINGS index_granularity = 8192;

insert into test3 values (1, &#39;小明&#39;, 1000);
insert into test3 values (1, &#39;小明&#39;, 2000);
optimize table test3;
select * from test3;
</code></pre>
<h4 id="（3）SummingMergeTree"><a href="#（3）SummingMergeTree" class="headerlink" title="（3）SummingMergeTree"></a>（3）SummingMergeTree</h4><pre><code class="sql">CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
(
    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
    ...
) ENGINE = SummingMergeTree([columns])
[PARTITION BY expr]
[ORDER BY expr]
[SAMPLE BY expr]
[SETTINGS name=value, ...]
</code></pre>
<p>在MergeTree的基础上，提供了“<strong>预聚合</strong>”的功能，且<strong>根据order by字段进行sum聚合</strong>。</p>
<p>聚合后其他字段怎么办：对columns字段进行sum，其他字段保留最早的那一条，即最早插入的那一条。</p>
<p>聚合范围：在同一分区内进行聚合。</p>
<p>聚合时机：并不是实时聚合，而是在合并分区时进行聚合。</p>
<h3 id="5、ReplacatedMergeTree系列"><a href="#5、ReplacatedMergeTree系列" class="headerlink" title="5、ReplacatedMergeTree系列"></a>5、ReplacatedMergeTree系列</h3><p>该系列表引擎用于副本备份，MergeTree系列的所有子引擎都有对应的ReplacetedMergeTree引擎，如ReplacetedReplacingMergeTree等。</p>
<h2 id="五、SQL语法"><a href="#五、SQL语法" class="headerlink" title="五、SQL语法"></a>五、SQL语法</h2><p>只介绍和标准sql不一样的地方</p>
<h3 id="1、Update和Delete"><a href="#1、Update和Delete" class="headerlink" title="1、Update和Delete"></a>1、Update和Delete</h3><p>clickhosue提供update和delete的功能，但没有这两个关键字，即不能直接使用delete或update语句，而是提供了一类称为Mutation的查询，是Alter的一种。</p>
<pre><code class="sql">-- 删除一条数据
alter table test2 delete where id = 10000;

-- 更新一条数据
alter table test2 update age = 99 where id = 1;
</code></pre>
<p>和普通的OLTP数据库不一样，Mutation语句是一种很重的操作，而且不支持事务。<font color="Red">原因在于每次操作都要放弃目标数据的原有分区，重新建分区，旧分区被打上逻辑上的失效标记，只有分区合并的时候，才会删除旧分区旧数据。因此要尽量做批量的变更，避免做频繁的小变更。</font></p>
<p>实现高性能的update和delete思路：</p>
<p>表结构额外新增两个字段：<code>isvalid</code>，0表示无效，1表示有效；<code>version</code>，表示数据版本号，最大为最新数据</p>
<p>更新：插入一条<code>version = max(version) + 1</code>的数据即可</p>
<p>删除：插入一条<code>isvalid = 0</code>的数据即可</p>
<p>查询：每次查询条件都要加上<code>version = max(version) and isvalid = 1</code></p>
<p>问题：数据膨胀，需要定期清理过期数据，并对数据分区做合并</p>
<h3 id="2、Join"><a href="#2、Join" class="headerlink" title="2、Join"></a>2、Join</h3><pre><code class="sql">select test2.id, sum(test3.money) from test2, test3 where test2.id = test3.id group by test2.id;
</code></pre>
<p>clickhouse的join原理：将右表（test3）加载到内存中，再和左表一条一条匹配。</p>
<p><font color="Red">因此这里最好不要将大表写到右边</font>，这一点和MySQL的“小表驱动大表”不一样。</p>
<h3 id="3、函数"><a href="#3、函数" class="headerlink" title="3、函数"></a>3、函数</h3><p>窗口函数，21.7.3.14版本还处于实验中，需要使用的话，要将一个设置打开：</p>
<pre><code class="sql">set allow_experimental_window_functions = 1;
</code></pre>
<p>自定义函数，不支持。</p>
<p>multiif(cond_1, then_1, cond_2, then_2, …)，类似于case when。</p>
<h3 id="4、group"><a href="#4、group" class="headerlink" title="4、group"></a>4、group</h3><p>group增加了with rollup、with cube、with totals用于统计不同维度的值。</p>
<pre><code class="sql">-- rollup是从右至左依次减少一个维度进行统计
group A, B with rollup = 
group by A, B
union
group by A, null
union
group by null, null

-- cube是排列组合的维度统计
group by A, B with cube =
group by A, B
union
group by A, null
union
group by B, null
union
group by null, null

-- total是额外加上统计总计值
group by A, B with totals =
group by A, B
union
group by null, null
</code></pre>
<p>示例：</p>
<pre><code class="sql">select id % 2 as flag, substring(name, 7, 1) as flag2, sum(age) from test2 group by id % 2, substring(name, 7, 1) with rollup;

select id % 2 as flag, substring(name, 7, 1) as flag2, sum(age) from test2 group by id % 2, substring(name, 7, 1) with cube;

select id % 2 as flag, substring(name, 7, 1) as flag2, sum(age) from test2 group by id % 2, substring(name, 7, 1) with totals;
</code></pre>
<h2 id="六、副本写入（备份、高可用）"><a href="#六、副本写入（备份、高可用）" class="headerlink" title="六、副本写入（备份、高可用）"></a>六、副本写入（备份、高可用）</h2><p>clickhouse副本类似于MySQL的replicate，备份用。对应的表引擎为ReplicatedMergeTree。</p>
<p>依赖zookeeper，没有主次之分。</p>
<p><img src="/images/%E5%89%AF%E6%9C%AC%E5%86%99%E5%85%A5.png"></p>
<p>配置的两种方式：</p>
<p>1、修改<code>/etc/clickhouse/config.xml</code>配置文件中的zookeeper配置</p>
<pre><code class="xml">    &lt;zookeeper&gt;
        &lt;node&gt;
            &lt;host&gt;example1&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
        &lt;node&gt;
            &lt;host&gt;example2&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
        &lt;node&gt;
            &lt;host&gt;example3&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
    &lt;/zookeeper&gt;
</code></pre>
<p>2、在<code>/etc/clickhouse-server/config.d/</code>目录下新增一个<code>metrika.xml</code>文件，按照<code>config.xml</code>中zookeeper的配置样式写入自己的配置，再添加一个<code>include_from</code>标签和一个带有<code>incl</code>属性的<code>zookeeper</code>标签，引入这个配置文件。该文件的配置会覆盖<code>config.xml</code>中zookeeper的配置。</p>
<p>以这种方式创建xml文件不要忘了<font color="Red">用户权限</font>，将文件owner设置为clickhouse:clickhouse</p>
<pre><code class="shell">chown clickhouse:clickhouse metrika.xml
</code></pre>
<pre><code class="xml">&lt;!-- metrika.xml --&gt;
&lt;zookeeper&gt;
    &lt;node&gt;
        &lt;host&gt;example1&lt;/host&gt;
        &lt;port&gt;2181&lt;/port&gt;
    &lt;/node&gt;
    &lt;node&gt;
        &lt;host&gt;example2&lt;/host&gt;
        &lt;port&gt;2181&lt;/port&gt;
    &lt;/node&gt;
    &lt;node&gt;
        &lt;host&gt;example3&lt;/host&gt;
        &lt;port&gt;2181&lt;/port&gt;
    &lt;/node&gt;
&lt;/zookeeper&gt;

&lt;!-- config.xml --&gt;
&lt;zookeeper incl=&quot;zookeeper-servers&quot; optional=&quot;true&quot;/&gt;
&lt;include_from&gt;/etc/clickhouse-server/config.d/metrika.xml&lt;/include_from&gt;
</code></pre>
<h2 id="七、分片集群"><a href="#七、分片集群" class="headerlink" title="七、分片集群"></a>七、分片集群</h2><p>副本是指备份，每个服务器都有全量数据（就像复制了一个服务器一样，所以叫replicate）。</p>
<p>分片集群是指分布式，将一份数据切片，分布到不同服务器上，每次查询需要将任务分发到各个服务器中，最后汇总结果（类似于map-reduce）。</p>
<p>分片集群对应的表引擎为Distributed。</p>
<h3 id="1、集群写入"><a href="#1、集群写入" class="headerlink" title="1、集群写入"></a>1、集群写入</h3><p>以3分片，2副本为例。</p>
<p><img src="/images/%E9%9B%86%E7%BE%A4%E5%86%99%E5%85%A5.png"></p>
<p>distribute hdp1类似于一个master（以下为了方便，简称为master），并不实际存储数据，而是起一个控制器和数据分发的作用。下面的hdp1-hdp6类似于worker，存储数据。客户端发送写入命令，master将接收到的数据发送给下面的worker写入数据。</p>
<p>问题：每个副本都要由master亲自分发数据吗？</p>
<p>internal_replication参数，</p>
<ul>
<li>true，表示master只将数据分发给分片，每个副本的数据写入有对应的分片进行内部复制</li>
<li>false，表示每个副本的数据写入也由master亲自分发，而不是由分片内部复制</li>
</ul>
<p>一般来说都会把这个参数设置为true，原因：</p>
<ul>
<li>副本服务器也要master来分发的话，会对master造成压力，写入效率会降低</li>
<li>对副本的分发过程中如果出现异常，则可能造成分片和对应的副本数据不一致</li>
<li>每一层只负责自己的事，master是为分布式而产生了，并不是为副本而产生的，因此master不太应该管副本的备份工作。</li>
</ul>
<h3 id="2、集群读取"><a href="#2、集群读取" class="headerlink" title="2、集群读取"></a>2、集群读取</h3><p><img src="/images/%E9%9B%86%E7%BE%A4%E8%AF%BB%E5%8F%96.png"></p>
<p>errors_count，即读取时发生错误的次数，每次发生错误会记录在对应的服务器中，读取时会选择错误次数更小的副本读取。</p>
<h3 id="3、配置"><a href="#3、配置" class="headerlink" title="3、配置"></a>3、配置</h3><p>同样可以将这个配置写在默认配置文件config.xml中，也可以在外部文件中加配置再在config.xml中引入。</p>
<p>在<code>/etc/clickhouse-server/config.xml</code>中加上</p>
<pre><code class="xml">&lt;include_from&gt;/etc/clickhouse-server/config.d/metrika-shard.xml&lt;/include_from&gt;
</code></pre>
<p><code>/etc/clickhouse-server/config.d/metrika-shard.xml</code>配置如下：</p>
<p>注意同样别忘了文件的用户权限。</p>
<pre><code class="xml">&lt;remote_servers&gt;
    &lt;perftest_1shards_3replicas&gt;&lt;!-- 集群名称，可以修改 --&gt;
        &lt;shard&gt;
            &lt;internal_replication&gt;true&lt;/internal_replication&gt;
            &lt;replica&gt;
                &lt;host&gt;example-perftest01j.yandex.ru&lt;/host&gt;
                &lt;port&gt;9000&lt;/port&gt;
             &lt;/replica&gt;
             &lt;replica&gt;
                &lt;host&gt;example-perftest02j.yandex.ru&lt;/host&gt;
                &lt;port&gt;9000&lt;/port&gt;
             &lt;/replica&gt;
        &lt;/shard&gt;
        &lt;shard&gt;
            &lt;internal_replication&gt;true&lt;/internal_replication&gt;
            &lt;replica&gt;
                &lt;host&gt;example-perftest03j.yandex.ru&lt;/host&gt;
                &lt;port&gt;9000&lt;/port&gt;
             &lt;/replica&gt;
             &lt;replica&gt;
                &lt;host&gt;example-perftest04j.yandex.ru&lt;/host&gt;
                &lt;port&gt;9000&lt;/port&gt;
             &lt;/replica&gt;
        &lt;/shard&gt;
        &lt;shard&gt;
            &lt;internal_replication&gt;true&lt;/internal_replication&gt;
            &lt;replica&gt;
                &lt;host&gt;example-perftest05j.yandex.ru&lt;/host&gt;
                &lt;port&gt;9000&lt;/port&gt;
             &lt;/replica&gt;
             &lt;replica&gt;
                &lt;host&gt;example-perftest06j.yandex.ru&lt;/host&gt;
                &lt;port&gt;9000&lt;/port&gt;
             &lt;/replica&gt;
        &lt;/shard&gt;
    &lt;/perftest_1shards_3replicas&gt;
&lt;/remote_servers&gt;

&lt;zookeeper&gt;&lt;!-- 有几台服务器就需要几台zookeeper，按照以上需求，要写六台，这里省略 --&gt;
    &lt;node&gt;
        &lt;host&gt;zoo01.yandex.ru&lt;/host&gt;
        &lt;port&gt;2181&lt;/port&gt;
    &lt;/node&gt;
    &lt;node&gt;
        &lt;host&gt;zoo02.yandex.ru&lt;/host&gt;
        &lt;port&gt;2181&lt;/port&gt;
    &lt;/node&gt;
    &lt;node&gt;
        &lt;host&gt;zoo03.yandex.ru&lt;/host&gt;
        &lt;port&gt;2181&lt;/port&gt;
    &lt;/node&gt;
&lt;/zookeeper&gt;

&lt;macros&gt;
    &lt;shard&gt;01&lt;/shard&gt;
    &lt;replica&gt;rep_1_1&lt;/replica&gt;
&lt;/macros&gt;
</code></pre>
<p><font color="Red">macros用于建表时识别分片和副本，shard为分片名称，replica为副本名称，这个配置每台服务器上都要单独配置。</font></p>
<p>可以自定义配置，这里配置的含义是：<code>rep_&#123;a&#125;_&#123;b&#125;</code>，a表示分片编号，b表示副本编号。</p>
<pre><code class="xml">&lt;macros&gt;&lt;!-- 第1个分片，第1个副本 --&gt;
    &lt;shard&gt;01&lt;/shard&gt;
    &lt;replica&gt;rep_1_1&lt;/replica&gt;
&lt;/macros&gt;
&lt;macros&gt;&lt;!-- 第1个分片，第2个副本 --&gt;
    &lt;shard&gt;01&lt;/shard&gt;
    &lt;replica&gt;rep_1_2&lt;/replica&gt;
&lt;/macros&gt;
&lt;macros&gt;&lt;!-- 第2个分片，第1个副本 --&gt;
    &lt;shard&gt;02&lt;/shard&gt;
    &lt;replica&gt;rep_2_1&lt;/replica&gt;
&lt;/macros&gt;
&lt;macros&gt;&lt;!-- 第2个分片，第2个副本 --&gt;
    &lt;shard&gt;02&lt;/shard&gt;
    &lt;replica&gt;rep_2_2&lt;/replica&gt;
&lt;/macros&gt;
&lt;macros&gt;&lt;!-- 第3个分片，第1个副本 --&gt;
    &lt;shard&gt;03&lt;/shard&gt;
    &lt;replica&gt;rep_3_1&lt;/replica&gt;
&lt;/macros&gt;
&lt;macros&gt;&lt;!-- 第3个分片，第2个副本 --&gt;
    &lt;shard&gt;03&lt;/shard&gt;
    &lt;replica&gt;rep_3_2&lt;/replica&gt;
&lt;/macros&gt;
</code></pre>
<p><font color="Red">默认配置文件config.xml中官方也给我们配好了几个示例集群的配置，可以参考上面的配置以及配置说明。</font></p>
<pre><code class="sql">myubuntu1 :) show clusters;

SHOW CLUSTERS

Query id: 1d87a0a9-9e49-498d-b097-c22a23530d2e

┌─cluster──────────────────────────────────────┐
│ test_cluster_two_shards                      │
│ test_cluster_two_shards_internal_replication │
│ test_cluster_two_shards_localhost            │
│ test_shard_localhost                         │
│ test_shard_localhost_secure                  │
│ test_unavailable_shard                       │
└──────────────────────────────────────────────┘
</code></pre>
<h3 id="4、使用"><a href="#4、使用" class="headerlink" title="4、使用"></a>4、使用</h3><h4 id="（1）建表"><a href="#（1）建表" class="headerlink" title="（1）建表"></a>（1）建表</h4><p><strong>先建本地表：</strong></p>
<p>即创建之前说的worker，存储数据用的本地表。</p>
<pre><code class="sql">create table test_cluster_table on cluster test_shard_localhost(
    id UInt8,
    name String,
    create_time Datetime default now()
)
engine = ReplicatedMergeTree(&#39;clickhouse/tables/&#123;shard&#125;/test_cluster_table&#39;, &#39;&#123;replica&#125;&#39;)
order by (id, name);

Query id: 86b73340-2c9d-42a1-97e2-f99d8d388f0e

┌─host────────────┬─port─┬─status─┬─error─┬─num_hosts_remaining─┬─num_hosts_active─┐
│ 192.168.141.141 │ 9003 │      0 │       │                   0 │                0 │
└─────────────────┴──────┴────────┴───────┴─────────────────────┴──────────────────┘

1 rows in set. Elapsed: 0.111 sec.
</code></pre>
<p>test_shard_localhost为集群名称</p>
<p>{shard}和{replica}都是配置文件中macros导入，不用自己填。</p>
<p><strong>再创建分布式表：</strong></p>
<p>即创建之前说的master</p>
<pre><code class="sql">create table test_cluster_table_all on cluster test_shard_localhost(
    id UInt8,
    name String,
    create_time Datetime default now()
)
engine=Distributed(test_shard_localhost, default, test_cluster_table, hiveHash(id));

Query id: 69a6f496-e916-47ac-ae0b-636c9244bc90

┌─host────────────┬─port─┬─status─┬─error─┬─num_hosts_remaining─┬─num_hosts_active─┐
│ 192.168.141.141 │ 9003 │      0 │       │                   0 │                0 │
└─────────────────┴──────┴────────┴───────┴─────────────────────┴──────────────────┘

1 rows in set. Elapsed: 0.114 sec.
</code></pre>
<p>test_shard_localhost为集群名称</p>
<p>default为数据库名</p>
<p>test_cluster_table为刚才创建的本地表名</p>
<p>hiveHash(id)，hiveHash表示采用什么算法分片，id表示用哪个字段分片。</p>
<h4 id="（2）使用"><a href="#（2）使用" class="headerlink" title="（2）使用"></a>（2）使用</h4><p>分布式表不存储数据，为每个本地表的逻辑汇总表。</p>
<pre><code class="sql">-- 从分布式表插入数据，本地表可以看到
insert into test_cluster_table_all values(1, &#39;小明&#39;, now());
select * from test_cluster_table;

-- 从本地表插入，从分布式表也可以看得到
insert into test_cluster_table values(2, &#39;小红&#39;, now());
select * from test_cluster_table_all;
</code></pre>
<p>本地表只能看到本服务器上存储的数据分片，无法看到其他服务器上存储的数据分片，而分布式表可以看到所有数据。</p>
<p>因此一般不用将本地表暴露给用户写入，统一从分布式表做增删改查。</p>
<h2 id="八、进阶语法"><a href="#八、进阶语法" class="headerlink" title="八、进阶语法"></a>八、进阶语法</h2><h3 id="1、explain"><a href="#1、explain" class="headerlink" title="1、explain"></a>1、explain</h3><p>20.6.3.28及以后版本才有explain功能。</p>
<p>官方文档：<a target="_blank" rel="noopener" href="https://clickhouse.com/docs/en/sql-reference/statements/explain/">EXPLAIN | ClickHouse Documentation</a></p>
<pre><code class="sql">EXPLAIN [AST | SYNTAX | PLAN | PIPELINE] [setting = value, ...] SELECT ... [FORMAT ...]
</code></pre>
<p>EXPLAIN SYNTAX …可以看到系统给你优化后的语法。</p>
<p>比如上面那个join的sql，通过系统优化后的sql语句如下。</p>
<pre><code class="sql">explain syntax select test2.id, sum(test3.money) from test3, test2 where test2.id = test3.id group by test2.id;

┌─explain─────────────────────┐
│ SELECT                      │
│     id,                     │
│     sum(test3.money)        │
│ FROM test2                  │
│ ALL INNER JOIN              │
│ (                           │
│     SELECT                  │
│         id,                 │
│         money               │
│     FROM test3              │
│ ) AS test3 ON id = test3.id │
│ WHERE id = test3.id         │
│ GROUP BY id                 │
└─────────────────────────────┘
</code></pre>
<p>但这里优化后的结果不对，是因为id没有加上test2表名，并且where条件也是多余的，手动修改为如下sql，对比原始sql执行速度，优化后的sql性能确实提升了一些。</p>
<p><font color="Red">千万不要盲目相信explain syntax后的结果，可能会让你得不偿失。</font></p>
<pre><code class="sql">SELECT
    test2.id,
    sum(test3.money)
FROM test2
ALL INNER JOIN
(
    SELECT
        id,
        money
    FROM test3
) AS test3 ON test2.id = test3.id
GROUP BY test2.id;
</code></pre>
<p>老版本没有explain语法，要查看执行计划，可以在进入客户端时，加上<code>--send_logs_level=trade &lt;&lt;&lt; &quot;sql&quot;</code>，然后去看日志即可。</p>
<h3 id="2、optimize"><a href="#2、optimize" class="headerlink" title="2、optimize"></a>2、optimize</h3><pre><code class="sql">OPTIMIZE TABLE [db.]name [ON CLUSTER cluster] [PARTITION partition | PARTITION ID &#39;partition_id&#39;] [FINAL] [DEDUPLICATE [BY expression]]
</code></pre>
<p>optimize功能是触发一个表数据的合并。只能用于MergeTree系列表引擎、Buffer表引擎和MaterializedView表引擎。</p>
<p>触发数据合并的意思是，例如在一个ReplacingMergeTree表执行，则会执行一个任务，按order by指定的字段去重；又例如在SummingMergeTree表执行，则会执行一个任务，按照order by指定字段进行聚合，对SummingMergeTree指定字段进行sum计算，其他字段保留最新数据。<font color="Red">即将数据初始化，或重新整理分区数据。</font></p>
<p>如果指定了<code>FINAL</code>，则会强制执行数据合并，即使所有数据都在一个分区，或者已经有并发的合并正在进行。</p>
<h2 id="九、常用配置"><a href="#九、常用配置" class="headerlink" title="九、常用配置"></a>九、常用配置</h2><h3 id="1、CPU"><a href="#1、CPU" class="headerlink" title="1、CPU"></a>1、CPU</h3><table>
<thead>
<tr>
<th>位置</th>
<th>配置</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>user.xml</td>
<td>background_pool_size</td>
<td>表引擎相关的后台线程池大小，常用于merge任务。默认值为16，允许的情况下可以调整称服务器的逻辑线程数据。</td>
</tr>
<tr>
<td>user.xml</td>
<td>background_schedule_pool_size</td>
<td>后台任务线程池大小，常用于副本数据备份、kafka流、DNS缓存更新。默认值128。</td>
</tr>
<tr>
<td>user.xml</td>
<td>background_distributed_schedule_pool_size</td>
<td>后台任务线程池大小，常用于分布式数据发送。默认值为16。</td>
</tr>
<tr>
<td>config.xml</td>
<td>max_concurrent_queries</td>
<td>最大并发请求数，默认设置为100，建议不超过300。</td>
</tr>
<tr>
<td>user.xml</td>
<td>max_threads</td>
<td>单个查询使用的最大线程数，默认值为cpu物理核心数。这个值越低，查询时占用的内存越低。</td>
</tr>
</tbody></table>
<h3 id="2、内存"><a href="#2、内存" class="headerlink" title="2、内存"></a>2、内存</h3><table>
<thead>
<tr>
<th>位置</th>
<th>配置</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>user.xml</td>
<td>max_memory_usage</td>
<td>单次查询使用的最大内存。默认值为10G左右。</td>
</tr>
<tr>
<td>user.xml</td>
<td>max_bytes_before_external_groupby</td>
<td>当使用groupby时内存占用超过最大内存时，将数据写入磁盘做缓存进行groupby时大小。一般可以设置为max_memory_usage的一半。</td>
</tr>
<tr>
<td>user.xml</td>
<td>max_bytes_before_external_sort</td>
<td>当使用sort时内存占用超过最大内存时，将数据写入磁盘作为临时缓存进行sort时的大小。一般可以设置为max_memory_usage的一半。</td>
</tr>
<tr>
<td>config.xml</td>
<td>max_table_size_to_drop</td>
<td>drop table时，允许删除的表的最大大小，表大小超过这个值时删除表不被允许。设置为0时，认为没有限制。</td>
</tr>
</tbody></table>
<h2 id="十、查询优化"><a href="#十、查询优化" class="headerlink" title="十、查询优化"></a>十、查询优化</h2><h3 id="1、单表优化"><a href="#1、单表优化" class="headerlink" title="1、单表优化"></a>1、单表优化</h3><h4 id="（1）prewhere"><a href="#（1）prewhere" class="headerlink" title="（1）prewhere"></a>（1）prewhere</h4><p>只能用于MergeTree系列表引擎。</p>
<p>prewhere执行时，会直接扫描过滤条件中的列，将数据过滤后，再将需要的字段补全。</p>
<p>当查询列明显多于过滤列时，可以使用prewhere，降低IO。</p>
<p><code>optimize_move_to_prewhere</code>参数是在有需要的时候自动将where优化成prewhere，默认是打开的。即这个参数打开时，不需要手动写prewhere，系统会在有需要的时候自动优化成prewhere去查询。</p>
<p>部分情况下，不会自动优化成prewhere：</p>
<ul>
<li>使用常用表达式</li>
<li>使用默认值的alias类型的字段</li>
<li>包含了arrayJOIN，globalIn，globalNotIn，indexHint的查询</li>
<li>select字段和where字段相同</li>
<li>使用了主键字段</li>
</ul>
<p><font color="Red">有需要时，还是自己手动使用prewhere好，别太依赖系统的优化。</font></p>
<h4 id="（2）sample"><a href="#（2）sample" class="headerlink" title="（2）sample"></a>（2）sample</h4><p>只能用于MergeTree系列表引擎。</p>
<p>采样。不会对所有数据执行查询，而是对特定部分数据（样本）进行查询。</p>
<p>且这个样本并不是严格精确的数据量。</p>
<pre><code class="sql">-- 约10%样本查询
select *
from test
sample 0.1;

-- 当比例值远大于1时，这个含义便会转换为数据条数据，如这里采样至少10000条
select *
from test
sample 10000;

-- 加上offset表示跳过前面部分数据
select *
from test
sample 0.1 offset 0.5
</code></pre>
<p>和limit的区别在于，limit是严格精确的数据量，而sample并不是严格的数据量。同时sample可以用在where、group和orderby前面。</p>
<pre><code class="sql">SELECT
    Title,
    count() * 10 AS PageViews
FROM hits_distributed
SAMPLE 0.1
WHERE
    CounterID = 34
GROUP BY Title
ORDER BY PageViews DESC LIMIT 1000;
</code></pre>
<p>在一些特殊情况下，可以使用sample：</p>
<ul>
<li>业务需要查询延迟很低，但无法通过优化来降低延迟，且对具体结果的精确性不太讲究，可以使用近似结果</li>
<li>只想大致查看数据的分布以及数据质量</li>
</ul>
<h4 id="（3）orderby"><a href="#（3）orderby" class="headerlink" title="（3）orderby"></a>（3）orderby</h4><p>order by不要单独使用，结合where和limit一起使用。</p>
<p>一般排序后很少需要完整排序的结果，因此可以加一个limit。加limit和不加limit的效率还是有些差距的。</p>
<h4 id="（4）虚拟列"><a href="#（4）虚拟列" class="headerlink" title="（4）虚拟列"></a>（4）虚拟列</h4><p>clickhouse中<font color="Red">尽量不要使用虚拟列</font>，很消耗性能。可以在服务端或者其他地方处理数据，不要在clickhouse中使用虚拟列来处理数据。</p>
<pre><code class="sql">select a/b from test;
select a, b from test;
</code></pre>
<h3 id="2、多表关联"><a href="#2、多表关联" class="headerlink" title="2、多表关联"></a>2、多表关联</h3><p>数据集：</p>
<pre><code class="shell">curl -O https://datasets.clickhouse.com/hits/partitions/hits_v1.tar
tar xvf hits_v1.tar -C /var/lib/clickhouse # path to ClickHouse data directory
# check permissions on unpacked data, fix if required

curl -O https://datasets.clickhouse.com/visits/partitions/visits_v1.tar
tar xvf visits_v1.tar -C /var/lib/clickhouse # path to ClickHouse data directory
# check permissions on unpacked data, fix if required

sudo service clickhouse-server restart
</code></pre>
<p>clickhouse建表时没有like语法，因此使用以下语法创建一个张和hits_v1一样表结构的表。</p>
<pre><code class="sql">create table hits_test
ENGINE = MergeTree
PARTITION BY toYYYYMM(EventDate)
ORDER BY (CounterID, EventDate, intHash32(UserID))
SAMPLE BY intHash32(UserID)
SETTINGS index_granularity = 8192
as select * from datasets.hits_v1 where 1 = 0;
</code></pre>
<h4 id="（1）用in代替join"><a href="#（1）用in代替join" class="headerlink" title="（1）用in代替join"></a>（1）用in代替join</h4><p>但是in的使用场景很有限，而且使用时要特别注意查询结果，因为join是笛卡尔积，右表有重复数据的话，结果集可能不止一条，而使用in的话，右表有重复数据对于左表来说是没关系的。</p>
<h4 id="（2）大表在左，小表在右"><a href="#（2）大表在左，小表在右" class="headerlink" title="（2）大表在左，小表在右"></a>（2）大表在左，小表在右</h4><p>和MySQL或其他行式数据库不一样，clickhouse在使用join时，需要将大表作为主表，小表作为被关联的表。</p>
<pre><code class="sql">-- 内存不足，直接报错
insert into hits_test
select a.* from datasets.visits_v1 b join datasets.hits_v1 a on a.CounterID = b.CounterID where a.CounterID &gt; 100000;

-- 3.751秒，可以执行
insert into hits_test
select a.* from datasets.hits_v1 a join datasets.visits_v1 b on a.CounterID = b.CounterID where a.CounterID &gt; 100000;
</code></pre>
<h4 id="（3）先过滤再关联"><a href="#（3）先过滤再关联" class="headerlink" title="（3）先过滤再关联"></a>（3）先过滤再关联</h4><p>对于查询大数据量的sql，先过滤再关联，可以减少扫描的数据量，提升一点效率。即先对某一个表写where条件，形成一个子查询，再用子查询来关联，效率会比直接关联然后统一写where条件来的高。</p>
<p>使用先过滤再关联还有一个好处在于，如果过滤条件是右表，则可以减少将数据加载到内存的量。</p>
<pre><code class="sql">-- 1.999秒，效率低
insert into hits_test
select a.* from datasets.hits_v1 a join datasets.visits_v1 b on a.CounterID = b.CounterID where a.EventDate = &#39;2014-03-17&#39;;

-- 1.772秒，效率高
insert into hits_test
select a.* from (select * from datasets.hits_v1 where EventDate = &#39;2014-03-17&#39;) a join datasets.visits_v1 b on a.CounterID = b.CounterID;

-- 2.509秒，占用内存峰值1.91G
insert into hits_test
select a.* from datasets.hits_v1 a join datasets.visits_v1 b on a.CounterID = b.CounterID where b.StartDate = &#39;2014-03-17&#39;;

-- 2.478秒，占用内存峰值1.7G
insert into hits_test
select a.* from datasets.hits_v1 a join (select CounterID from datasets.visits_v1 where StartDate = &#39;2014-03-17&#39;) b on a.CounterID = b.CounterID;
</code></pre>
<h4 id="（4）分布式表用GLOBAL"><a href="#（4）分布式表用GLOBAL" class="headerlink" title="（4）分布式表用GLOBAL"></a>（4）分布式表用GLOBAL</h4><p>以3分片为例，hits_v1表为分布式表，visits_v1为普通表，以下查询为例。</p>
<pre><code class="sql">select * from (select * from hits_v1 perwhere CounterID &gt; 1000) a join visits_v1 b on on a.CounterID = b.CounterID;
</code></pre>
<p>分布式表在join时，每台服务器都会将右表加载到各自服务器的内存中，然后进行匹配。不使用GLOBAL时，需要查3次右表；使用GLOBAL，可以只进行1次查询，并分发到其他节点。如果此时右表也是分布式表的话，不适用GLOBAL就会查9次；使用GLOBAL只需要查3次。这就是<code>查询放大</code>。</p>
<h4 id="（5）使用字典表"><a href="#（5）使用字典表" class="headerlink" title="（5）使用字典表"></a>（5）使用字典表</h4><p>Dictionary。</p>
<p><a target="_blank" rel="noopener" href="https://clickhouse.com/docs/en/sql-reference/dictionaries/internal-dicts/">https://clickhouse.com/docs/en/sql-reference/dictionaries/internal-dicts/</a></p>
<h3 id="3、其他优化"><a href="#3、其他优化" class="headerlink" title="3、其他优化"></a>3、其他优化</h3><h4 id="（1）写入数据时先排序。"><a href="#（1）写入数据时先排序。" class="headerlink" title="（1）写入数据时先排序。"></a>（1）写入数据时先排序。</h4><p>因为写入数据时，数据不会直接分配到实际所在的那个分区，而是会先临时放在一个新的分区。无序的数据会产生大量的新分区，merge时会产生性能问题。</p>
<h4 id="（2）关注CPU"><a href="#（2）关注CPU" class="headerlink" title="（2）关注CPU"></a>（2）关注CPU</h4><p>cpu负载在50%时，会对查询性能会产生影响；cpu负载超过70%时，会出现大范围超时情况。因此不要以为在单次查询时性能很好就以为在实际生产环境中没有问题，放到生产环境中的查询sql要尽量优化好。</p>
<h2 id="十一、一致性"><a href="#十一、一致性" class="headerlink" title="十一、一致性"></a>十一、一致性</h2><p>clickhouse只能保证<font color="Red">最终一致性</font>！如ReplacingMergeTree的删除重复数据的功能，只会在进行数据合并时进行，即在不确定的时候后台进行。因此平时使用时，<font color="Red">不能保证没有重复数据</font>。问题：如何保证查询时数据的一致性。</p>
<pre><code class="sql">-- 创建表
create table test_replacing_mt(
    user_id UInt64,
    score String,
    deleted UInt8 default 0,
    create_time DateTime default toDateTime(0)
)engine=ReplacingMergeTree(create_time)
order by user_id;

-- 写入数据
insert into test_replacing_mt(user_id, score)
with (
    select [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;, &#39;G&#39;]
) as dict
select number as user_id, dict[rand()%7+1] as score
from numbers(10000000);

-- 修改数据（其实还是写入数据）
insert into test_replacing_mt(user_id, score, create_time)
with (
    select [&#39;AA&#39;, &#39;BB&#39;, &#39;CC&#39;, &#39;DD&#39;, &#39;EE&#39;, &#39;FF&#39;, &#39;GG&#39;]
) as dict
select number as user_id, dict[rand()%7+1] as score, now() as create_time
from numbers(500000);

-- 查询数据量，为10500000条，并没有立即去重
select count(1) from test_replacing_mt;
</code></pre>
<h3 id="1、手动optimize（错误）"><a href="#1、手动optimize（错误）" class="headerlink" title="1、手动optimize（错误）"></a>1、手动optimize（错误）</h3><p>每次执行完insert都手动进行optimize。</p>
<p>绝对不可取！！手动optimize只能空闲时间进行。</p>
<h3 id="2、手动通过sql实现"><a href="#2、手动通过sql实现" class="headerlink" title="2、手动通过sql实现"></a>2、手动通过sql实现</h3><p>通过设置一些特殊字段，如<code>deleted</code>控制该条数据是否被删除掉了，<code>create_time</code>控制该条数据的创建时间。</p>
<p>然后查询<code>create_time</code>为最新的，且<code>deleted</code>为0的数据，即可实现手动去重。</p>
<pre><code class="sql">select
    user_id,
    argMax(score, create_time) as score,
    argMax(deleted, create_time) as deleted,
    max(create_time) as ctime
from test_replacing_mt
group by user_id
having deleted = 0
</code></pre>
<p>以上面sql做一个普通视图，并在这个视图上查询数据，则可以查询到最新的有效数据。</p>
<pre><code class="sql">create view test_replacing_mt_view as
select
    user_id,
    argMax(score, create_time) as score,
    argMax(deleted, create_time) as deleted,
    max(create_time) as ctime
from test_replacing_mt
group by user_id
having deleted = 0;

-- 在视图上进行查询
select count(1) from test_replacing_mt_view;
select * from test_replacing_mt_view where user_id == 100;
</code></pre>
<p>删除数据则通过插入一条<code>deleted = 0 and create_time = now()</code>的数据</p>
<pre><code class="sql">insert into test_replacing_mt values(100, &#39;AA&#39;, 1, now());

-- 再次查询test_replacing_mt_view，发现没有数据返回
select * from test_replacing_mt_view where user_id == 100;

-- 9999999条数据
select count(1) from test_replacing_mt_view;
</code></pre>
<p>注：</p>
<ul>
<li>argMax(field1, field2)：按照field2的最大值取field1的值。</li>
</ul>
<h3 id="3、通过FINAL查询"><a href="#3、通过FINAL查询" class="headerlink" title="3、通过FINAL查询"></a>3、通过FINAL查询</h3><p>在sql语句中写final只支持ReplacingMergeTree和SummingMergeTree，因为这两个表引擎在合并数据时需要聚合，普通的MergeTree表引擎合merge时不需要聚合。</p>
<p>final的作用是根据order by指定的字段，查询版本号最新的一条数据。</p>
<p>20.5.2.7-stable版本之前是单线程进行，速度很慢，老版本不建议使用。后面新版本支持多线程，并且可以通过<code>max_final_threads</code>参数控制单个final查询的线程数。</p>
<pre><code class="sql">-- 当然deleted还是要加的
select * from test_replacing_mt final where user_id = 100 and deleted = 0;

-- 如果不加deleted字段，可以看到最新版本的数据是那条deleted为1的数据
select * from test_replacing_mt final where user_id = 100;

┌─user_id─┬─score─┬─deleted─┬─────────create_time─┐
│     100 │ AA    │       1 │ 2021-12-11 13:27:54 │
└─────────┴───────┴─────────┴─────────────────────┘
</code></pre>
<h3 id="4、结论"><a href="#4、结论" class="headerlink" title="4、结论"></a>4、结论</h3><p>由于clickhouse对update和delete的操作不友善，因此在实际生产环境中最好加上这么几个字段：</p>
<p><code>create_time</code>：数据创建时间</p>
<p><code>deleted</code>：该数据是否被删除了，1代表是，0代表没有删除</p>
<p>或者使用</p>
<p><code>isvalid</code>：该条数据是否有效，1代表有效数据，0代表无效数据，即和<code>deleted</code>作用相反。</p>
<p>如果是新版本，则可以在使用final查询和自定义视图的方式之间权衡，对比两种方式的效率以及对数据库造成的压力，选择最适合的方式。注意并不一定是所有表都要用一种方式，可能有些数据使用自定义视图合适，而有些数据使用final合适。</p>
<p>如果是旧版本，final无法设置多线程，导致效率很低，则还是选择自定义视图的方式来实现吧。</p>
<p>其次，每天在空闲时间定时进行数据合并。</p>
<p>最后，如果以上方式效率都很低，但是某个业务对于查询出来的数据的准确定并不讲究，则可以不进行去重。</p>
<h2 id="十二、物化视图（MaterializedView）"><a href="#十二、物化视图（MaterializedView）" class="headerlink" title="十二、物化视图（MaterializedView）"></a>十二、物化视图（MaterializedView）</h2><p>普通视图只保存查询逻辑，并不保存数据。而物化视图会保存数据，即<font color="Red">真正创建一张隐藏表来存储数据</font>，当原始表数据插入数据时，新数据会按照物化视图的逻辑生成最新的数据。</p>
<p><strong>优点</strong>：快！当需要查询一些聚合的操作时，使用原始数据进行聚合查询可能会比较慢，但是可以通过物化视图将聚合数据加载到一张表中，然后直接查物化视图中的数据，速度就可想而知很快了。</p>
<pre><code class="sql">CREATE MATERIALIZED VIEW [IF NOT EXISTS] [db.]table_name [ON CLUSTER] [TO[db.]name] [ENGINE = engine] [POPULATE] AS SELECT ...
</code></pre>
<ul>
<li><p>TO：隐藏表名，如果不加的话，默认为<code>.inner_id.xxxxxxxx</code></p>
</li>
<li><p>POPULATE：创建后会对数据进行初始化，即执行视图逻辑，并将所有数据写入隐藏表。生产环境不建议加，如果需要历史数据，可以手动insert。</p>
</li>
</ul>
<h3 id="1、使用"><a href="#1、使用" class="headerlink" title="1、使用"></a>1、使用</h3><pre><code class="sql">-- 创建原始数据表
create table test_score(
    user_id UInt64,
    subject String,
    score String,
    isvalid UInt8 default 1,
    create_time DateTime default toDateTime(0)
)engine=ReplacingMergeTree(create_time)
order by (user_id, subject);

-- 写入初始化数据
insert into test_score(user_id, subject, score, create_time)
with
(
    select range(1000)
) as user_dict,
(
    select [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;, &#39;G&#39;]
) as score_dict,
(
    select [&#39;语文&#39;, &#39;数学&#39;, &#39;英语&#39;]
) as subject_dict,
(
    select [&#39;2021-12-11 15:00:00&#39;, &#39;2021-12-10 15:00:00&#39;, &#39;2021-12-09 15:00:00&#39;]
) as ctime_dict
select
    user_dict[rand()%1000+1] as user_id,
    subject_dict[rand()%3+1] as subject,
    score_dict[rand()%7+1] as score,
    ctime_dict[rand()%3+1] as create_time
from numbers(100000)
order by user_id, subject;

-- 创建物化视图，业务逻辑：将每个user_id的成绩记录成json格式，写入score_json字段
create materialized view test_score_mv
engine=ReplacingMergeTree(create_time)
order by (user_id)
as select user_id, score_json, isvalid, create_time
from (
    select
        user_id,
        &#39;&#123;&#39; || arrayStringConcat(groupArray(&#39;&quot;&#39; || subject || &#39;&quot;:&quot;&#39; || score || &#39;&quot;&#39;), &#39;,&#39;) || &#39;&#125;&#39; as score_json,
        1 as isvalid,
        max(ctime) as create_time
    from (
        select
            user_id,
            subject,
            argMax(score, create_time) as score,
            argMax(isvalid, create_time) as isvalid,
            max(create_time) as ctime
        from test_score
        group by user_id, subject
        having isvalid = 1
    ) as tmp1
    group by user_id
) as tmp2;
</code></pre>
<p>记录这个创建时间，初始化数据时可以通过筛选小于这个创建时间来初始化。</p>
<p>由于内存限制，初始化时，可以一批一批来</p>
<pre><code class="shell">clickhouse-client -m -h 192.168.141.141 --port 9003 --query &quot;select user_id from test_score group by user_id order by user_id format CSV;&quot; &gt; test_score.csv

while read user_id; do echo &quot;$&#123;user_id&#125;&quot;; clickhouse-client -m -h 192.168.141.141 --port 9003 --query &quot;\
insert into test_score_mv \
select \
    user_id, \
    &#39;&#123;&#39; || arrayStringConcat(groupArray(&#39;\&quot;&#39; || subject || &#39;\&quot;:\&quot;&#39; || score || &#39;\&quot;&#39;), &#39;,&#39;) || &#39;&#125;&#39; as score_json, \
    1 as isvalid, \
    max(create_time) as create_time \
from test_score \
final \
where user_id = $&#123;user_id&#125; \
group by user_id \
&quot;; done &lt; test_score.csv;
</code></pre>
<p>测试插入新数据到<code>hits_test</code>：</p>
<pre><code class="sql">-- 测试插入前，可以查询历史数据是否成功插入到物化视图中
select * from test_score_mv where user_id = 0 and subject = &#39;语文&#39;;

┌─user_id─┬─score_json────────────────────────────┬─isvalid─┬─────────create_time─┐
│       0 │ &#123;&quot;数学&quot;:&quot;EE&quot;,&quot;语文&quot;:&quot;EEE&quot;,&quot;英语&quot;:&quot;F&quot;&#125; │       1 │ 2021-12-11 17:54:16 │
└─────────┴───────────────────────────────────────┴─────────┴─────────────────────┘

-- 插入一条数据
insert into test_score values (0, &#39;数学&#39;, &#39;EEE&#39;, 1, now());

-- 再次查询物化视图，是有数据的，但是只有最新数据的group。
select * from test_score_mv where user_id = 0 and subject = &#39;语文&#39;;
┌─user_id─┬─score_json─────┬─isvalid─┬─────────create_time─┐
│       0 │ &#123;&quot;数学&quot;:&quot;EEE&quot;&#125; │       1 │ 2021-12-11 18:31:53 │
└─────────┴────────────────┴─────────┴─────────────────────┘
┌─user_id─┬─score_json────────────────────────────┬─isvalid─┬─────────create_time─┐
│       0 │ &#123;&quot;数学&quot;:&quot;EE&quot;,&quot;语文&quot;:&quot;EEE&quot;,&quot;英语&quot;:&quot;F&quot;&#125; │       1 │ 2021-12-11 17:54:16 │
└─────────┴───────────────────────────────────────┴─────────┴─────────────────────┘
</code></pre>
<p>测试更新数据（实际业务场景中不建议用update语句，这里只用作测试）。</p>
<pre><code class="sql">optimize table test_score;
alter table test_score update score = &#39;AAA&#39; where user_id = 0;

-- 发现物化视图数据并没有修改
select * from test_score_mv where user_id = 0;
┌─user_id─┬─score_json─────┬─isvalid─┬─────────create_time─┐
│       0 │ &#123;&quot;数学&quot;:&quot;EEE&quot;&#125; │       1 │ 2021-12-11 18:31:53 │
└─────────┴────────────────┴─────────┴─────────────────────┘
┌─user_id─┬─score_json────────────────────────────┬─isvalid─┬─────────create_time─┐
│       0 │ &#123;&quot;数学&quot;:&quot;EE&quot;,&quot;语文&quot;:&quot;EEE&quot;,&quot;英语&quot;:&quot;F&quot;&#125; │       1 │ 2021-12-11 17:54:16 │
└─────────┴───────────────────────────────────────┴─────────┴─────────────────────┘
</code></pre>
<p>测试删除数据（实际业务场景中不建议用update语句，这里只用作测试）。</p>
<pre><code class="sql">alter table test_score delete where user_id = 0;

-- 发现物化视图数据同样没有修改
select * from test_score_mv where user_id = 0;
┌─user_id─┬─score_json─────┬─isvalid─┬─────────create_time─┐
│       0 │ &#123;&quot;数学&quot;:&quot;EEE&quot;&#125; │       1 │ 2021-12-11 18:31:53 │
└─────────┴────────────────┴─────────┴─────────────────────┘
┌─user_id─┬─score_json────────────────────────────┬─isvalid─┬─────────create_time─┐
│       0 │ &#123;&quot;数学&quot;:&quot;EE&quot;,&quot;语文&quot;:&quot;EEE&quot;,&quot;英语&quot;:&quot;F&quot;&#125; │       1 │ 2021-12-11 17:54:16 │
└─────────┴───────────────────────────────────────┴─────────┴─────────────────────┘
</code></pre>
<p><font color="Red">clickhouse的物化视图更像是触发器，且只对新增的部分数据有效，对历史数据，或update操作或delete操作都是无效的。物化视图功能有限，对于上面这种业务逻辑，是不适用的。</font></p>
<h3 id="2、简单聚合业务"><a href="#2、简单聚合业务" class="headerlink" title="2、简单聚合业务"></a>2、简单聚合业务</h3><p>简单聚合，如sum、max、min、count等业务，可以使用物化视图来实现。</p>
<p>物化视图使用的隐藏表不使用默认的，而是我们自己定义的表，具体如下：</p>
<p>业务介绍：统计每个设备出现的次数和最大值、最小值、平均值。</p>
<pre><code class="sql">-- 基础数据表
CREATE TABLE counter (
  when DateTime DEFAULT now(),
  device UInt32,
  value Float32
) ENGINE=MergeTree
PARTITION BY toYYYYMM(when)
ORDER BY (device, when);

-- 物化视图物理表，以天为单位做预聚合
CREATE TABLE counter_daily (
  day DateTime,
  device UInt32,
  count UInt64,
  max_value_state AggregateFunction(max, Float32),
  min_value_state AggregateFunction(min, Float32),
  avg_value_state AggregateFunction(avg, Float32)
)
ENGINE = SummingMergeTree()
PARTITION BY tuple()
ORDER BY (device, day);

-- 物化视图
CREATE MATERIALIZED VIEW counter_daily_mv
TO counter_daily
AS SELECT
    toStartOfDay(when) as day,
    device,
    count(*) as count,
    maxState(value) AS max_value_state,
    minState(value) AS min_value_state,
    avgState(value) AS avg_value_state
FROM counter
WHERE when &gt;= toDate(&#39;2019-01-01 00:00:00&#39;)
GROUP BY device, day
ORDER BY device, day;

-- 查询
SELECT
  device,
  sum(count) AS count,
  maxMerge(max_value_state) AS max,
  minMerge(min_value_state) AS min,
  avgMerge(avg_value_state) AS avg
FROM counter_daily_mv
GROUP BY device
ORDER BY device ASC;
</code></pre>
<p>物化视图的物理表使用的是SummingMergeTree表引擎，且定义三个状态函数。</p>
<p>物化视图则像一个触发器，将新插入的数据转换为以天为单位的状态写入物理表中。</p>
<p>最后查询时需要指定对应的聚合类型，获取对应的聚合值。</p>
<h3 id="3、应用场景"><a href="#3、应用场景" class="headerlink" title="3、应用场景"></a>3、应用场景</h3><ul>
<li>历史状态可用的业务，比如上面说的sum等聚合。但是对于历史状态不可用的聚合，比如中位数、方差等，无法通过简单使用物化视图或预聚合来实现。</li>
<li>数据过滤</li>
<li>当作触发器来用</li>
<li>流数据处理</li>
<li><font color="Red">重排序。</font>即基础数据表只能有一个排序字段，但是如果某个业务想要以其他字段排序或group查询效率很低，则可以新建一个以其他字段为order by的物化视图，再将基础数据表的指定数据插入到物化视图中。这种玩法本质是为了做两张表。且一张表是另外一个张表的子集，则可以通过一个sql往两张表写入数据。</li>
</ul>
<h2 id="十三、MaterializeMySQL引擎"><a href="#十三、MaterializeMySQL引擎" class="headerlink" title="十三、MaterializeMySQL引擎"></a>十三、MaterializeMySQL引擎</h2><p>MaterializeMySQL引擎是一个<strong>库引擎</strong>，作用是直接将MySQL的数据变化通过流的形式同步到clickhouse中，底层原理和flinkcdc、canal一样都是基于binlog实现的。</p>
<p>MySQL配置：</p>
<pre><code class="shell">default-authentication-plugin=mysql_native_password # 需要用这种密码验证方式

# 如果配置了主从，则要加上以下配置
gtid-mode=on # 主从切换时保证数据一致性
enforce-gtid-consistency=1 # 强一致性
log-slave-updates=1 # 从服务器日志记录
</code></pre>
<p>clickhouse配置：</p>
<pre><code class="sql">set allow_experimental_database_materialize_mysql=1; -- 当前版本这个配置是关闭的
</code></pre>
<p>创建对应的数据库：</p>
<pre><code class="sql">-- MySQL中创建新数据库和表
create database testck;
create table testck.test1(
    seq bigint auto_increment primary key
)default charset=utf8mb4;

-- clickhouse创建以下数据库
create database testck engine=MaterializeMySQL(&#39;192.168.141.141:3306&#39;, &#39;testck&#39;, &#39;root&#39;, &#39;root&#39;);
</code></pre>
<p>注意，通过MaterializeMySQL同步的数据库表必须要有主键，不然会报错。</p>
<p>如果你的的密码认证插件是从<code>caching_sha2_password</code>临时修改成<code>mysql_native_password</code>的话，你还<font color="Red">需要修改mysql.user表中的<code>plugin</code>字段修改为<code>mysql_native_password</code></font>。不然查询clickhouse的数据时会报错：</p>
<pre><code class="shell">Code: 100. DB::Exception: Received from myubuntu1:9003. DB::Exception: Access denied for user root.
</code></pre>
<p>同步后，会自动生成对应的clickhouse表，如下。</p>
<p>MySQL的主键 = clickhouse的order by，并且自动加上了<code>_sign</code>和<code>_version</code>字段。</p>
<p><code>_sign</code>字段：数据是否有效的标识，1代表有效数据，-1代表无效数据，即被删除了的数据。</p>
<p><code>_version</code>字段：数据版本号，每次执行MySQL的sql语句，转换为clickhouse的一条sql语句插入的数据版本号都是相同的。clickhouse内部应该维护了一个最大版本号，最新sql语句写入的数据会使用这个最大版本号，使用完之后再让这个最大版本号+1。</p>
<pre><code class="sql">┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ CREATE TABLE testck.test1
(
    `seq` Int64,
    `_sign` Int8 MATERIALIZED 1,
    `_version` UInt64 MATERIALIZED 1,
    INDEX _version _version TYPE minmax GRANULARITY 1
)
ENGINE = ReplacingMergeTree(_version)
PARTITION BY intDiv(seq, 18446744073709551)
ORDER BY tuple(seq)
SETTINGS index_granularity = 8192 │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
</code></pre>
<p>MySQL的增删改操作都会通过一定的方式转换为clickhouse的insert操作。</p>
<ul>
<li>MySQL insert = clickhouse的<code>insert with _sign = 1</code>的语句。</li>
<li>MySQL update = clickhouse的<code>insert with _sign = -1</code>和<code>insert with _sign = 1</code>的两条语句。</li>
<li>MySQL delete = clickhouse的<code>insert with _sign = -1</code>的语句。</li>
</ul>
<h2 id="十四、常见问题"><a href="#十四、常见问题" class="headerlink" title="十四、常见问题"></a>十四、常见问题</h2><p><a target="_blank" rel="noopener" href="https://help.aliyun.com/document_detail/162815.html">常见问题排查 (aliyun.com)</a></p>
<h2 id="十五、监控和备份"><a href="#十五、监控和备份" class="headerlink" title="十五、监控和备份"></a>十五、监控和备份</h2><h3 id="1、监控"><a href="#1、监控" class="headerlink" title="1、监控"></a>1、监控</h3><p>Promethseus + Grafana</p>
<p>Promethseus：<a target="_blank" rel="noopener" href="http://promethseus.io/download">promethseus.io</a></p>
<p>Grafana：<a target="_blank" rel="noopener" href="https://grafana.com/Grafana/download">Download Grafana | Grafana Labs</a></p>
<h3 id="2、备份"><a href="#2、备份" class="headerlink" title="2、备份"></a>2、备份</h3><p>手动：</p>
<pre><code class="sql">-- 备份到freeze目录下
ALTER TABLE table_name FREEZE [PARTITION partition_expr] [WITH NAME &#39;backup_name&#39;]

-- 从detach目录下恢复
ALTER TABLE table_name ATTACH PARTITION|PART partition_expr
</code></pre>
<p>自动：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/AlexAkulov/clickhouse-backup">GitHub - AlexAkulov/clickhouse-backup: Tool for easy ClickHouse backup and restore with cloud storages support</a>（小心使用，可能有版本兼容问题。）</p>
<h2 id="N、一些不同寻常的点（坑）"><a href="#N、一些不同寻常的点（坑）" class="headerlink" title="N、一些不同寻常的点（坑）"></a>N、一些不同寻常的点（坑）</h2><p>（1）普通类型的默认值不为null，都有各自的默认值，具体见Nullable类型说明。</p>
<p>（2）clickhouse的primary key是可以重复的，有需要的话得手动将其设置为unique。</p>
<p>（3）ReplacingMergeTree的聚合并不是实时的，每次查询都需要手动聚合去重。</p>

</div>


    <div class="post-guide">
        <div class="item left">
            
              <a href="/database/mysql/MySQL%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE%E7%9B%AE%E5%BD%95/MySQL%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE%E7%9B%AE%E5%BD%95">
                  <i class="fa fa-angle-left" aria-hidden="true"></i>
                  MySQL迁移数据目录
              </a>
            
        </div>
        <div class="item right">
            
              <a href="/bigdata/kafka/kafka">
                kafka
                <i class="fa fa-angle-right" aria-hidden="true"></i>
              </a>
            
        </div>
    </div>




<script>
	
	
</script>
	</div>
	<div id="footer">
	<p>
	©2019-<span id="footerYear"></span> 
	
	<br>
	Theme <a href="//github.com/wujun234/hexo-theme-tree" target="_blank">Tree</a>
	Powered by <a href="//hexo.io" target="_blank">Hexo</a>
	</p>
</div>
<script type="text/javascript"> 
	document.getElementById('footerYear').innerHTML = new Date().getFullYear() + '';
</script>
	<button id="totop-toggle" class="toggle"><i class="fa fa-angle-double-up" aria-hidden="true"></i></button>
</body>
</html>